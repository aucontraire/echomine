{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Echomine","text":"<p>Library-first tool for parsing AI conversation exports with search, filtering, and markdown export</p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p>Echomine is a Python library and CLI tool for parsing, searching, and exporting AI conversation exports. Initially designed for ChatGPT exports, it uses a multi-provider adapter pattern to support future AI platforms (Claude, Gemini, etc.).</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Memory Efficient: Stream-based parsing handles 1GB+ files with constant memory usage</li> <li>Advanced Search: BM25 relevance ranking with exact phrase matching, boolean logic, role filtering, and keyword exclusion (v1.1.0+)</li> <li>Message Snippets: Automatic preview generation for search results with match context (v1.1.0+)</li> <li>Statistics &amp; Analytics: Calculate export statistics, conversation metrics, and temporal patterns (v1.2.0+)</li> <li>Rich CLI Output: Color-coded terminal formatting, tables, progress bars, and syntax highlighting (v1.2.0+)</li> <li>Multiple Export Formats: Export to Markdown (with YAML frontmatter), JSON, or CSV (v1.2.0+)</li> <li>Type Safe: Strict typing with Pydantic v2 and mypy --strict compliance</li> <li>Library First: All CLI capabilities available as importable Python library</li> <li>Multi-Provider Ready: Adapter pattern supports multiple AI export formats</li> </ul>"},{"location":"#design-principles","title":"Design Principles","text":"<ol> <li>Library-First Architecture: CLI built on top of library, not vice versa</li> <li>Strict Type Safety: mypy --strict, no <code>Any</code> types in public API</li> <li>Memory Efficiency: Stream-based parsing, never load entire file into memory</li> <li>Test-Driven Development: All features test-first validated</li> <li>YAGNI: Simple solutions, no speculative features</li> </ol>"},{"location":"#quick-example","title":"Quick Example","text":""},{"location":"#library-api-primary-interface","title":"Library API (Primary Interface)","text":"<pre><code>from echomine import OpenAIAdapter, SearchQuery\nfrom pathlib import Path\n\n# Initialize adapter (stateless, reusable)\nadapter = OpenAIAdapter()\nexport_file = Path(\"conversations.json\")\n\n# 1. List all conversations (discovery)\nfor conversation in adapter.stream_conversations(export_file):\n    print(f\"[{conversation.created_at.date()}] {conversation.title}\")\n    print(f\"  Messages: {len(conversation.messages)}\")\n\n# 2. Advanced search with v1.1.0 features\nquery = SearchQuery(\n    keywords=[\"algorithm\", \"design\"],\n    phrases=[\"algo-insights\"],  # Exact phrase matching\n    match_mode=\"all\",  # Require ALL keywords (AND logic)\n    exclude_keywords=[\"test\"],  # Filter out unwanted results\n    role_filter=\"user\",  # Search only user messages\n    limit=10\n)\nfor result in adapter.search(export_file, query):\n    print(f\"{result.conversation.title} (score: {result.score:.2f})\")\n    print(f\"  Preview: {result.snippet}\")  # v1.1.0: automatic snippets\n\n# 3. Filter by date range\nfrom datetime import date\nquery = SearchQuery(\n    keywords=[\"refactor\"],\n    from_date=date(2024, 1, 1),\n    to_date=date(2024, 3, 31),\n    limit=5\n)\nresults = list(adapter.search(export_file, query))\n\n# 4. Calculate statistics (v1.2.0+)\nfrom echomine import calculate_statistics\nstats = calculate_statistics(export_file)\nprint(f\"Total: {stats.total_conversations} conversations\")\nprint(f\"Messages: {stats.total_messages}\")\n\n# 5. Get specific conversation by ID\nconversation = adapter.get_conversation_by_id(export_file, \"conv-abc123\")\nif conversation:\n    print(f\"Found: {conversation.title}\")\n</code></pre>"},{"location":"#cli-usage-built-on-library","title":"CLI Usage (Built on Library)","text":"<pre><code># List all conversations\nechomine list export.json\n\n# Search by keywords\nechomine search export.json --keywords \"algorithm,design\" --limit 10\n\n# v1.1.0: Exact phrase matching\nechomine search export.json --phrase \"algo-insights\"\n\n# v1.1.0: Boolean match mode (require ALL keywords)\nechomine search export.json -k \"python\" -k \"async\" --match-mode all\n\n# v1.1.0: Exclude unwanted results\nechomine search export.json -k \"python\" --exclude \"django\" --exclude \"flask\"\n\n# v1.1.0: Role filtering\nechomine search export.json -k \"refactor\" --role user\n\n# Search by title (fast, metadata-only)\nechomine search export.json --title \"Project\"\n\n# Filter by date range\nechomine search export.json --from-date \"2024-01-01\" --to-date \"2024-03-31\"\n\n# v1.2.0: View statistics\nechomine stats export.json\n\n# v1.2.0: Get conversation by ID\nechomine get export.json conv-abc123\n\n# Export conversation to markdown with YAML frontmatter (v1.2.0 default)\nechomine export export.json conv-abc123 --output algo.md\n\n# v1.2.0: Export as CSV\nechomine export export.json conv-abc123 --format csv --output algo.csv\n\n# Export as JSON for piping\nechomine export export.json conv-abc123 -f json | jq '.messages | length'\n\n# JSON output for search results\nechomine search export.json --keywords \"python\" --json | jq '.results[].title'\n</code></pre>"},{"location":"#performance","title":"Performance","text":"<p>Echomine is designed for memory efficiency and speed:</p> <ul> <li>Memory: O(1) memory usage regardless of file size (streaming-based)</li> <li>Search: &lt;30 seconds for 1.6GB files (10K conversations, 50K messages)</li> <li>Listing: &lt;5 seconds for 10K conversations</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#from-pypi-when-published","title":"From PyPI (when published)","text":"<pre><code>pip install echomine\n</code></pre>"},{"location":"#from-source","title":"From Source","text":"<pre><code># Clone repository\ngit clone https://github.com/echomine/echomine.git\ncd echomine\n\n# Install with development dependencies\npip install -e \".[dev]\"\n\n# Install pre-commit hooks (optional)\npre-commit install\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide: Get started with library and CLI usage</li> <li>Library Usage: Comprehensive library API guide</li> <li>CLI Usage: Command-line interface reference</li> <li>API Reference: Detailed API documentation</li> <li>Architecture: Design principles and patterns</li> <li>Contributing: Development setup and guidelines</li> </ul>"},{"location":"#license","title":"License","text":"<p>AGPL-3.0 License - See LICENSE file for details</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>Built with: - Pydantic - Data validation and type safety - ijson - Streaming JSON parser - Typer - CLI framework - Rich - Terminal formatting - structlog - Structured logging</p>"},{"location":"architecture/","title":"Architecture","text":"<p>This document describes the design principles, patterns, and architectural decisions that guide Echomine development.</p>"},{"location":"architecture/#design-principles","title":"Design Principles","text":"<p>Echomine follows 8 core Constitution Principles that guide all architectural and implementation decisions:</p>"},{"location":"architecture/#i-library-first-architecture","title":"I. Library-First Architecture","text":"<p>Core functionality is built as an importable library, with CLI as a thin wrapper on top.</p> <p>Rationale: Primary use case is cognivault integration. CLI is a convenience layer, not the core product.</p> <p>Implementation:</p> <pre><code># \u2705 CORRECT: CLI wraps library\nfrom echomine import OpenAIAdapter, SearchQuery\n\ndef search_command(file: Path, keywords: list[str]):\n    adapter = OpenAIAdapter()  # Library component\n    query = SearchQuery(keywords=keywords)\n    for result in adapter.search(file, query):\n        print_result(result)  # CLI formatting\n\n# \u274c WRONG: Library calls CLI\ndef search(file: Path, query: SearchQuery):\n    subprocess.run([\"echomine\", \"search\", ...])  # NO!\n</code></pre>"},{"location":"architecture/#cli-interface-contract","title":"II. CLI Interface Contract","text":"<p>Results go to stdout, progress and errors go to stderr, with standard exit codes.</p> <p>Contract:</p> <ul> <li>stdout: Results only (JSON or human-readable)</li> <li>stderr: Progress, warnings, errors</li> <li>Exit codes: 0 (success), 1 (operational error), 2 (usage error)</li> </ul> <p>Benefits:</p> <ul> <li>Pipeline-friendly (compose with jq, grep, xargs)</li> <li>Separates data from metadata</li> <li>Standard UNIX conventions</li> </ul>"},{"location":"architecture/#iii-test-driven-development-tdd","title":"III. Test-Driven Development (TDD)","text":"<p>All features follow the RED-GREEN-REFACTOR cycle with no exceptions.</p> <p>Workflow:</p> <ol> <li>RED: Write failing test first (verify it fails)</li> <li>GREEN: Write minimal code to pass test</li> <li>REFACTOR: Improve code while keeping tests green</li> </ol> <p>Enforcement: Pre-commit hooks reject commits without test coverage.</p>"},{"location":"architecture/#iv-observability-debuggability","title":"IV. Observability &amp; Debuggability","text":"<p>JSON structured logs via structlog with contextual fields.</p> <p>Logging Pattern:</p> <pre><code>from echomine.utils.logging import get_logger\n\nlogger = get_logger(__name__)\n\nlogger.info(\n    \"Processing conversation\",\n    operation=\"stream_conversations\",\n    file_name=str(file_path),\n    conversation_id=conversation.id,\n    count=count,\n)\n</code></pre> <p>Graceful Degradation:</p> <ul> <li>Malformed entries logged and skipped (WARNING level)</li> <li>Processing continues for valid entries</li> <li>Summary reports include skip counts</li> </ul>"},{"location":"architecture/#v-simplicity-yagni","title":"V. Simplicity &amp; YAGNI","text":"<p>Implement ONLY what the spec requires. No speculative features.</p> <p>Examples:</p> <ul> <li>No database layer (just file parsing)</li> <li>No caching (streaming is sufficient)</li> <li>No async (sync generators are simpler and adequate)</li> </ul> <p>When Complexity is Justified:</p> <ul> <li>ijson: Required for O(1) memory usage on 1GB+ files</li> <li>BM25 ranking: Spec requirement for search quality</li> </ul>"},{"location":"architecture/#vi-strict-typing-mandatory","title":"VI. Strict Typing Mandatory","text":"<p>mypy --strict with ZERO TOLERANCE for errors.</p> <p>Requirements:</p> <ul> <li>Type hints on ALL functions, methods, variables</li> <li>No <code>Any</code> types in public API</li> <li>Use Protocol for abstractions</li> <li>Pydantic models for all structured data</li> </ul> <p>Example:</p> <pre><code>from typing import Iterator, Optional\nfrom pathlib import Path\nfrom echomine.models import Conversation, SearchQuery, SearchResult\n\ndef search(\n    file_path: Path,\n    query: SearchQuery,\n    *,\n    progress_callback: Optional[Callable[[int], None]] = None,\n) -&gt; Iterator[SearchResult[Conversation]]:\n    \"\"\"Full type safety.\"\"\"\n    pass\n</code></pre>"},{"location":"architecture/#vii-multi-provider-adapter-pattern","title":"VII. Multi-Provider Adapter Pattern","text":"<p>Stateless adapters implement ConversationProvider protocol.</p> <p>Design:</p> <ul> <li>OpenAIAdapter for ChatGPT (v1.0)</li> <li>Future: ClaudeAdapter, GeminiAdapter (v2.0+)</li> <li>Shared models (Message, Conversation) across providers</li> <li>Provider-specific data in <code>metadata</code> dict</li> </ul> <p>Stateless Pattern:</p> <pre><code># \u2705 CORRECT: Stateless adapter\nclass OpenAIAdapter:\n    def stream_conversations(self, file_path: Path) -&gt; Iterator[Conversation]:\n        # file_path passed as argument\n        pass\n\n# \u274c WRONG: Stateful adapter\nclass OpenAIAdapter:\n    def __init__(self, file_path: Path):  # NO!\n        self.file_path = file_path\n</code></pre>"},{"location":"architecture/#viii-memory-efficiency-streaming","title":"VIII. Memory Efficiency &amp; Streaming","text":"<p>O(1) memory usage regardless of file size via ijson streaming.</p> <p>Pattern:</p> <pre><code># \u2705 CORRECT: Streaming with ijson\ndef stream_conversations(file_path: Path) -&gt; Iterator[Conversation]:\n    with open(file_path, \"rb\") as f:\n        parser = ijson.items(f, \"item\")\n        for item in parser:\n            yield Conversation.model_validate(item)\n\n# \u274c WRONG: Load entire file\ndef stream_conversations(file_path: Path) -&gt; Iterator[Conversation]:\n    with open(file_path) as f:\n        data = json.load(f)  # Loads entire file into memory!\n        for item in data:\n            yield Conversation.model_validate(item)\n</code></pre> <p>Performance Contracts:</p> <ul> <li>1.6GB file search in &lt;30 seconds</li> <li>10K conversations + 50K messages on 8GB RAM</li> <li>O(1) memory (constant, not proportional to file size)</li> </ul>"},{"location":"architecture/#architectural-patterns","title":"Architectural Patterns","text":""},{"location":"architecture/#adapter-pattern","title":"Adapter Pattern","text":"<p>All provider-specific logic is encapsulated in adapters that implement the <code>ConversationProvider</code> protocol:</p> <pre><code>from typing import Protocol, Iterator, TypeVar\nfrom echomine.models import Conversation, SearchQuery, SearchResult\n\nConversationT = TypeVar(\"ConversationT\", bound=\"Conversation\")\n\nclass ConversationProvider(Protocol[ConversationT]):\n    \"\"\"Protocol for conversation export adapters.\"\"\"\n\n    def stream_conversations(\n        self,\n        file_path: Path,\n    ) -&gt; Iterator[ConversationT]:\n        \"\"\"Stream conversations from export file.\"\"\"\n        ...\n\n    def search(\n        self,\n        file_path: Path,\n        query: SearchQuery,\n    ) -&gt; Iterator[SearchResult[ConversationT]]:\n        \"\"\"Search conversations with BM25 ranking.\"\"\"\n        ...\n\n    def get_conversation_by_id(\n        self,\n        file_path: Path,\n        conversation_id: str,\n    ) -&gt; Optional[ConversationT]:\n        \"\"\"Retrieve specific conversation by ID.\"\"\"\n        ...\n</code></pre>"},{"location":"architecture/#immutable-data-models","title":"Immutable Data Models","text":"<p>All data models use Pydantic with strict validation and immutability:</p> <pre><code>from pydantic import BaseModel, ConfigDict, Field\n\nclass Message(BaseModel):\n    model_config = ConfigDict(\n        frozen=True,           # Immutability\n        strict=True,           # No type coercion\n        extra=\"forbid\",        # Reject unknown fields\n        validate_assignment=True,\n    )\n\n    id: str = Field(..., min_length=1)\n    content: str\n    role: Literal[\"user\", \"assistant\", \"system\"]\n    timestamp: datetime  # UTC, timezone-aware\n    parent_id: Optional[str] = None\n</code></pre>"},{"location":"architecture/#streaming-pattern","title":"Streaming Pattern","text":"<p>All operations use generators for memory efficiency:</p> <pre><code># Returns Iterator, not List\ndef stream_conversations(file_path: Path) -&gt; Iterator[Conversation]:\n    with open(file_path, \"rb\") as f:\n        parser = ijson.items(f, \"item\")\n        for item in parser:\n            try:\n                yield Conversation.model_validate(item)\n            except ValidationError as e:\n                logger.warning(\"Skipped malformed entry\", reason=str(e))\n                continue\n</code></pre>"},{"location":"architecture/#error-handling-strategy","title":"Error Handling Strategy","text":"<p>Fail-Fast on Unrecoverable Errors:</p> <ul> <li>FileNotFoundError: File doesn't exist</li> <li>PermissionError: No read access</li> <li>SchemaVersionError: Unsupported export version</li> </ul> <p>Graceful Degradation on Data Errors:</p> <ul> <li>ValidationError: Skip malformed conversation, log warning, continue</li> <li>ParseError: Skip malformed JSON entry, log warning, continue</li> </ul> <p>No Retries: All errors are permanent. Users must fix the issue manually.</p>"},{"location":"architecture/#project-structure","title":"Project Structure","text":"<pre><code>echomine/\n\u251c\u2500\u2500 src/echomine/           # Library source code\n\u2502   \u251c\u2500\u2500 models/             # Pydantic data models\n\u2502   \u2502   \u251c\u2500\u2500 conversation.py # Conversation, Message\n\u2502   \u2502   \u251c\u2500\u2500 search.py       # SearchQuery, SearchResult\n\u2502   \u2502   \u2514\u2500\u2500 protocols.py    # ConversationProvider protocol\n\u2502   \u251c\u2500\u2500 adapters/           # Provider adapters\n\u2502   \u2502   \u2514\u2500\u2500 openai/         # OpenAI (ChatGPT) adapter\n\u2502   \u251c\u2500\u2500 search/             # Search and ranking logic\n\u2502   \u2502   \u2514\u2500\u2500 ranking.py      # BM25 algorithm\n\u2502   \u251c\u2500\u2500 exporters/          # Export formatters\n\u2502   \u2502   \u2514\u2500\u2500 markdown.py     # Markdown exporter\n\u2502   \u251c\u2500\u2500 utils/              # Utilities\n\u2502   \u2502   \u2514\u2500\u2500 logging.py      # Structured logging setup\n\u2502   \u2514\u2500\u2500 cli/                # CLI commands (thin wrapper)\n\u2502       \u251c\u2500\u2500 app.py          # Typer app\n\u2502       \u2514\u2500\u2500 commands/       # Individual commands\n\u251c\u2500\u2500 tests/                  # Test suite\n\u2502   \u251c\u2500\u2500 unit/               # Unit tests (70%)\n\u2502   \u251c\u2500\u2500 integration/        # Integration tests (20%)\n\u2502   \u251c\u2500\u2500 contract/           # Protocol contract tests (5%)\n\u2502   \u2514\u2500\u2500 performance/        # Performance benchmarks (5%)\n\u2514\u2500\u2500 specs/                  # Design documents\n    \u2514\u2500\u2500 001-ai-chat-parser/ # Feature specification\n</code></pre>"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#streaming-operation-flow","title":"Streaming Operation Flow","text":"<pre><code>Export File (JSON)\n    \u2193\nijson.items() [Streaming Parser]\n    \u2193\ndict (raw JSON object)\n    \u2193\nConversation.model_validate() [Pydantic Validation]\n    \u2193\nConversation (Immutable Model)\n    \u2193\nGenerator Yield\n    \u2193\nConsumer (CLI, Library User)\n</code></pre>"},{"location":"architecture/#search-operation-flow","title":"Search Operation Flow","text":"<pre><code>Export File\n    \u2193\nstream_conversations() [Stream All]\n    \u2193\nFilter by Date Range (if specified)\n    \u2193\nFilter by Title (if specified) [Metadata-only]\n    \u2193\nBM25 Ranking (if keywords specified) [Full-text]\n    \u2193\nSort by Relevance Score (descending)\n    \u2193\nLimit Results\n    \u2193\nSearchResult[Conversation] Generator\n    \u2193\nConsumer\n</code></pre>"},{"location":"architecture/#technology-choices","title":"Technology Choices","text":""},{"location":"architecture/#core-stack","title":"Core Stack","text":"Technology Purpose Why Chosen Python 3.12+ Language Modern type hints (PEP 695, improved generics) Pydantic v2 Data validation Comprehensive validation, immutability, JSON schema ijson JSON parsing Streaming for O(1) memory (handles 1GB+ files) typer CLI framework Native type hint support, automatic help rich Terminal output Tables, progress bars, syntax highlighting structlog Logging JSON output for observability, contextual fields"},{"location":"architecture/#development-tools","title":"Development Tools","text":"Tool Purpose Why Chosen pytest Testing De facto standard, excellent fixtures/plugins mypy Type checking Strict mode for zero-tolerance type safety ruff Linting/Formatting Fast (10-100x faster than alternatives) pre-commit Git hooks Automated quality gates"},{"location":"architecture/#alternative-considerations","title":"Alternative Considerations","text":"<p>Why not async?</p> <ul> <li>Sync generators are simpler</li> <li>No I/O-bound operations (just CPU + disk reads)</li> <li>ijson streaming is adequate for performance</li> </ul> <p>Why not database?</p> <ul> <li>YAGNI: Not required by spec</li> <li>Export files are read-only</li> <li>Streaming handles large files efficiently</li> </ul> <p>Why not caching?</p> <ul> <li>Export files don't change during read</li> <li>Memory efficiency is more important</li> <li>Adds complexity without clear benefit</li> </ul>"},{"location":"architecture/#extension-points","title":"Extension Points","text":""},{"location":"architecture/#adding-new-providers","title":"Adding New Providers","text":"<ol> <li>Implement <code>ConversationProvider</code> protocol</li> <li>Map provider-specific roles to standard roles</li> <li>Store provider-specific data in <code>metadata</code> dict</li> <li>Add provider-specific tests</li> </ol> <p>Example:</p> <pre><code>class ClaudeAdapter:\n    \"\"\"Adapter for Anthropic Claude exports.\"\"\"\n\n    def stream_conversations(\n        self,\n        file_path: Path,\n    ) -&gt; Iterator[Conversation]:\n        # Parse Claude-specific format\n        # Map \"human\" \u2192 \"user\", \"assistant\" \u2192 \"assistant\"\n        # Store Claude-specific fields in metadata\n        pass\n</code></pre>"},{"location":"architecture/#adding-new-search-filters","title":"Adding New Search Filters","text":"<ol> <li>Add optional field to <code>SearchQuery</code> model</li> <li>Update search logic in adapters</li> <li>Add tests for new filter</li> <li>Update CLI to accept new filter</li> </ol> <p>Backward compatibility: New filters must be optional with sensible defaults.</p>"},{"location":"architecture/#performance-optimization","title":"Performance Optimization","text":""},{"location":"architecture/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>Streaming: Never load entire file into memory</li> <li>Generators: Use <code>Iterator</code> return types, not <code>List</code></li> <li>Context Managers: Ensure file handles are closed</li> </ul>"},{"location":"architecture/#search-performance","title":"Search Performance","text":"<ul> <li>Title Filtering: Metadata-only (no message content scan)</li> <li>BM25 Ranking: Only when keywords specified</li> <li>Early Termination: Stop after <code>limit</code> results</li> </ul>"},{"location":"architecture/#profiling","title":"Profiling","text":"<p>Use pytest-benchmark for performance regression testing:</p> <pre><code>def test_search_performance(benchmark):\n    \"\"\"Search 1.6GB file completes in &lt;30 seconds.\"\"\"\n    result = benchmark(adapter.search, large_file, query)\n    assert benchmark.stats.stats.mean &lt; 30.0\n</code></pre>"},{"location":"architecture/#security-considerations","title":"Security Considerations","text":""},{"location":"architecture/#input-validation","title":"Input Validation","text":"<ul> <li>All file paths validated (Path objects, existence checks)</li> <li>All search queries validated (Pydantic models)</li> <li>No shell execution or eval()</li> </ul>"},{"location":"architecture/#resource-limits","title":"Resource Limits","text":"<ul> <li>Streaming prevents OOM attacks</li> <li>File handle cleanup ensures no resource leaks</li> </ul>"},{"location":"architecture/#data-privacy","title":"Data Privacy","text":"<ul> <li>No network calls (offline library)</li> <li>No telemetry or tracking</li> <li>All processing local</li> </ul>"},{"location":"architecture/#concurrency-model","title":"Concurrency Model","text":""},{"location":"architecture/#thread-safety","title":"Thread Safety","text":"<ul> <li>Adapter instances: Thread-safe (stateless)</li> <li>Iterators: NOT thread-safe (each thread needs its own)</li> </ul>"},{"location":"architecture/#multi-process-safety","title":"Multi-Process Safety","text":"<ul> <li>Multiple processes can read same file concurrently</li> <li>File system provides read isolation</li> </ul>"},{"location":"architecture/#future-considerations","title":"Future Considerations","text":""},{"location":"architecture/#multi-provider-support-v20","title":"Multi-Provider Support (v2.0)","text":"<ul> <li>Add ClaudeAdapter, GeminiAdapter</li> <li>Auto-detection helper (optional)</li> <li>Provider registry pattern</li> </ul>"},{"location":"architecture/#advanced-search-v11","title":"Advanced Search (v1.1)","text":"<ul> <li>Semantic search (embeddings)</li> <li>Regex pattern matching</li> <li>Boolean query syntax</li> </ul>"},{"location":"architecture/#export-formats-v11","title":"Export Formats (v1.1)","text":"<ul> <li>HTML export</li> <li>PDF export</li> <li>CSV export (metadata)</li> </ul>"},{"location":"architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Library Usage: Comprehensive API guide</li> <li>CLI Usage: Command-line reference</li> <li>API Reference: Detailed API documentation</li> <li>Contributing: Development guidelines</li> </ul>"},{"location":"cli-usage/","title":"CLI Usage Guide","text":"<p>Complete reference for using Echomine from the command line.</p>"},{"location":"cli-usage/#installation","title":"Installation","text":"<pre><code>pip install echomine\n</code></pre> <p>Verify installation:</p> <pre><code>echomine --version\n</code></pre>"},{"location":"cli-usage/#global-options","title":"Global Options","text":"<p>Available for all commands:</p> <pre><code>echomine [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --version  Show version and exit\n  --help     Show help message and exit\n</code></pre>"},{"location":"cli-usage/#commands","title":"Commands","text":""},{"location":"cli-usage/#list","title":"list","text":"<p>List all conversations in an export file.</p> <p>Usage:</p> <pre><code>echomine list [OPTIONS] FILE_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>FILE_PATH</code>: Path to OpenAI export JSON file (required)</li> </ul> <p>Options:</p> <ul> <li><code>--limit INTEGER</code>: Maximum number of conversations to list</li> <li><code>--json</code>: Output as JSON (for programmatic use)</li> <li><code>--help</code>: Show help message</li> </ul> <p>Examples:</p> <pre><code># List all conversations (human-readable)\nechomine list conversations.json\n\n# List with limit\nechomine list conversations.json --limit 10\n\n# JSON output for piping\nechomine list conversations.json --json | jq '.conversations[].title'\n\n# Count conversations\nechomine list conversations.json --json | jq '.conversations | length'\n</code></pre> <p>Output (Human-Readable):</p> <pre><code>Conversations in conversations.json\n\n[2024-01-15] Python Async Best Practices\n  Messages: 42\n  ID: conv-abc123\n\n[2024-01-14] Algorithm Design Patterns\n  Messages: 28\n  ID: conv-xyz789\n\n...\n\nTotal: 145 conversations\n</code></pre> <p>Output (JSON):</p> <pre><code>{\n  \"conversations\": [\n    {\n      \"id\": \"conv-abc123\",\n      \"title\": \"Python Async Best Practices\",\n      \"created_at\": \"2024-01-15T10:30:00Z\",\n      \"message_count\": 42\n    }\n  ],\n  \"total\": 145\n}\n</code></pre>"},{"location":"cli-usage/#search","title":"search","text":"<p>Search conversations with keyword matching and relevance ranking.</p> <p>Usage:</p> <pre><code>echomine search [OPTIONS] FILE_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>FILE_PATH</code>: Path to OpenAI export JSON file (required)</li> </ul> <p>Options:</p> <p>Content Matching (v1.1.0+): - <code>--keywords, -k TEXT</code>: Keywords to search for (can specify multiple, OR logic by default) - <code>--phrase TEXT</code>: Exact phrase to match (can specify multiple, preserves hyphens/special chars) - <code>--match-mode TEXT</code>: Keyword matching mode: 'any' (OR, default) or 'all' (AND) - <code>--exclude TEXT</code>: Keywords to exclude from results (can specify multiple, OR logic) - <code>--role TEXT</code>: Filter to messages from specific role: 'user', 'assistant', or 'system'</p> <p>Legacy Filters: - <code>--title, -t TEXT</code>: Filter by conversation title (partial match, case-insensitive) - <code>--from-date DATE</code>: Filter conversations created on or after date (YYYY-MM-DD) - <code>--to-date DATE</code>: Filter conversations created on or before date (YYYY-MM-DD)</p> <p>Output Control: - <code>--limit, -n INTEGER</code>: Maximum number of results to return (default: 10) - <code>--format, -f TEXT</code>: Output format ('text' or 'json') - <code>--quiet, -q</code>: Suppress progress indicators - <code>--json</code>: Output as JSON (alias for --format json) - <code>--help</code>: Show help message</p>"},{"location":"cli-usage/#how-search-filters-combine","title":"How Search Filters Combine","text":"<p>Search filters follow a two-stage matching process:</p> <p>Stage 1: Content Matching (OR relationship)</p> <p>Conversations are included if ANY of these match: - Phrases: ANY phrase is found (exact match, case-insensitive) - Keywords: Keywords match according to <code>--match-mode</code>   - <code>--match-mode any</code> (default): ANY keyword matches   - <code>--match-mode all</code>: ALL keywords must be present</p> <p>Phrases and keywords are alternatives: <code>--phrase \"api\" -k \"python\"</code> matches conversations containing EITHER \"api\" phrase OR \"python\" keyword.</p> <p>Stage 2: Post-Match Filters (AND relationship)</p> <p>After content matching, results are filtered by ALL of these conditions: - <code>--exclude</code>: Removes results containing ANY excluded term - <code>--role</code>: Only includes messages from the specified role - <code>--title</code>: Only includes conversations with matching title - <code>--from-date</code> / <code>--to-date</code>: Only includes conversations in date range</p> <p>Example Scenarios:</p> <pre><code># Matches if \"api\" phrase found OR \"python\" keyword found\nechomine search export.json --phrase \"api\" -k \"python\"\n\n# Matches if \"python\" AND \"async\" keywords both present\nechomine search export.json -k \"python\" -k \"async\" --match-mode all\n\n# Matches if (\"api\" phrase OR \"python\" keyword) AND NOT contains \"java\"\nechomine search export.json --phrase \"api\" -k \"python\" --exclude \"java\"\n\n# Matches if \"python\" keyword found in user messages only\nechomine search export.json -k \"python\" --role user\n\n# Matches if (\"tutorial\" phrase OR \"python\" keyword) AND title contains \"Guide\"\nechomine search export.json --phrase \"tutorial\" -k \"python\" --title \"Guide\"\n</code></pre> <p>Examples:</p> <pre><code># Basic keyword search\nechomine search export.json --keywords \"algorithm,design\"\n\n# Search with limit\nechomine search export.json --keywords \"python\" --limit 5\n\n# NEW v1.1.0: Exact phrase matching\nechomine search export.json --phrase \"algo-insights\"\nechomine search export.json --phrase \"data pipeline\" --phrase \"api design\"  # Multiple phrases (OR)\n\n# NEW v1.1.0: Boolean match mode (require ALL keywords)\nechomine search export.json -k \"python\" -k \"async\" --match-mode all\nechomine search export.json -k \"python\" -k \"async\" --match-mode any  # Default: OR logic\n\n# NEW v1.1.0: Exclude keywords\nechomine search export.json -k \"python\" --exclude \"django\"\nechomine search export.json -k \"python\" --exclude \"django\" --exclude \"flask\"  # Multiple exclusions\n\n# NEW v1.1.0: Role filtering\nechomine search export.json -k \"refactor\" --role user       # Your questions\nechomine search export.json -k \"recommend\" --role assistant # AI responses\nechomine search export.json -k \"system\" --role system       # System messages\n\n# NEW v1.1.0: Combined advanced search\nechomine search export.json \\\n  --phrase \"algo-insights\" \\\n  -k \"python\" \\\n  --exclude \"test\" \\\n  --role user \\\n  --match-mode all \\\n  --limit 5\n\n# Search by title (fast, metadata-only)\nechomine search export.json --title \"Project\"\n\n# Filter by date range\nechomine search export.json --from-date \"2024-01-01\" --to-date \"2024-03-31\"\n\n# Combine all filters\nechomine search export.json \\\n  --phrase \"api design\" \\\n  --keywords \"python,async\" \\\n  --exclude \"test\" \\\n  --role user \\\n  --title \"Tutorial\" \\\n  --from-date \"2024-01-01\" \\\n  --match-mode all \\\n  --limit 10\n\n# JSON output for processing (includes snippets in v1.1.0+)\nechomine search export.json --keywords \"machine learning\" --json | \\\n  jq '.results[] | select(.score &gt; 0.8) | {title: .conversation.title, snippet: .snippet}'\n</code></pre> <p>Output (Human-Readable):</p> <pre><code>Search Results\n\nScore  ID         Title                         Created     Snippet                                          Messages\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n0.92   abc-123    Python Async Best Practices   2024-01-15  How do I use async/await with Python...          42\n0.85   xyz-789    Algorithm Design Patterns     2024-01-14  I need to refactor my algorithm using... (+2)    28\n\nShowing 2 of 5 results\n</code></pre> <p>Note: v1.1.0+ automatically includes message snippets (~100 characters) with match preview. The \"(+N)\" indicator shows when multiple messages matched.</p> <p>Output (JSON):</p> <pre><code>{\n  \"results\": [\n    {\n      \"conversation\": {\n        \"id\": \"conv-abc123\",\n        \"title\": \"Python Async Best Practices\",\n        \"created_at\": \"2024-01-15T10:30:00Z\",\n        \"message_count\": 42\n      },\n      \"score\": 0.92,\n      \"matched_message_ids\": [\"msg-001\", \"msg-015\"],\n      \"snippet\": \"How do I use async/await with Python...\"\n    }\n  ],\n  \"total\": 5,\n  \"query\": {\n    \"keywords\": [\"algorithm\", \"design\"],\n    \"phrases\": null,\n    \"match_mode\": \"any\",\n    \"exclude_keywords\": null,\n    \"role_filter\": null,\n    \"limit\": 10\n  }\n}\n</code></pre> <p>Note: v1.1.0+ adds <code>matched_message_ids</code> (list of matching message IDs) and <code>snippet</code> (preview text) to each result.</p>"},{"location":"cli-usage/#advanced-search-features-v110","title":"Advanced Search Features (v1.1.0+)","text":"<p>Version 1.1.0 introduces five powerful search enhancements:</p>"},{"location":"cli-usage/#1-exact-phrase-matching","title":"1. Exact Phrase Matching","text":"<p>Search for exact multi-word phrases while preserving special characters like hyphens and underscores.</p> <p>Use Cases: - Find project-specific terminology: <code>\"algo-insights\"</code>, <code>\"data-pipeline\"</code> - Match code patterns: <code>\"async/await\"</code>, <code>\"error-handling\"</code> - Locate specific concepts: <code>\"machine learning\"</code>, <code>\"database migration\"</code></p> <p>Examples: <pre><code># Find exact phrase (hyphen preserved)\nechomine search export.json --phrase \"algo-insights\"\n\n# Multiple phrases (OR logic - matches any)\nechomine search export.json --phrase \"api design\" --phrase \"system architecture\"\n\n# Combine with keywords\nechomine search export.json --phrase \"algo-insights\" -k \"optimization\"\n</code></pre></p>"},{"location":"cli-usage/#2-boolean-match-mode","title":"2. Boolean Match Mode","text":"<p>Control whether keywords use AND logic (all required) or OR logic (any match).</p> <p>Use Cases: - Narrow results: Require ALL keywords present (<code>--match-mode all</code>) - Broad discovery: Match ANY keyword (<code>--match-mode any</code>, default) - Topic intersection: Find conversations covering multiple topics</p> <p>Examples: <pre><code># Require ALL keywords (AND logic)\nechomine search export.json -k \"python\" -k \"async\" -k \"testing\" --match-mode all\n\n# Default: ANY keyword matches (OR logic)\nechomine search export.json -k \"python\" -k \"javascript\" --match-mode any\n</code></pre></p> <p>Important: <code>--match-mode</code> only affects keywords. Phrases always use OR logic.</p>"},{"location":"cli-usage/#3-exclude-keywords","title":"3. Exclude Keywords","text":"<p>Filter out unwanted results containing specific terms.</p> <p>Use Cases: - Remove noise: Exclude \"test\", \"example\", \"tutorial\" - Filter frameworks: Exclude \"django\" when searching Python - Avoid topics: Exclude \"deprecated\", \"legacy\"</p> <p>Examples: <pre><code># Exclude single term\nechomine search export.json -k \"python\" --exclude \"django\"\n\n# Exclude multiple terms (OR logic - excludes if ANY present)\nechomine search export.json -k \"python\" --exclude \"django\" --exclude \"flask\" --exclude \"pyramid\"\n\n# Combine with other filters\nechomine search export.json -k \"refactor\" --exclude \"test\" --role user\n</code></pre></p> <p>Important: Excluded terms use OR logic - a result is removed if it contains ANY excluded term.</p>"},{"location":"cli-usage/#4-role-filtering","title":"4. Role Filtering","text":"<p>Search only messages from a specific author role.</p> <p>Use Cases: - Find your questions: <code>--role user</code> - Find AI recommendations: <code>--role assistant</code> - Find system prompts: <code>--role system</code></p> <p>Examples: <pre><code># Search only your messages\nechomine search export.json -k \"how do I\" --role user\n\n# Search only AI responses\nechomine search export.json -k \"recommend\" --role assistant\n\n# Search system messages\nechomine search export.json -k \"context\" --role system\n</code></pre></p> <p>Note: Role filtering is case-insensitive (<code>user</code>, <code>User</code>, <code>USER</code> all work).</p>"},{"location":"cli-usage/#5-message-snippets-automatic","title":"5. Message Snippets (Automatic)","text":"<p>All search results automatically include ~100 character previews of matched content.</p> <p>Features: - Shows first matching message content - Truncated with \"...\" for long messages - Multiple matches indicated with \"+N more\" - Fallback text for empty/malformed content</p> <p>Example Output: <pre><code>Score  Title                    Snippet\n0.92   Python Async Tutorial    How do I use async/await with Python...\n0.85   Refactoring Guide        I need to refactor my algorithm... (+2 more)\n</code></pre></p> <p>JSON Output: <pre><code>{\n  \"snippet\": \"How do I use async/await with Python...\",\n  \"matched_message_ids\": [\"msg-001\", \"msg-015\", \"msg-023\"]\n}\n</code></pre></p>"},{"location":"cli-usage/#combining-advanced-features","title":"Combining Advanced Features","text":"<p>All features work together for powerful precision searches:</p> <pre><code># Find conversations where:\n# - You asked about refactoring (role=user)\n# - Mentioning \"algo-insights\" exactly (phrase)\n# - Discussing Python (keyword)\n# - NOT about testing (exclude)\n# - All conditions must match (match-mode=all)\nechomine search export.json \\\n  --phrase \"algo-insights\" \\\n  -k \"python\" \\\n  --exclude \"test\" \\\n  --role user \\\n  --match-mode all \\\n  --limit 10\n</code></pre>"},{"location":"cli-usage/#filter-combination-logic","title":"Filter Combination Logic","text":"<p>Stage 1: Content Matching (OR relationship) - Phrases: Match if ANY phrase is found (exact, case-insensitive) - Keywords: Match according to <code>--match-mode</code> (any OR all) - Phrase OR keyword match (not both required)</p> <p>Stage 2: Post-Match Filters (AND relationship) - <code>--exclude</code>: Remove if ANY excluded term found - <code>--role</code>: Only messages from specified role - <code>--title</code>: Only conversations with matching title - <code>--from-date</code> / <code>--to-date</code>: Only in date range</p> <p>All post-match filters must be satisfied.</p>"},{"location":"cli-usage/#stats","title":"stats","text":"<p>Generate comprehensive statistics for your conversation export.</p> <p>Usage:</p> <pre><code>echomine stats [OPTIONS] FILE_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>FILE_PATH</code>: Path to OpenAI export JSON file (required)</li> </ul> <p>Options:</p> <ul> <li><code>--json</code>: Output as JSON (for programmatic use)</li> <li><code>--help</code>: Show help message</li> </ul> <p>Examples:</p> <pre><code># View export statistics (human-readable)\nechomine stats export.json\n\n# JSON output for scripting\nechomine stats export.json --json | jq '.total_conversations'\n\n# Analyze message distribution\nechomine stats export.json --json | jq '.average_messages'\n</code></pre> <p>Output (Human-Readable):</p> <pre><code>Export Statistics\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nTotal Conversations:  1,234\nTotal Messages:       45,678\nDate Range:           2024-01-01 to 2024-12-07\nAverage Messages:     37.0 per conversation\n\nLargest Conversation:\n  Title:     \"Deep Python Discussion\"\n  ID:        abc-123\n  Messages:  245\n\nSmallest Conversation:\n  Title:     \"Quick Question\"\n  ID:        xyz-789\n  Messages:  2\n</code></pre> <p>Output (JSON):</p> <pre><code>{\n  \"total_conversations\": 1234,\n  \"total_messages\": 45678,\n  \"date_range\": {\n    \"earliest\": \"2024-01-01T10:00:00Z\",\n    \"latest\": \"2024-12-07T15:30:00Z\"\n  },\n  \"average_messages\": 37.0,\n  \"largest_conversation\": {\n    \"id\": \"abc-123\",\n    \"title\": \"Deep Python Discussion\",\n    \"message_count\": 245\n  },\n  \"smallest_conversation\": {\n    \"id\": \"xyz-789\",\n    \"title\": \"Quick Question\",\n    \"message_count\": 2\n  }\n}\n</code></pre>"},{"location":"cli-usage/#get","title":"get","text":"<p>Retrieve and display a specific conversation by ID with Rich terminal formatting.</p> <p>Usage:</p> <pre><code>echomine get [OPTIONS] FILE_PATH CONVERSATION_ID\n</code></pre> <p>Arguments:</p> <ul> <li><code>FILE_PATH</code>: Path to OpenAI export JSON file (required)</li> <li><code>CONVERSATION_ID</code>: Conversation ID to retrieve (required)</li> </ul> <p>Options:</p> <ul> <li><code>--display TEXT</code>: Display mode: <code>full</code> (default, shows all messages), <code>summary</code> (metadata only), or <code>messages-only</code></li> <li><code>--json</code>: Output as JSON (for programmatic use)</li> <li><code>--help</code>: Show help message</li> </ul> <p>Examples:</p> <pre><code># Get conversation with full display (default)\nechomine get export.json conv-abc123\n\n# Show summary only (metadata, no messages)\nechomine get export.json conv-abc123 --display summary\n\n# Show messages only (no metadata header)\nechomine get export.json conv-abc123 --display messages-only\n\n# Get conversation as JSON\nechomine get export.json conv-abc123 --json\n\n# Pipe JSON to jq for processing\nechomine get export.json conv-abc123 --json | \\\n  jq '.messages[] | select(.role == \"user\") | .content'\n</code></pre> <p>Output (Full Display - Default):</p> <pre><code>Conversation: Python AsyncIO Tutorial\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nID:          conv-abc123\nCreated:     2023-11-14 22:13:20 UTC\nUpdated:     2023-11-14 22:30:00 UTC\nMessages:    2\n\nMessages:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nUser (2023-11-14 22:13:20 UTC)\nExplain Python asyncio\n\nAssistant (2023-11-14 22:13:45 UTC)\nPython asyncio is a library to write concurrent code...\n</code></pre> <p>Output (Summary Display):</p> <pre><code>Conversation: Python AsyncIO Tutorial\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nID:          conv-abc123\nCreated:     2023-11-14 22:13:20 UTC\nUpdated:     2023-11-14 22:30:00 UTC\nMessages:    2\n\nRole Breakdown:\n  user:      1 message\n  assistant: 1 message\n</code></pre> <p>Output (JSON):</p> <pre><code>{\n  \"id\": \"conv-abc123\",\n  \"title\": \"Python AsyncIO Tutorial\",\n  \"created_at\": \"2023-11-14T22:13:20Z\",\n  \"updated_at\": \"2023-11-14T22:30:00Z\",\n  \"message_count\": 2,\n  \"messages\": [\n    {\n      \"id\": \"msg-001-1\",\n      \"role\": \"user\",\n      \"content\": \"Explain Python asyncio\",\n      \"timestamp\": \"2023-11-14T22:13:20Z\",\n      \"parent_id\": null\n    }\n  ]\n}\n</code></pre>"},{"location":"cli-usage/#export","title":"export","text":"<p>Export a specific conversation to markdown, JSON, or CSV format.</p> <p>Usage:</p> <pre><code>echomine export [OPTIONS] FILE_PATH CONVERSATION_ID\n</code></pre> <p>Arguments:</p> <ul> <li><code>FILE_PATH</code>: Path to OpenAI export JSON file (required)</li> <li><code>CONVERSATION_ID</code>: ID of conversation to export (required)</li> </ul> <p>Options:</p> <ul> <li><code>--output PATH</code>: Output file path (if not specified, prints to stdout)</li> <li><code>--format TEXT</code>: Export format: <code>markdown</code> (default), <code>json</code>, or <code>csv</code></li> <li><code>--fields TEXT</code>: CSV only - comma-separated field names (default: all fields)</li> <li><code>--no-metadata</code>: Markdown only - exclude YAML frontmatter (v1.1.0 compatibility)</li> <li><code>--help</code>: Show help message</li> </ul> <p>Examples:</p> <pre><code># Export to stdout (markdown with YAML frontmatter, default in v1.2.0)\nechomine export export.json conv-abc123\n\n# Export to markdown file\nechomine export export.json conv-abc123 --output algorithm.md\n\n# Export without YAML frontmatter (v1.1.0 style)\nechomine export export.json conv-abc123 --output algo.md --no-metadata\n\n# Export as JSON to file\nechomine export export.json conv-abc123 --format json --output algo.json\n\n# Export as CSV (v1.2.0+)\nechomine export export.json conv-abc123 --format csv --output algo.csv\n\n# CSV with specific fields only\nechomine export export.json conv-abc123 --format csv \\\n  --fields \"id,role,content,timestamp\" --output messages.csv\n\n# Export JSON to stdout for piping\nechomine export export.json conv-abc123 -f json | jq '.messages | length'\n\n# Count user messages in a conversation\nechomine export export.json conv-abc123 -f json | jq '[.messages[] | select(.role == \"user\")] | length'\n\n# Pipe markdown to file\nechomine export export.json conv-abc123 &gt; conversation.md\n\n# Export multiple conversations with bash loop\nfor id in conv-abc123 conv-xyz789; do\n  echomine export export.json \"$id\" --output \"${id}.md\"\ndone\n</code></pre> <p>Output (Markdown with YAML Frontmatter - v1.2.0 Default):</p> <pre><code>---\nid: conv-abc123\ntitle: Python Async Best Practices\ncreated_at: 2024-01-15T10:30:00+00:00\nupdated_at: 2024-01-15T12:45:00+00:00\nmessage_count: 42\nexport_date: 2024-12-07T15:30:00+00:00\nexported_by: echomine\n---\n\n# Python Async Best Practices\n\n## User (`msg-001`) - 2024-01-15 10:30:15 UTC\n\nHow do I properly use async/await in Python?\n\n## Assistant (`msg-002`) - 2024-01-15 10:30:45 UTC\n\nHere's a comprehensive guide to async/await in Python...\n</code></pre> <p>Output (Markdown without Metadata - v1.1.0 Style):</p> <pre><code># Python Async Best Practices\n\n**Created:** 2024-01-15 10:30:00 UTC\n**Messages:** 42\n\n---\n\n## Message 1\n\n**User** - 2024-01-15 10:30:15 UTC\n\nHow do I properly use async/await in Python?\n\n---\n\n## Message 2\n\n**Assistant** - 2024-01-15 10:30:45 UTC\n\nHere's a comprehensive guide to async/await in Python...\n</code></pre> <p>Output (JSON):</p> <pre><code>{\n  \"id\": \"conv-abc123\",\n  \"title\": \"Python Async Best Practices\",\n  \"created_at\": \"2024-01-15T10:30:00Z\",\n  \"messages\": [\n    {\n      \"id\": \"msg-001\",\n      \"role\": \"user\",\n      \"content\": \"How do I properly use async/await in Python?\",\n      \"timestamp\": \"2024-01-15T10:30:15Z\"\n    }\n  ]\n}\n</code></pre> <p>Output (CSV - v1.2.0+):</p> <pre><code>id,role,content,timestamp,parent_id\nmsg-001,user,\"How do I properly use async/await in Python?\",2024-01-15T10:30:15Z,\nmsg-002,assistant,\"Here's a comprehensive guide to async/await in Python...\",2024-01-15T10:30:45Z,msg-001\n</code></pre>"},{"location":"cli-usage/#output-formats","title":"Output Formats","text":""},{"location":"cli-usage/#human-readable-output","title":"Human-Readable Output","text":"<p>Default format with rich terminal formatting:</p> <ul> <li>Progress indicators</li> <li>Color-coded output</li> <li>Tables for structured data</li> <li>Formatted timestamps</li> </ul>"},{"location":"cli-usage/#json-output","title":"JSON Output","text":"<p>All commands support <code>--json</code> flag for machine-readable output:</p> <ul> <li>Structured JSON on stdout</li> <li>Progress and errors to stderr</li> <li>Exit codes: 0 (success), 1 (error), 2 (usage error)</li> </ul>"},{"location":"cli-usage/#exit-codes","title":"Exit Codes","text":"<p>Echomine follows standard UNIX exit code conventions:</p> <ul> <li>0: Success</li> <li>1: Operational error (file not found, parsing error, etc.)</li> <li>2: Usage error (invalid arguments, missing required options)</li> </ul> <p>Examples:</p> <pre><code># Success (exit code 0)\nechomine list export.json &amp;&amp; echo \"Success\"\n\n# File not found (exit code 1)\nechomine list nonexistent.json || echo \"Error: $?\"\n\n# Invalid arguments (exit code 2)\nechomine search --invalid-option || echo \"Usage error: $?\"\n</code></pre>"},{"location":"cli-usage/#pipeline-integration","title":"Pipeline Integration","text":"<p>Echomine is designed for UNIX pipeline composition:</p>"},{"location":"cli-usage/#with-jq","title":"With jq","text":"<pre><code># Extract conversation titles\nechomine list export.json --json | jq '.conversations[].title'\n\n# Filter by message count\nechomine list export.json --json | \\\n  jq '.conversations[] | select(.message_count &gt; 20)'\n\n# Get conversation IDs\nechomine search export.json --keywords \"python\" --json | \\\n  jq -r '.results[].conversation.id'\n</code></pre>"},{"location":"cli-usage/#with-grep","title":"With grep","text":"<pre><code># Search titles\nechomine list export.json | grep -i \"python\"\n\n# Filter results\nechomine search export.json --keywords \"algorithm\" | grep \"Messages:\"\n</code></pre>"},{"location":"cli-usage/#with-awk","title":"With awk","text":"<pre><code># Extract specific fields\nechomine list export.json | awk '/Messages:/ {print $2}'\n</code></pre>"},{"location":"cli-usage/#batch-processing","title":"Batch Processing","text":"<pre><code># Export all search results\nechomine search export.json --keywords \"python\" --json | \\\n  jq -r '.results[].conversation.id' | \\\n  while read -r id; do\n    echomine export export.json \"$id\" --output \"${id}.md\"\n  done\n\n# Count conversations by date\nechomine list export.json --json | \\\n  jq -r '.conversations[].created_at' | \\\n  cut -d'T' -f1 | \\\n  sort | uniq -c\n</code></pre>"},{"location":"cli-usage/#progress-and-error-reporting","title":"Progress and Error Reporting","text":""},{"location":"cli-usage/#progress-indicators","title":"Progress Indicators","text":"<p>Long-running operations show progress to stderr:</p> <pre><code>echomine search large_export.json --keywords \"python\"\n# stderr: Processing conversations... 1000/10000 (10%)\n# stdout: [results]\n</code></pre>"},{"location":"cli-usage/#error-messages","title":"Error Messages","text":"<p>Errors are printed to stderr with context:</p> <pre><code>echomine list nonexistent.json\n# stderr: Error: File not found: nonexistent.json\n# exit code: 1\n</code></pre>"},{"location":"cli-usage/#graceful-degradation","title":"Graceful Degradation","text":"<p>Malformed entries are skipped with warnings:</p> <pre><code>echomine list export.json\n# stderr: Warning: Skipped malformed conversation: conv-broken-123 (invalid timestamp)\n# stdout: [remaining valid conversations]\n</code></pre>"},{"location":"cli-usage/#environment-variables","title":"Environment Variables","text":"<p>None currently. All configuration is via command-line flags.</p>"},{"location":"cli-usage/#configuration-files","title":"Configuration Files","text":"<p>None currently. All options are passed as command-line arguments.</p>"},{"location":"cli-usage/#tips-and-tricks","title":"Tips and Tricks","text":""},{"location":"cli-usage/#1-save-search-results","title":"1. Save Search Results","text":"<pre><code># Save high-relevance results\nechomine search export.json --keywords \"machine learning\" --json &gt; ml_convs.json\n</code></pre>"},{"location":"cli-usage/#2-quick-title-search","title":"2. Quick Title Search","text":"<pre><code># Faster than full-text search\nechomine search export.json --title \"Project Alpha\"\n</code></pre>"},{"location":"cli-usage/#3-date-range-filtering","title":"3. Date Range Filtering","text":"<pre><code># Q1 2024 conversations\nechomine search export.json \\\n  --from-date \"2024-01-01\" \\\n  --to-date \"2024-03-31\" \\\n  --json &gt; q1_2024.json\n</code></pre>"},{"location":"cli-usage/#4-batch-export-with-filtering","title":"4. Batch Export with Filtering","text":"<pre><code># Export all Python-related conversations\nechomine search export.json --keywords \"python\" --limit 100 --json | \\\n  jq -r '.results[].conversation.id' | \\\n  while read id; do\n    echomine export export.json \"$id\" --output \"exports/${id}.md\"\n  done\n</code></pre>"},{"location":"cli-usage/#5-statistics","title":"5. Statistics","text":"<pre><code># Count conversations\nechomine list export.json --json | jq '.total'\n\n# Average messages per conversation\nechomine list export.json --json | \\\n  jq '[.conversations[].message_count] | add / length'\n</code></pre>"},{"location":"cli-usage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli-usage/#command-not-found","title":"Command Not Found","text":"<pre><code># Ensure installed\npip install echomine\n\n# Check PATH\nwhich echomine\n</code></pre>"},{"location":"cli-usage/#file-not-found","title":"File Not Found","text":"<pre><code># Use absolute path\nechomine list /path/to/export.json\n\n# Or check file exists\nls -la export.json\n</code></pre>"},{"location":"cli-usage/#no-results","title":"No Results","text":"<pre><code># Check if file has conversations\nechomine list export.json --json | jq '.total'\n\n# Try broader keywords\nechomine search export.json --keywords \"python\"\n</code></pre>"},{"location":"cli-usage/#invalid-json","title":"Invalid JSON","text":"<pre><code># Validate JSON file\njq empty export.json\n\n# Check for corruption\nechomine list export.json  # Will report parsing errors\n</code></pre>"},{"location":"cli-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Library Usage: Use Echomine programmatically</li> <li>API Reference: Detailed API documentation</li> <li>Architecture: Design principles</li> <li>Contributing: Development guidelines</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to Echomine!</p> <p>For comprehensive development guidelines, please see:</p> <p>CONTRIBUTING.md</p>"},{"location":"contributing/#quick-links","title":"Quick Links","text":""},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Clone repository: <code>git clone https://github.com/echomine/echomine.git</code></li> <li>Install dependencies: <code>pip install -e \".[dev]\"</code></li> <li>Install hooks: <code>pre-commit install</code></li> <li>Run tests: <code>pytest</code></li> </ol>"},{"location":"contributing/#key-guidelines","title":"Key Guidelines","text":"<ul> <li>TDD: Write tests first (RED-GREEN-REFACTOR)</li> <li>Type Safety: mypy --strict must pass</li> <li>Code Quality: ruff for linting/formatting</li> <li>Test Coverage: 80% minimum, 95% for public API</li> </ul>"},{"location":"contributing/#common-tasks","title":"Common Tasks","text":"<pre><code># Run tests with coverage\npytest --cov=echomine --cov-report=term-missing\n\n# Type checking\nmypy --strict src/echomine/\n\n# Linting and formatting\nruff check --fix src/ tests/\nruff format src/ tests/\n\n# Pre-commit hooks\npre-commit run --all-files\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create feature branch</li> <li>Write failing tests (RED)</li> <li>Implement feature (GREEN)</li> <li>Refactor (REFACTOR)</li> <li>Ensure all quality checks pass</li> <li>Submit PR with conventional commit message</li> </ol>"},{"location":"contributing/#architecture-principles","title":"Architecture Principles","text":"<ol> <li>Library-First: Core in library, CLI wraps it</li> <li>Type Safety: mypy --strict, no <code>Any</code> types</li> <li>Memory Efficiency: Streaming, O(1) memory</li> <li>YAGNI: Implement only spec requirements</li> </ol>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Docs: Full Documentation</li> </ul> <p>For complete details, see CONTRIBUTING.md.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12 or higher</li> <li>8GB RAM (for processing large exports)</li> </ul>"},{"location":"installation/#install-from-pypi","title":"Install from PyPI","text":"<p>When published, install using pip:</p> <pre><code>pip install echomine\n</code></pre>"},{"location":"installation/#install-from-source","title":"Install from Source","text":""},{"location":"installation/#clone-repository","title":"Clone Repository","text":"<pre><code>git clone https://github.com/echomine/echomine.git\ncd echomine\n</code></pre>"},{"location":"installation/#basic-installation","title":"Basic Installation","text":"<p>For library use only:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>With all development dependencies (testing, linting, type checking):</p> <pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<pre><code># Check CLI works\nechomine --version\n\n# Run tests\npytest\n\n# Check type checking\nmypy --strict src/echomine/\n</code></pre>"},{"location":"installation/#pre-commit-hooks-optional","title":"Pre-commit Hooks (Optional)","text":"<p>Install pre-commit hooks for automatic code quality checks:</p> <pre><code>pre-commit install\n</code></pre> <p>This will run: - Type checking (mypy --strict) - Linting and formatting (ruff) - Tests (pytest)</p> <p>On every commit.</p>"},{"location":"installation/#dependencies","title":"Dependencies","text":""},{"location":"installation/#core-dependencies","title":"Core Dependencies","text":"<ul> <li>pydantic (&gt;=2.6.0): Data validation and type safety</li> <li>ijson (&gt;=3.2.0): Streaming JSON parser</li> <li>typer (&gt;=0.9.0): CLI framework</li> <li>rich (&gt;=13.0.0): Terminal formatting</li> <li>structlog (&gt;=23.0.0): Structured logging</li> <li>python-slugify (&gt;=8.0.0): Text slugification</li> <li>python-dateutil (&gt;=2.8.0): Date parsing utilities</li> </ul>"},{"location":"installation/#development-dependencies","title":"Development Dependencies","text":"<ul> <li>pytest (&gt;=7.4.0): Testing framework</li> <li>pytest-cov (&gt;=4.1.0): Coverage reporting</li> <li>pytest-mock (&gt;=3.11.0): Mocking utilities</li> <li>pytest-benchmark (&gt;=4.0.0): Performance benchmarks</li> <li>psutil (&gt;=5.9.0): Resource monitoring</li> <li>mypy (&gt;=1.5.0): Static type checker</li> <li>ruff (&gt;=0.1.0): Linter and formatter</li> <li>pre-commit (&gt;=3.4.0): Git hooks</li> </ul>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#python-version-issues","title":"Python Version Issues","text":"<p>Echomine requires Python 3.12+ for modern type hints. Check your version:</p> <pre><code>python --version  # Should show 3.12 or higher\n</code></pre>"},{"location":"installation/#import-errors","title":"Import Errors","text":"<p>If you see import errors after installation:</p> <pre><code># Reinstall in development mode\npip install -e .\n\n# Or with full dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#type-checking-errors","title":"Type Checking Errors","text":"<p>Ensure mypy is installed and configured:</p> <pre><code>pip install mypy\nmypy --strict src/echomine/\n</code></pre>"},{"location":"installation/#upgrading","title":"Upgrading","text":""},{"location":"installation/#from-pypi","title":"From PyPI","text":"<pre><code>pip install --upgrade echomine\n</code></pre>"},{"location":"installation/#from-source","title":"From Source","text":"<pre><code>cd echomine\ngit pull origin master\npip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#uninstalling","title":"Uninstalling","text":"<pre><code>pip uninstall echomine\n</code></pre>"},{"location":"library-usage/","title":"Library Usage Guide","text":"<p>This guide covers comprehensive usage of Echomine as a Python library. Perfect for integrating with tools like cognivault or building custom analysis workflows.</p>"},{"location":"library-usage/#installation","title":"Installation","text":"<pre><code>pip install echomine\n</code></pre> <p>See Installation for details.</p>"},{"location":"library-usage/#core-concepts","title":"Core Concepts","text":""},{"location":"library-usage/#library-first-architecture","title":"Library-First Architecture","text":"<p>Echomine is designed as a library first, with the CLI built on top. All functionality is available programmatically:</p> <pre><code>from echomine import OpenAIAdapter\nfrom echomine.models import SearchQuery\n\n# Core library components\nadapter = OpenAIAdapter()          # Stateless adapter\nquery = SearchQuery(keywords=[\"python\"])  # Type-safe query model\n\n# Use in your application\nfor result in adapter.search(file_path, query):\n    process(result.conversation)\n</code></pre>"},{"location":"library-usage/#stateless-adapters","title":"Stateless Adapters","text":"<p>Adapters have no <code>__init__</code> parameters and maintain no internal state:</p> <pre><code># Reusable across different files\nadapter = OpenAIAdapter()\n\nfor file in export_files:\n    for conv in adapter.stream_conversations(file):\n        process(conv)\n</code></pre>"},{"location":"library-usage/#streaming-operations","title":"Streaming Operations","text":"<p>All operations use generators for O(1) memory usage:</p> <pre><code># Handles 1GB+ files with constant memory\nfor conversation in adapter.stream_conversations(large_file):\n    # Process one at a time\n    analyze(conversation)\n</code></pre>"},{"location":"library-usage/#basic-operations","title":"Basic Operations","text":""},{"location":"library-usage/#stream-all-conversations","title":"Stream All Conversations","text":"<p>Memory-efficient iteration over all conversations:</p> <pre><code>from echomine import OpenAIAdapter\nfrom pathlib import Path\n\nadapter = OpenAIAdapter()\nexport_file = Path(\"conversations.json\")\n\nfor conversation in adapter.stream_conversations(export_file):\n    print(f\"{conversation.title} ({conversation.created_at})\")\n    print(f\"  Messages: {len(conversation.messages)}\")\n</code></pre>"},{"location":"library-usage/#search-with-keywords","title":"Search with Keywords","text":"<p>Find conversations matching specific keywords with BM25 ranking:</p> <pre><code>from echomine.models import SearchQuery\n\nquery = SearchQuery(\n    keywords=[\"algorithm\", \"leetcode\"],\n    limit=10\n)\n\nfor result in adapter.search(export_file, query):\n    print(f\"[{result.score:.2f}] {result.conversation.title}\")\n    print(f\"  Preview: {result.snippet}\")  # v1.1.0: automatic snippets\n    print(f\"  Matches: {len(result.matched_message_ids)} messages\")\n</code></pre>"},{"location":"library-usage/#understanding-filter-combinations","title":"Understanding Filter Combinations","text":"<p>Search filters use a two-stage process:</p> <p>Stage 1: Content Matching (OR relationship) - Phrases: ANY phrase matches (exact, case-insensitive) - Keywords: Match according to <code>match_mode</code>   - <code>match_mode=\"any\"</code> (default): ANY keyword matches   - <code>match_mode=\"all\"</code>: ALL keywords must be present</p> <p>If you specify both phrases and keywords, a conversation matches if EITHER a phrase matches OR keywords match (they are alternatives, not cumulative).</p> <p>Stage 2: Post-Match Filters (AND relationship) - <code>exclude_keywords</code>: Removes results containing ANY excluded term - <code>role_filter</code>: Only searches messages from specified role - <code>title_filter</code>: Only includes conversations with matching title - <code>from_date</code> / <code>to_date</code>: Only includes conversations in date range</p> <p>Examples:</p> <pre><code># Phrase OR keyword (matches conversations with \"api\" phrase OR \"python\" keyword)\nquery = SearchQuery(phrases=[\"api\"], keywords=[\"python\"])\n\n# Multiple keywords with ALL mode (requires both \"python\" AND \"async\")\nquery = SearchQuery(keywords=[\"python\", \"async\"], match_mode=\"all\")\n\n# Content matching + exclusion (phrase OR keyword, then exclude \"java\")\nquery = SearchQuery(\n    phrases=[\"api\"],\n    keywords=[\"python\"],\n    exclude_keywords=[\"java\"]\n)\n\n# Role-specific search (only search user messages)\nquery = SearchQuery(keywords=[\"python\"], role_filter=\"user\")\n\n# Complex combination\nquery = SearchQuery(\n    phrases=[\"algo-insights\"],\n    keywords=[\"refactor\"],\n    exclude_keywords=[\"test\", \"documentation\"],\n    role_filter=\"user\",\n    title_filter=\"Project\",\n    match_mode=\"any\"  # Only affects keywords when multiple specified\n)\n</code></pre>"},{"location":"library-usage/#filter-by-title","title":"Filter by Title","text":"<p>Fast metadata-only search:</p> <pre><code>query = SearchQuery(\n    title_filter=\"Project\",  # Partial match, case-insensitive\n    limit=10\n)\n\nfor result in adapter.search(export_file, query):\n    print(result.conversation.title)\n</code></pre>"},{"location":"library-usage/#filter-by-date-range","title":"Filter by Date Range","text":"<p>Narrow down conversations by creation date:</p> <pre><code>from datetime import date\n\nquery = SearchQuery(\n    from_date=date(2024, 1, 1),\n    to_date=date(2024, 3, 31),\n    keywords=[\"refactor\"],\n    limit=5\n)\n\nfor result in adapter.search(export_file, query):\n    print(f\"{result.conversation.title} - {result.conversation.created_at}\")\n</code></pre>"},{"location":"library-usage/#get-conversation-by-id","title":"Get Conversation by ID","text":"<p>Retrieve a specific conversation:</p> <pre><code>conversation = adapter.get_conversation_by_id(export_file, \"conv-abc123\")\n\nif conversation:\n    print(f\"Found: {conversation.title}\")\nelse:\n    print(\"Conversation not found\")\n</code></pre>"},{"location":"library-usage/#calculate-statistics-v120","title":"Calculate Statistics (v1.2.0+)","text":"<p>Get comprehensive statistics about your export:</p> <pre><code>from echomine import calculate_statistics, calculate_conversation_statistics\n\n# Export-level statistics (streaming, O(1) memory)\nstats = calculate_statistics(export_file)\n\nprint(f\"Total conversations: {stats.total_conversations}\")\nprint(f\"Total messages: {stats.total_messages}\")\nprint(f\"Date range: {stats.date_range.earliest} to {stats.date_range.latest}\")\nprint(f\"Average messages: {stats.average_messages:.1f}\")\n\n# Largest and smallest conversations\nif stats.largest_conversation:\n    largest = stats.largest_conversation\n    print(f\"Largest: {largest.title} ({largest.message_count} messages)\")\n\nif stats.smallest_conversation:\n    smallest = stats.smallest_conversation\n    print(f\"Smallest: {smallest.title} ({smallest.message_count} messages)\")\n\n# Per-conversation statistics\nconversation = adapter.get_conversation_by_id(export_file, \"conv-abc123\")\nif conversation:\n    conv_stats = calculate_conversation_statistics(conversation)\n\n    # Message counts by role\n    print(f\"User messages: {conv_stats.message_count_by_role.user}\")\n    print(f\"Assistant messages: {conv_stats.message_count_by_role.assistant}\")\n    print(f\"System messages: {conv_stats.message_count_by_role.system}\")\n\n    # Temporal patterns\n    print(f\"Duration: {conv_stats.duration_seconds:.0f} seconds\")\n    if conv_stats.average_gap_seconds:\n        print(f\"Average gap: {conv_stats.average_gap_seconds:.1f} seconds\")\n</code></pre>"},{"location":"library-usage/#advanced-search-features-v110","title":"Advanced Search Features (v1.1.0+)","text":"<p>Version 1.1.0 introduces five powerful search enhancements for the library API:</p>"},{"location":"library-usage/#1-exact-phrase-matching","title":"1. Exact Phrase Matching","text":"<p>Search for exact multi-word phrases while preserving special characters:</p> <pre><code>from echomine.models import SearchQuery\n\n# Single phrase\nquery = SearchQuery(phrases=[\"algo-insights\"])\nfor result in adapter.search(export_file, query):\n    print(f\"{result.conversation.title}: {result.snippet}\")\n\n# Multiple phrases (OR logic - matches any)\nquery = SearchQuery(phrases=[\"algo-insights\", \"data pipeline\", \"api design\"])\nfor result in adapter.search(export_file, query):\n    print(f\"[{result.score:.2f}] {result.conversation.title}\")\n\n# Combine phrases and keywords\nquery = SearchQuery(\n    phrases=[\"algo-insights\"],\n    keywords=[\"optimization\", \"performance\"]\n)\nresults = list(adapter.search(export_file, query))\n</code></pre> <p>Use Cases: - Project-specific terminology with special characters - Code patterns like \"async/await\", \"error-handling\" - Multi-word concepts that must appear together</p>"},{"location":"library-usage/#2-boolean-match-mode","title":"2. Boolean Match Mode","text":"<p>Control keyword matching logic with AND or OR:</p> <pre><code># Require ALL keywords (AND logic)\nquery = SearchQuery(\n    keywords=[\"python\", \"async\", \"testing\"],\n    match_mode=\"all\"  # All three keywords must be present\n)\nfor result in adapter.search(export_file, query):\n    print(f\"Contains ALL keywords: {result.conversation.title}\")\n\n# Default: ANY keyword matches (OR logic)\nquery = SearchQuery(\n    keywords=[\"python\", \"javascript\", \"rust\"],\n    match_mode=\"any\"  # At least one keyword present (default)\n)\nfor result in adapter.search(export_file, query):\n    print(f\"Contains ANY keyword: {result.conversation.title}\")\n\n# Compare results\nquery_all = SearchQuery(keywords=[\"python\", \"async\"], match_mode=\"all\")\nquery_any = SearchQuery(keywords=[\"python\", \"async\"], match_mode=\"any\")\n\nresults_all = list(adapter.search(export_file, query_all))\nresults_any = list(adapter.search(export_file, query_any))\n\nprint(f\"AND mode: {len(results_all)} results\")\nprint(f\"OR mode: {len(results_any)} results\")\n</code></pre> <p>Use Cases: - Narrow results: Find conversations covering multiple topics - Broad discovery: Cast wide net across related keywords - Topic intersection: Require specific keyword combinations</p> <p>Note: <code>match_mode</code> only affects keywords. Phrases always use OR logic.</p>"},{"location":"library-usage/#3-exclude-keywords","title":"3. Exclude Keywords","text":"<p>Filter out unwanted results containing specific terms:</p> <pre><code># Exclude single term\nquery = SearchQuery(\n    keywords=[\"python\"],\n    exclude_keywords=[\"django\"]\n)\nfor result in adapter.search(export_file, query):\n    print(result.conversation.title)\n    # Guaranteed: no results contain \"django\"\n\n# Exclude multiple terms (OR logic - excludes if ANY present)\nquery = SearchQuery(\n    keywords=[\"python\"],\n    exclude_keywords=[\"django\", \"flask\", \"pyramid\"]\n)\nfor result in adapter.search(export_file, query):\n    print(result.conversation.title)\n    # None of these contain django, flask, or pyramid\n\n# Combine with other filters\nquery = SearchQuery(\n    keywords=[\"refactor\", \"optimization\"],\n    exclude_keywords=[\"test\", \"example\", \"tutorial\"],\n    match_mode=\"all\"\n)\nresults = list(adapter.search(export_file, query))\n</code></pre> <p>Use Cases: - Remove noise: Exclude \"test\", \"example\", \"deprecated\" - Filter frameworks: Search Python without specific frameworks - Avoid unrelated topics: Exclude terms that pollute results</p> <p>Note: Excluded terms use OR logic - a result is removed if it contains ANY excluded term.</p>"},{"location":"library-usage/#4-role-filtering","title":"4. Role Filtering","text":"<p>Search only messages from specific author roles:</p> <pre><code># Search only YOUR questions\nquery = SearchQuery(\n    keywords=[\"how do I\", \"refactor\", \"optimize\"],\n    role_filter=\"user\"\n)\nfor result in adapter.search(export_file, query):\n    print(f\"You asked: {result.snippet}\")\n\n# Search only AI responses\nquery = SearchQuery(\n    keywords=[\"recommend\", \"suggest\", \"best practice\"],\n    role_filter=\"assistant\"\n)\nfor result in adapter.search(export_file, query):\n    print(f\"AI suggested: {result.snippet}\")\n\n# Search system messages\nquery = SearchQuery(\n    keywords=[\"context\", \"instructions\"],\n    role_filter=\"system\"\n)\nfor result in adapter.search(export_file, query):\n    print(f\"System: {result.snippet}\")\n\n# Compare user vs assistant content\nuser_query = SearchQuery(keywords=[\"algorithm\"], role_filter=\"user\")\nassistant_query = SearchQuery(keywords=[\"algorithm\"], role_filter=\"assistant\")\n\nuser_results = list(adapter.search(export_file, user_query))\nassistant_results = list(adapter.search(export_file, assistant_query))\n\nprint(f\"You mentioned 'algorithm' in {len(user_results)} conversations\")\nprint(f\"AI mentioned 'algorithm' in {len(assistant_results)} conversations\")\n</code></pre> <p>Use Cases: - Find your questions: <code>role_filter=\"user\"</code> - Find AI recommendations: <code>role_filter=\"assistant\"</code> - Analyze system prompts: <code>role_filter=\"system\"</code> - Compare user vs AI language patterns</p> <p>Valid Roles: <code>\"user\"</code>, <code>\"assistant\"</code>, <code>\"system\"</code> (case-insensitive)</p>"},{"location":"library-usage/#5-message-snippets-automatic","title":"5. Message Snippets (Automatic)","text":"<p>All search results automatically include message previews:</p> <pre><code>query = SearchQuery(keywords=[\"algorithm\"])\n\nfor result in adapter.search(export_file, query):\n    # v1.1.0: snippet field always present\n    print(f\"Title: {result.conversation.title}\")\n    print(f\"Score: {result.score:.2f}\")\n    print(f\"Preview: {result.snippet}\")\n    print(f\"Matched messages: {len(result.matched_message_ids)}\")\n    print(f\"Message IDs: {result.matched_message_ids[:3]}\")  # First 3\n    print(\"---\")\n</code></pre> <p>Snippet Features: - ~100 character preview from first matching message - Truncated with \"...\" for long content - Multiple matches indicated in <code>matched_message_ids</code> - Fallback text for empty/malformed content - Always present (never None)</p> <p>Working with Matched Messages:</p> <pre><code>from echomine import OpenAIAdapter, SearchQuery\n\nadapter = OpenAIAdapter()\nquery = SearchQuery(keywords=[\"refactor\"])\n\nfor result in adapter.search(export_file, query):\n    conversation = result.conversation\n    matched_ids = result.matched_message_ids\n\n    # Find the actual matched messages\n    matched_messages = [\n        msg for msg in conversation.messages\n        if msg.id in matched_ids\n    ]\n\n    print(f\"Conversation: {conversation.title}\")\n    print(f\"Matched {len(matched_messages)} messages:\")\n    for msg in matched_messages:\n        print(f\"  [{msg.role}] {msg.content[:80]}...\")\n</code></pre>"},{"location":"library-usage/#combining-advanced-features","title":"Combining Advanced Features","text":"<p>All features work together for powerful precision searches:</p> <pre><code>from datetime import date\n\n# Complex query combining all v1.1.0 features\nquery = SearchQuery(\n    # Content matching (Stage 1: OR relationship)\n    keywords=[\"python\", \"optimization\"],\n    phrases=[\"algo-insights\"],\n    match_mode=\"all\",  # Only affects keywords\n\n    # Post-match filters (Stage 2: AND relationship)\n    exclude_keywords=[\"test\", \"documentation\"],\n    role_filter=\"user\",\n    title_filter=\"Project\",\n    from_date=date(2024, 1, 1),\n    to_date=date(2024, 12, 31),\n\n    # Output control\n    limit=10\n)\n\nfor result in adapter.search(export_file, query):\n    print(f\"[{result.score:.2f}] {result.conversation.title}\")\n    print(f\"  Created: {result.conversation.created_at.date()}\")\n    print(f\"  Snippet: {result.snippet}\")\n    print(f\"  Matches: {len(result.matched_message_ids)} messages\")\n</code></pre>"},{"location":"library-usage/#filter-combination-logic","title":"Filter Combination Logic","text":"<p>Understanding how filters interact is crucial:</p> <p>Stage 1: Content Matching (OR relationship) - Phrases: Match if ANY phrase is found (exact, case-insensitive) - Keywords: Match according to <code>match_mode</code>   - <code>match_mode=\"any\"</code> (default): Match if ANY keyword present   - <code>match_mode=\"all\"</code>: Match if ALL keywords present - Key insight: Phrases OR keywords (not both required)</p> <p>Stage 2: Post-Match Filters (AND relationship) - <code>exclude_keywords</code>: Remove if ANY excluded term found - <code>role_filter</code>: Only messages from specified role - <code>title_filter</code>: Only conversations with matching title - <code>from_date</code> / <code>to_date</code>: Only in date range - All must be satisfied</p> <p>Examples:</p> <pre><code># Phrase OR keyword (matches either)\nquery = SearchQuery(phrases=[\"api\"], keywords=[\"python\"])\n# Matches: conversations with \"api\" phrase OR \"python\" keyword\n\n# Multiple keywords with ALL mode\nquery = SearchQuery(keywords=[\"python\", \"async\"], match_mode=\"all\")\n# Matches: conversations with BOTH \"python\" AND \"async\"\n\n# Content + exclusion\nquery = SearchQuery(\n    phrases=[\"api\"],\n    keywords=[\"python\"],\n    exclude_keywords=[\"java\"]\n)\n# Matches: (\"api\" phrase OR \"python\" keyword) AND NOT \"java\"\n\n# Role-specific search\nquery = SearchQuery(keywords=[\"python\"], role_filter=\"user\")\n# Matches: \"python\" in user messages only\n\n# Complex combination\nquery = SearchQuery(\n    phrases=[\"algo-insights\"],\n    keywords=[\"refactor\"],\n    exclude_keywords=[\"test\", \"docs\"],\n    role_filter=\"user\",\n    title_filter=\"Project\",\n    match_mode=\"any\"\n)\n# Matches: (\"algo-insights\" phrase OR \"refactor\" keyword) in user messages\n#          in conversations titled \"Project\" WITHOUT \"test\" or \"docs\"\n</code></pre>"},{"location":"library-usage/#advanced-usage","title":"Advanced Usage","text":""},{"location":"library-usage/#message-tree-navigation","title":"Message Tree Navigation","text":"<p>Conversations can have branching message trees (e.g., regenerated AI responses). Helper methods to navigate:</p> <pre><code>conversation = adapter.get_conversation_by_id(export_file, \"conv-abc123\")\n\n# Get all threads (root-to-leaf paths)\nthreads = conversation.get_all_threads()\n\nprint(f\"Conversation has {len(threads)} branches:\")\nfor i, thread in enumerate(threads, 1):\n    print(f\"\\nThread {i} ({len(thread)} messages):\")\n    for msg in thread:\n        print(f\"  {msg.role}: {msg.content[:50]}...\")\n\n# Get specific thread by leaf message ID\nthread = conversation.get_thread(\"msg-xyz-789\")\n\n# Get root messages\nroots = conversation.get_root_messages()\n\n# Get children of a message\nchildren = conversation.get_children(\"msg-abc-123\")\n\n# Check if message has children\nhas_branches = conversation.has_children(\"msg-abc-123\")\n</code></pre>"},{"location":"library-usage/#data-validation-and-immutability","title":"Data Validation and Immutability","text":"<p>All models use Pydantic with strict validation and immutability:</p> <pre><code>from pydantic import ValidationError\n\n# Models are frozen (immutable)\ntry:\n    conversation.title = \"New Title\"  # Raises ValidationError\nexcept ValidationError as e:\n    print(f\"Error: {e}\")\n\n# Create modified copy instead\nupdated = conversation.model_copy(update={\"title\": \"New Title\"})\nprint(f\"Original: {conversation.title}\")\nprint(f\"Updated: {updated.title}\")\n</code></pre>"},{"location":"library-usage/#timezone-aware-timestamps","title":"Timezone-Aware Timestamps","text":"<p>All timestamps are timezone-aware UTC datetimes:</p> <pre><code>from datetime import timezone\n\nfor message in conversation.messages:\n    # All timestamps guaranteed to be UTC\n    assert message.timestamp.tzinfo == timezone.utc\n\n    # Safe to compare and serialize\n    print(f\"{message.timestamp.isoformat()}: {message.content[:30]}...\")\n\n# Convert to local timezone for display\nimport datetime\nlocal_tz = datetime.datetime.now().astimezone().tzinfo\nfor msg in conversation.messages:\n    local_time = msg.timestamp.astimezone(local_tz)\n    print(f\"[{local_time}] {msg.role}: {msg.content[:30]}...\")\n</code></pre>"},{"location":"library-usage/#role-normalization","title":"Role Normalization","text":"<p>Message roles are normalized to standard values:</p> <pre><code># Message.role is Literal[\"user\", \"assistant\", \"system\"]\nfor message in conversation.messages:\n    if message.role == \"user\":\n        print(f\"User: {message.content}\")\n    elif message.role == \"assistant\":\n        print(f\"AI: {message.content}\")\n    elif message.role == \"system\":\n        print(f\"System: {message.content}\")\n    # No other values possible - type safety guaranteed!\n</code></pre>"},{"location":"library-usage/#error-handling","title":"Error Handling","text":""},{"location":"library-usage/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>All library operational errors inherit from <code>EchomineError</code>:</p> <pre><code>from echomine import (\n    OpenAIAdapter,\n    EchomineError,      # Base exception\n    ParseError,         # Malformed JSON/structure\n    ValidationError,    # Pydantic validation failures\n    SchemaVersionError  # Unsupported schema version\n)\n</code></pre>"},{"location":"library-usage/#fail-fast-vs-skip-malformed-strategy","title":"Fail-Fast vs Skip-Malformed Strategy","text":"<p>Echomine distinguishes between operational errors (fail-fast) and data quality issues (skip-malformed):</p> <p>Fail-Fast: Operational Errors (raises exceptions) - JSON syntax errors: Completely malformed file structure - File access errors: Missing files, permission denied, disk errors - Unsupported schema version: Export format version mismatch</p> <p>These errors indicate problems with the export file itself or the environment. Processing cannot continue safely, so exceptions are raised immediately.</p> <p>Skip-Malformed: Data Quality Issues (log warning, continue processing)</p> <p>The library categorizes malformed entries into three types (per FR-264):</p> <ol> <li>JSON Syntax Errors (Structural)</li> <li>Completely malformed conversation objects within valid array</li> <li>Example: Truncated objects, unescaped quotes, invalid nesting</li> <li> <p>Handling: Skip conversation, log JSON parse error</p> </li> <li> <p>Schema Violations (Missing Required Fields)</p> </li> <li>Conversations missing required fields: <code>id</code>, <code>title</code>, <code>create_time</code></li> <li>Messages missing required fields: <code>id</code>, <code>author.role</code>, <code>content</code></li> <li>Example: <code>{\"title\": \"Test\"}</code> (missing id, create_time)</li> <li> <p>Handling: Skip conversation, log \"missing field: {field_name}\"</p> </li> <li> <p>Validation Failures (Invalid Field Values)</p> </li> <li>Fields present but values violate type/format constraints</li> <li>Examples: Non-UTC timestamps, invalid role values, negative timestamps</li> <li>Handling: Skip conversation, log \"invalid {field}: {reason}\"</li> </ol> <p>For all three categories, the library: 1. Logs WARNING with conversation ID and category-specific error message 2. Invokes <code>on_skip</code> callback if provided (with conversation ID and reason) 3. Continues processing remaining conversations 4. Returns partial results (graceful degradation)</p> <p>This strategy ensures maximum data recovery while maintaining safety for operational errors.</p>"},{"location":"library-usage/#recommended-error-handling-pattern","title":"Recommended Error Handling Pattern","text":"<p>For library consumers (e.g., cognivault integration):</p> <pre><code>from echomine import OpenAIAdapter, EchomineError\nimport structlog\n\nlogger = structlog.get_logger()\n\ntry:\n    adapter = OpenAIAdapter()\n    for conversation in adapter.stream_conversations(export_file):\n        knowledge_base.ingest(conversation)\n\nexcept EchomineError as e:\n    # All library operational errors\n    logger.error(\"echomine_parsing_failed\", error=str(e))\n    # Handle gracefully: notify user, log error, skip file\n\nexcept (FileNotFoundError, PermissionError) as e:\n    # Filesystem errors (not wrapped by library)\n    logger.error(\"file_access_failed\", error=str(e))\n    # Handle: check permissions, verify path\n\nexcept Exception as e:\n    # Unexpected errors (library bugs or system issues)\n    logger.exception(\"unexpected_error\", error=str(e))\n    raise  # Re-raise to surface bugs\n</code></pre>"},{"location":"library-usage/#specific-exception-types","title":"Specific Exception Types","text":"<pre><code># ParseError (malformed export)\nfrom echomine import ParseError\n\ntry:\n    for conv in adapter.stream_conversations(export_file):\n        process(conv)\nexcept ParseError as e:\n    print(f\"Export file corrupted: {e}\")\n\n# ValidationError (invalid data)\nfrom echomine import ValidationError\n\ntry:\n    results = adapter.search(export_file, query)\nexcept ValidationError as e:\n    print(f\"Invalid query or data: {e}\")\n\n# SchemaVersionError (unsupported version)\nfrom echomine import SchemaVersionError\n\ntry:\n    for conv in adapter.stream_conversations(export_file):\n        process(conv)\nexcept SchemaVersionError as e:\n    print(f\"Unsupported export version: {e}\")\n</code></pre>"},{"location":"library-usage/#progress-reporting","title":"Progress Reporting","text":""},{"location":"library-usage/#custom-progress-callback","title":"Custom Progress Callback","text":"<p>Implement custom progress handlers:</p> <pre><code>def my_progress_handler(count: int) -&gt; None:\n    \"\"\"Custom progress callback for UI or logging.\"\"\"\n    if count % 100 == 0:\n        print(f\"Processed {count:,} conversations...\")\n\nadapter = OpenAIAdapter()\nfor conversation in adapter.stream_conversations(\n    Path(\"large_export.json\"),\n    progress_callback=my_progress_handler\n):\n    knowledge_base.ingest(conversation)\n\nprint(\"Ingestion complete!\")\n</code></pre>"},{"location":"library-usage/#graceful-degradation","title":"Graceful Degradation","text":"<p>Track malformed entries that were skipped:</p> <pre><code>skipped_entries = []\n\ndef handle_skipped(conversation_id: str, reason: str) -&gt; None:\n    \"\"\"Called when malformed entry is skipped.\"\"\"\n    skipped_entries.append({\n        \"id\": conversation_id,\n        \"reason\": reason,\n    })\n    logger.warning(\"conversation_skipped\", conv_id=conversation_id, reason=reason)\n\nfor conv in adapter.stream_conversations(export_file, on_skip=handle_skipped):\n    process(conv)\n\nif skipped_entries:\n    print(f\"Skipped {len(skipped_entries)} conversations\")\n</code></pre>"},{"location":"library-usage/#concurrency","title":"Concurrency","text":""},{"location":"library-usage/#multi-process-concurrent-reads-safe","title":"Multi-Process Concurrent Reads (Safe)","text":"<p>Multiple processes can read the same file:</p> <pre><code>from multiprocessing import Process\nfrom echomine import OpenAIAdapter\nfrom pathlib import Path\n\ndef worker_process(export_file, process_id):\n    \"\"\"Each process creates its own adapter instance.\"\"\"\n    adapter = OpenAIAdapter()\n    for conv in adapter.stream_conversations(export_file):\n        print(f\"[Process {process_id}] Processing: {conv.title}\")\n\n# Safe: Multiple processes, same file\nexport_file = Path(\"conversations.json\")\nprocesses = [\n    Process(target=worker_process, args=(export_file, i))\n    for i in range(4)\n]\n\nfor p in processes:\n    p.start()\nfor p in processes:\n    p.join()\n</code></pre>"},{"location":"library-usage/#multi-threading-safe-pattern","title":"Multi-Threading (Safe Pattern)","text":"<p>Adapter instances are thread-safe, but iterators are NOT:</p> <pre><code>from threading import Thread\nfrom echomine import OpenAIAdapter\n\nadapter = OpenAIAdapter()  # SAFE: Share adapter across threads\n\ndef worker_thread(thread_id):\n    \"\"\"Each thread creates its own iterator.\"\"\"\n    # SAFE: Each thread calls stream_conversations separately\n    for conv in adapter.stream_conversations(export_file):\n        process(conv, thread_id)\n\nthreads = [Thread(target=worker_thread, args=(i,)) for i in range(4)]\nfor t in threads:\n    t.start()\nfor t in threads:\n    t.join()\n</code></pre>"},{"location":"library-usage/#export-formats-v120","title":"Export Formats (v1.2.0+)","text":""},{"location":"library-usage/#markdown-export-with-yaml-frontmatter","title":"Markdown Export with YAML Frontmatter","text":"<pre><code>from echomine import OpenAIAdapter\nfrom echomine.exporters import MarkdownExporter\nfrom pathlib import Path\n\nadapter = OpenAIAdapter()\nexporter = MarkdownExporter()\n\nconversation = adapter.get_conversation_by_id(Path(\"export.json\"), \"conv-abc123\")\n\nif conversation:\n    # Export with YAML frontmatter (default in v1.2.0)\n    markdown = exporter.export_conversation(conversation)\n    Path(\"chat.md\").write_text(markdown)\n\n    # Export without frontmatter (v1.1.0 style)\n    markdown_plain = exporter.export_conversation(\n        conversation,\n        include_metadata=False,\n        include_message_ids=False\n    )\n    Path(\"chat_plain.md\").write_text(markdown_plain)\n</code></pre>"},{"location":"library-usage/#csv-export","title":"CSV Export","text":"<pre><code>from echomine import OpenAIAdapter, SearchQuery\nfrom echomine.exporters import CSVExporter\nfrom pathlib import Path\n\nadapter = OpenAIAdapter()\nexporter = CSVExporter()\n\n# Export search results to CSV\nquery = SearchQuery(keywords=[\"python\"], limit=100)\nresults = list(adapter.search(Path(\"export.json\"), query))\n\n# Conversation-level CSV\ncsv_content = exporter.export_search_results(results)\nPath(\"results.csv\").write_text(csv_content)\n\n# Message-level CSV\ncsv_messages = exporter.export_messages_from_results(results)\nPath(\"messages.csv\").write_text(csv_messages)\n\n# Export single conversation\nconversation = adapter.get_conversation_by_id(Path(\"export.json\"), \"conv-abc123\")\nif conversation:\n    csv_single = exporter.export_conversation(conversation)\n    Path(\"conversation.csv\").write_text(csv_single)\n</code></pre>"},{"location":"library-usage/#integration-examples","title":"Integration Examples","text":""},{"location":"library-usage/#cognivault-integration","title":"cognivault Integration","text":"<pre><code>from echomine import OpenAIAdapter, SearchQuery\nfrom pathlib import Path\nfrom typing import Iterator\n\nclass CognivaultIngestionPipeline:\n    \"\"\"Ingest AI conversation data into cognivault knowledge graph.\"\"\"\n\n    def __init__(self, cognivault_client):\n        self.adapter = OpenAIAdapter()\n        self.cognivault = cognivault_client\n\n    def ingest_export_file(self, export_file: Path) -&gt; int:\n        \"\"\"Ingest all conversations from export file.\"\"\"\n        count = 0\n        for conversation in self.adapter.stream_conversations(export_file):\n            knowledge_node = {\n                \"id\": conversation.id,\n                \"title\": conversation.title,\n                \"created_at\": conversation.created_at.isoformat(),\n                \"content\": self._flatten_messages(conversation),\n                \"tags\": self._extract_tags(conversation),\n            }\n\n            self.cognivault.ingest_node(knowledge_node)\n            count += 1\n\n        return count\n\n    def ingest_filtered_conversations(\n        self,\n        export_file: Path,\n        project_tag: str\n    ) -&gt; int:\n        \"\"\"Ingest only conversations matching a project tag.\"\"\"\n        query = SearchQuery(keywords=[project_tag], limit=1000)\n\n        count = 0\n        for result in self.adapter.search(export_file, query):\n            knowledge_node = {\n                \"id\": result.conversation.id,\n                \"title\": result.conversation.title,\n                \"relevance\": result.score,\n                \"content\": self._flatten_messages(result.conversation),\n                \"project\": project_tag,\n            }\n\n            self.cognivault.ingest_node(knowledge_node)\n            count += 1\n\n        return count\n\n    def _flatten_messages(self, conversation) -&gt; str:\n        \"\"\"Flatten conversation messages to text.\"\"\"\n        return \"\\\\n\\\\n\".join(\n            f\"{msg.role}: {msg.content}\"\n            for msg in conversation.messages\n        )\n\n    def _extract_tags(self, conversation) -&gt; list[str]:\n        \"\"\"Extract tags from conversation content.\"\"\"\n        # Implement your tag extraction logic\n        return []\n\n\n# Usage\npipeline = CognivaultIngestionPipeline(cognivault_client)\ncount = pipeline.ingest_export_file(Path(\"conversations.json\"))\nprint(f\"Ingested {count} conversations into cognivault\")\n</code></pre>"},{"location":"library-usage/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use streaming for large files: Don't convert iterators to lists</li> <li>Limit search results: Use <code>limit</code> parameter</li> <li>Use title filtering when possible: Faster than full-text search</li> <li>Monitor memory: Streaming uses O(1) memory</li> </ol>"},{"location":"library-usage/#type-safety","title":"Type Safety","text":"<p>Echomine provides full type hints for IDE support:</p> <pre><code>from echomine import OpenAIAdapter\nfrom echomine.models import Conversation, SearchResult\nfrom typing import Iterator\n\nadapter: OpenAIAdapter = OpenAIAdapter()\n\n# IDE autocomplete works!\nconversations: Iterator[Conversation] = adapter.stream_conversations(export_file)\n\nfor conv in conversations:\n    # Type checker knows these fields exist\n    title: str = conv.title\n    message_count: int = len(conv.messages)\n</code></pre>"},{"location":"library-usage/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference: Detailed API documentation</li> <li>CLI Usage: Command-line interface reference</li> <li>Architecture: Design principles and patterns</li> <li>Contributing: Development guidelines</li> </ul>"},{"location":"quickstart/","title":"Quick Start Guide","text":"<p>Get up and running with Echomine in minutes. This guide covers both library and CLI usage.</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<pre><code>pip install echomine\n</code></pre> <p>Or from source:</p> <pre><code>git clone https://github.com/echomine/echomine.git\ncd echomine\npip install -e \".[dev]\"\n</code></pre>"},{"location":"quickstart/#library-usage-recommended","title":"Library Usage (Recommended)","text":""},{"location":"quickstart/#basic-setup","title":"Basic Setup","text":"<pre><code>from echomine import OpenAIAdapter, ClaudeAdapter, SearchQuery\nfrom pathlib import Path\n\n# Create adapter for your provider\nadapter = OpenAIAdapter()  # For ChatGPT exports\n# adapter = ClaudeAdapter()  # For Claude exports\n\nexport_file = Path(\"path/to/conversations.json\")\n</code></pre>"},{"location":"quickstart/#multi-provider-support-v130","title":"Multi-Provider Support (v1.3.0+)","text":"<p>Echomine supports both OpenAI ChatGPT and Anthropic Claude exports:</p> <pre><code>from echomine import OpenAIAdapter, ClaudeAdapter\nfrom pathlib import Path\n\n# OpenAI ChatGPT exports\nopenai_adapter = OpenAIAdapter()\nfor conv in openai_adapter.stream_conversations(Path(\"chatgpt_export.json\")):\n    print(f\"ChatGPT: {conv.title}\")\n\n# Anthropic Claude exports\nclaude_adapter = ClaudeAdapter()\nfor conv in claude_adapter.stream_conversations(Path(\"claude_export.json\")):\n    print(f\"Claude: {conv.title}\")\n\n# Auto-detection (CLI only - see CLI Usage section)\nfrom echomine.cli.provider import detect_provider, get_adapter\n\nprovider = detect_provider(Path(\"export.json\"))  # Returns \"openai\" or \"claude\"\nadapter = get_adapter(None, Path(\"export.json\"))  # Auto-detects and returns appropriate adapter\n</code></pre> <p>Key Differences: - Both adapters share the same interface (stream_conversations, search, get_conversation_by_id, etc.) - Both use O(1) memory streaming with ijson - Export formats differ but are normalized to the same Conversation/Message models - Claude adapter handles content blocks and tool use automatically</p>"},{"location":"quickstart/#1-list-all-conversations","title":"1. List All Conversations","text":"<p>Browse what's in your export file:</p> <pre><code># List all conversations with metadata\nfor conversation in adapter.stream_conversations(export_file):\n    print(f\"[{conversation.created_at.date()}] {conversation.title}\")\n    print(f\"  Messages: {len(conversation.messages)}\")\n    print(f\"  ID: {conversation.id}\")\n</code></pre>"},{"location":"quickstart/#2-search-by-keywords","title":"2. Search by Keywords","text":"<p>Find conversations matching specific topics:</p> <pre><code># Create search query\nquery = SearchQuery(\n    keywords=[\"algorithm\", \"leetcode\"],\n    limit=5\n)\n\n# Execute search (returns ranked results)\nfor result in adapter.search(export_file, query):\n    print(f\"[{result.score:.2f}] {result.conversation.title}\")\n    print(f\"  Preview: {result.snippet}\")  # v1.1.0: automatic snippets\n</code></pre>"},{"location":"quickstart/#3-advanced-search-v110","title":"3. Advanced Search (v1.1.0+)","text":"<p>Use powerful new search features:</p> <pre><code>from datetime import date\n\n# Exact phrase matching + boolean logic + exclusions + role filtering\nquery = SearchQuery(\n    keywords=[\"refactor\", \"optimization\"],\n    phrases=[\"algo-insights\"],  # Exact phrase (preserves hyphens)\n    match_mode=\"all\",  # Require ALL keywords (AND logic)\n    exclude_keywords=[\"test\"],  # Filter out unwanted results\n    role_filter=\"user\",  # Search only your messages\n    from_date=date(2024, 1, 1),\n    to_date=date(2024, 3, 31),\n    limit=5\n)\n\nfor result in adapter.search(export_file, query):\n    print(f\"[{result.score:.2f}] {result.conversation.title}\")\n    print(f\"  Snippet: {result.snippet}\")\n    print(f\"  Matched: {len(result.matched_message_ids)} messages\")\n</code></pre>"},{"location":"quickstart/#4-calculate-statistics-v120","title":"4. Calculate Statistics (v1.2.0+)","text":"<p>Get comprehensive export statistics:</p> <pre><code>from echomine import calculate_statistics\n\n# Export-level statistics\nstats = calculate_statistics(export_file)\nprint(f\"Total conversations: {stats.total_conversations}\")\nprint(f\"Total messages: {stats.total_messages}\")\nprint(f\"Average messages: {stats.average_messages:.1f}\")\n\nif stats.largest_conversation:\n    print(f\"Largest: {stats.largest_conversation.title} \"\n          f\"({stats.largest_conversation.message_count} messages)\")\n</code></pre>"},{"location":"quickstart/#5-get-specific-conversation","title":"5. Get Specific Conversation","text":"<p>Retrieve a conversation by ID:</p> <pre><code>conversation = adapter.get_conversation_by_id(export_file, \"conv-abc123\")\n\nif conversation:\n    print(f\"Found: {conversation.title}\")\n    print(f\"Messages: {len(conversation.messages)}\")\n</code></pre>"},{"location":"quickstart/#cli-usage","title":"CLI Usage","text":""},{"location":"quickstart/#multi-provider-support-v130_1","title":"Multi-Provider Support (v1.3.0+)","text":"<p>All CLI commands support both OpenAI and Claude exports via auto-detection:</p> <pre><code># Auto-detect provider (default - works for both OpenAI and Claude)\nechomine list export.json\n\n# Explicit provider selection (bypasses auto-detection)\nechomine list chatgpt_export.json --provider openai\nechomine list claude_export.json --provider claude\n\n# Search works the same across providers\nechomine search export.json --keywords \"python\" --provider claude\nechomine search export.json --keywords \"python\" --provider openai\n</code></pre>"},{"location":"quickstart/#list-conversations","title":"List Conversations","text":"<pre><code># Human-readable list\nechomine list conversations.json\n\n# JSON output for processing\nechomine list conversations.json --json\n\n# Limit to 10 most recent\nechomine list conversations.json --limit 10\n\n# Claude export with explicit provider (v1.3.0+)\nechomine list claude_export.json --provider claude\n</code></pre>"},{"location":"quickstart/#search","title":"Search","text":"<pre><code># Search by keywords\nechomine search export.json --keywords \"algorithm,design\" --limit 10\n\n# v1.1.0: Exact phrase matching\nechomine search export.json --phrase \"algo-insights\"\n\n# v1.1.0: Boolean match mode (require ALL keywords)\nechomine search export.json -k \"python\" -k \"async\" --match-mode all\n\n# v1.1.0: Exclude unwanted results\nechomine search export.json -k \"python\" --exclude \"django\" --exclude \"flask\"\n\n# v1.1.0: Role filtering (search only user/assistant messages)\nechomine search export.json -k \"refactor\" --role user\n\n# Search by title (fast, metadata-only)\nechomine search export.json --title \"Project\"\n\n# Filter by date range\nechomine search export.json --from-date \"2024-01-01\" --to-date \"2024-03-31\"\n\n# v1.1.0: Combine all advanced features\nechomine search export.json \\\n  --phrase \"api design\" \\\n  --keywords \"python\" \\\n  --exclude \"test\" \\\n  --role user \\\n  --match-mode all \\\n  --title \"Tutorial\" \\\n  --from-date \"2024-01-01\" \\\n  --limit 5\n</code></pre> <p>How filters combine: - Content matching (Stage 1): Phrases OR Keywords (use --match-mode for keyword logic) - Post-filtering (Stage 2): --exclude, --role, --title, date range (all must match)</p> <p>See CLI Usage for detailed filter logic.</p>"},{"location":"quickstart/#view-statistics-v120","title":"View Statistics (v1.2.0+)","text":"<pre><code># View export statistics\nechomine stats export.json\n\n# JSON output for scripting\nechomine stats export.json --json\n</code></pre>"},{"location":"quickstart/#get-conversation-by-id-v120","title":"Get Conversation by ID (v1.2.0+)","text":"<pre><code># Display full conversation\nechomine get export.json conv-abc123\n\n# Summary only\nechomine get export.json conv-abc123 --display summary\n\n# JSON output\nechomine get export.json conv-abc123 --json\n</code></pre>"},{"location":"quickstart/#export-to-markdown-json-or-csv-v120","title":"Export to Markdown, JSON, or CSV (v1.2.0+)","text":"<pre><code># Export as markdown with YAML frontmatter (default in v1.2.0)\nechomine export export.json conv-abc123 --output algo.md\n\n# Export without frontmatter (v1.1.0 style)\nechomine export export.json conv-abc123 --output algo.md --no-metadata\n\n# Export as JSON (for programmatic use)\nechomine export export.json conv-abc123 --format json --output conversation.json\n\n# Export as CSV (v1.2.0+)\nechomine export export.json conv-abc123 --format csv --output messages.csv\n\n# Export JSON to stdout for piping\nechomine export export.json conv-abc123 -f json | jq '.messages[0].content'\n</code></pre>"},{"location":"quickstart/#json-output-for-piping","title":"JSON Output for Piping","text":"<p>All commands support <code>--json</code> flag for pipeline integration:</p> <pre><code># Extract titles with jq\nechomine search export.json --keywords \"python\" --json | jq '.results[].title'\n\n# Count results\nechomine search export.json --keywords \"algorithm\" --json | jq '.results | length'\n\n# Filter results\nechomine list export.json --json | jq '.conversations[] | select(.message_count &gt; 10)'\n</code></pre>"},{"location":"quickstart/#common-workflows","title":"Common Workflows","text":""},{"location":"quickstart/#workflow-1-discovery","title":"Workflow 1: Discovery","text":"<p>Find conversations about a specific topic:</p> <pre><code>from echomine import OpenAIAdapter, SearchQuery\nfrom pathlib import Path\n\nadapter = OpenAIAdapter()\nquery = SearchQuery(keywords=[\"machine learning\"], limit=10)\n\nfor result in adapter.search(Path(\"export.json\"), query):\n    print(f\"[{result.score:.2f}] {result.conversation.title}\")\n    print(f\"  Created: {result.conversation.created_at.date()}\")\n    print(f\"  Messages: {len(result.conversation.messages)}\")\n    print()\n</code></pre>"},{"location":"quickstart/#workflow-2-batch-export","title":"Workflow 2: Batch Export","text":"<p>Export multiple conversations to markdown:</p> <pre><code>from echomine import OpenAIAdapter, SearchQuery\nfrom echomine.exporters import MarkdownExporter\nfrom pathlib import Path\n\nadapter = OpenAIAdapter()\nexporter = MarkdownExporter()\nexport_file = Path(\"conversations.json\")\noutput_dir = Path(\"exported\")\noutput_dir.mkdir(exist_ok=True)\n\n# Search for project-related conversations\nquery = SearchQuery(keywords=[\"project\"], limit=20)\n\nfor result in adapter.search(export_file, query):\n    conv = result.conversation\n\n    # Export to markdown\n    markdown = exporter.export(conv)\n\n    # Save with slugified title\n    from slugify import slugify\n    filename = f\"{slugify(conv.title)}.md\"\n    output_path = output_dir / filename\n\n    output_path.write_text(markdown, encoding=\"utf-8\")\n    print(f\"Exported: {output_path}\")\n</code></pre>"},{"location":"quickstart/#workflow-3-knowledge-base-ingestion","title":"Workflow 3: Knowledge Base Ingestion","text":"<p>Integrate with a knowledge management system:</p> <pre><code>from echomine import OpenAIAdapter, SearchQuery\nfrom pathlib import Path\n\nadapter = OpenAIAdapter()\nexport_file = Path(\"conversations.json\")\n\n# Stream all conversations for ingestion\ncount = 0\nfor conversation in adapter.stream_conversations(export_file):\n    # Transform to your knowledge base format\n    knowledge_node = {\n        \"id\": conversation.id,\n        \"title\": conversation.title,\n        \"created_at\": conversation.created_at.isoformat(),\n        \"content\": \" \".join(msg.content for msg in conversation.messages),\n        \"tags\": extract_tags(conversation),  # Your custom logic\n    }\n\n    # Ingest into knowledge base\n    knowledge_base.add_node(knowledge_node)\n    count += 1\n\nprint(f\"Ingested {count} conversations\")\n</code></pre>"},{"location":"quickstart/#type-safety-example","title":"Type Safety Example","text":"<p>Echomine provides full type hints for IDE support:</p> <pre><code>from echomine import OpenAIAdapter\nfrom echomine.models import Conversation, SearchResult\nfrom typing import Iterator\n\nadapter = OpenAIAdapter()\n\n# IDE autocomplete works!\nconversations: Iterator[Conversation] = adapter.stream_conversations(export_file)\n\nfor conv in conversations:\n    # Type checker knows these fields exist\n    title: str = conv.title\n    message_count: int = len(conv.messages)\n\n    # mypy catches this error at type-check time!\n    # invalid_field = conv.nonexistent_field  # AttributeError\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Library Usage: Comprehensive library API guide with advanced patterns</li> <li>CLI Usage: Complete CLI reference</li> <li>API Reference: Detailed API documentation</li> <li>Architecture: Design principles and patterns</li> </ul>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"quickstart/#import-errors","title":"Import Errors","text":"<pre><code># If you see import errors, ensure package is installed\npip install -e .\n</code></pre>"},{"location":"quickstart/#file-not-found","title":"File Not Found","text":"<pre><code>from pathlib import Path\n\nexport_file = Path(\"conversations.json\")\nif not export_file.exists():\n    print(f\"Export file not found: {export_file}\")\n</code></pre>"},{"location":"quickstart/#empty-results","title":"Empty Results","text":"<pre><code># Check if file contains conversations\nconversations = list(adapter.stream_conversations(export_file))\nprint(f\"Found {len(conversations)} conversations\")\n</code></pre>"},{"location":"quickstart/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use streaming for large files: Don't convert iterators to lists unless necessary</li> <li>Limit search results: Use <code>limit</code> parameter to avoid processing thousands of results</li> <li>Use title filtering when possible: Title search is faster than full-text search</li> <li>Monitor memory: Streaming uses O(1) memory regardless of file size</li> </ol>"},{"location":"api/","title":"API Reference","text":"<p>Complete API documentation for Echomine library.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>Echomine provides a clean, type-safe API for parsing and searching AI conversation exports. All public APIs are fully typed and validated.</p>"},{"location":"api/#main-components","title":"Main Components","text":""},{"location":"api/#models","title":"Models","text":"<p>Data models built with Pydantic v2 for strict validation and type safety:</p> <ul> <li>Conversation: Represents a complete conversation with messages</li> <li>Message: Individual message in a conversation</li> <li>Search: Search query and result models</li> </ul>"},{"location":"api/#adapters","title":"Adapters","text":"<p>Provider-specific implementations for parsing conversation exports:</p> <ul> <li>OpenAI Adapter: ChatGPT conversation export parser</li> <li>Claude Adapter: Anthropic Claude conversation export parser</li> <li>Protocols: ConversationProvider protocol definition</li> </ul>"},{"location":"api/#search","title":"Search","text":"<p>Search and ranking algorithms:</p> <ul> <li>Ranking: BM25 relevance ranking implementation</li> </ul>"},{"location":"api/#cli","title":"CLI","text":"<p>Command-line interface (built on library):</p> <ul> <li>Commands: CLI command reference</li> </ul>"},{"location":"api/#quick-example","title":"Quick Example","text":"<pre><code>from echomine import OpenAIAdapter, SearchQuery\nfrom pathlib import Path\n\n# Initialize adapter\nadapter = OpenAIAdapter()\n\n# Stream conversations\nfor conversation in adapter.stream_conversations(Path(\"export.json\")):\n    print(f\"{conversation.title}: {len(conversation.messages)} messages\")\n\n# Search with keywords\nquery = SearchQuery(keywords=[\"python\"], limit=10)\nfor result in adapter.search(Path(\"export.json\"), query):\n    print(f\"[{result.score:.2f}] {result.conversation.title}\")\n</code></pre>"},{"location":"api/#type-safety","title":"Type Safety","text":"<p>All public APIs provide complete type hints for IDE support:</p> <pre><code>from typing import Iterator\nfrom echomine import OpenAIAdapter\nfrom echomine.models import Conversation, SearchQuery, SearchResult\n\nadapter: OpenAIAdapter = OpenAIAdapter()\nconversations: Iterator[Conversation] = adapter.stream_conversations(file_path)\nresults: Iterator[SearchResult[Conversation]] = adapter.search(file_path, query)\n</code></pre>"},{"location":"api/#import-paths","title":"Import Paths","text":""},{"location":"api/#top-level-imports-recommended","title":"Top-Level Imports (Recommended)","text":"<pre><code>from echomine import OpenAIAdapter\nfrom echomine.models import Conversation, Message, SearchQuery, SearchResult\n</code></pre>"},{"location":"api/#full-module-paths","title":"Full Module Paths","text":"<pre><code>from echomine.adapters.openai import OpenAIAdapter\nfrom echomine.models.conversation import Conversation\nfrom echomine.models.message import Message\nfrom echomine.models.search import SearchQuery, SearchResult\n</code></pre>"},{"location":"api/#navigation","title":"Navigation","text":"<ul> <li>Models: Data models and schemas</li> <li>Adapters: Provider implementations</li> <li>Search: Ranking algorithms</li> <li>CLI: Command-line interface</li> </ul>"},{"location":"api/adapters/claude/","title":"ClaudeAdapter","text":"<p>Streaming adapter for Anthropic Claude conversation export files.</p>"},{"location":"api/adapters/claude/#overview","title":"Overview","text":"<p>The <code>ClaudeAdapter</code> class provides O(1) memory streaming for Claude conversation exports using ijson. It implements the same interface as <code>OpenAIAdapter</code>, enabling seamless multi-provider support through a unified API.</p> <p>Module: <code>echomine.adapters.claude</code></p> <p>Import: <pre><code>from echomine import ClaudeAdapter\n</code></pre></p>"},{"location":"api/adapters/claude/#memory-characteristics","title":"Memory Characteristics","text":"<ul> <li>O(1) memory consumption: File size independent (streaming parser)</li> <li>O(N) per conversation: Where N = message count in single conversation</li> <li>Parser buffer: ~50MB max (ijson state + current conversation)</li> <li>No unbounded structures: Conversations yielded immediately</li> </ul>"},{"location":"api/adapters/claude/#claude-export-schema","title":"Claude Export Schema","text":"<p>Claude exports use a different JSON structure than OpenAI:</p>"},{"location":"api/adapters/claude/#root-structure","title":"Root Structure","text":"<pre><code>[\n  {\n    \"uuid\": \"conversation-id\",\n    \"name\": \"Conversation Title\",\n    \"created_at\": \"2025-10-01T18:42:27.303515Z\",\n    \"updated_at\": \"2025-10-01T18:45:30.123456Z\",\n    \"chat_messages\": [...]\n  }\n]\n</code></pre>"},{"location":"api/adapters/claude/#message-structure","title":"Message Structure","text":"<pre><code>{\n  \"uuid\": \"message-id\",\n  \"text\": \"Fallback message content\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Primary message content\"\n    }\n  ],\n  \"sender\": \"human\",\n  \"created_at\": \"2025-10-01T18:42:30.123456Z\",\n  \"updated_at\": \"2025-10-01T18:42:30.123456Z\"\n}\n</code></pre>"},{"location":"api/adapters/claude/#field-mappings","title":"Field Mappings","text":"<p>Claude export fields are normalized to echomine's unified <code>Conversation</code> and <code>Message</code> models:</p> Claude Field Echomine Field Notes <code>uuid</code> (conversation) <code>id</code> Conversation identifier <code>name</code> <code>title</code> Empty string \u2192 \"(No title)\" <code>created_at</code> <code>created_at</code> Parsed to timezone-aware datetime <code>updated_at</code> <code>updated_at</code> Parsed to timezone-aware datetime <code>chat_messages</code> <code>messages</code> Flat message array <code>uuid</code> (message) <code>id</code> Message identifier <code>content[type=text].text</code> <code>content</code> Extracted from content blocks <code>text</code> <code>content</code> Fallback if content blocks empty <code>sender</code> (\"human\") <code>role</code> (\"user\") Normalized role names <code>sender</code> (\"assistant\") <code>role</code> (\"assistant\") Normalized role names"},{"location":"api/adapters/claude/#content-block-handling","title":"Content Block Handling","text":"<p>Claude messages use a content block structure:</p> <pre><code>\"content\": [\n  {\"type\": \"text\", \"text\": \"Hello\"},\n  {\"type\": \"tool_use\", \"id\": \"toolu_123\", \"name\": \"calc\", \"input\": {}},\n  {\"type\": \"text\", \"text\": \"World\"}\n]\n</code></pre> <p>Extraction Strategy: 1. Extract text from all <code>type=\"text\"</code> blocks 2. Skip <code>type=\"tool_use\"</code> and <code>type=\"tool_result\"</code> blocks (tool invocations ignored) 3. Concatenate text blocks with newline separator 4. Fallback to <code>text</code> field if content extraction yields empty string</p>"},{"location":"api/adapters/claude/#class-definition","title":"Class Definition","text":"<pre><code>class ClaudeAdapter:\n    \"\"\"Adapter for streaming Anthropic Claude conversation exports.\"\"\"\n</code></pre> <p>The adapter is stateless - no instance variables or configuration. All methods accept file paths as arguments, enabling reuse across multiple export files.</p>"},{"location":"api/adapters/claude/#methods","title":"Methods","text":""},{"location":"api/adapters/claude/#stream_conversations","title":"stream_conversations","text":"<p>Stream all conversations from a Claude export file.</p> <p>Signature: <pre><code>def stream_conversations(\n    self,\n    file_path: Path,\n    *,\n    progress_callback: ProgressCallback | None = None,\n    on_skip: OnSkipCallback | None = None,\n) -&gt; Iterator[Conversation]:\n</code></pre></p> <p>Parameters: - <code>file_path</code> (Path): Path to Claude export JSON file - <code>progress_callback</code> (Optional[ProgressCallback]): Callback invoked every 100 conversations for progress reporting - <code>on_skip</code> (Optional[OnSkipCallback]): Callback invoked when malformed entries are skipped</p> <p>Returns: - Iterator[Conversation]: Lazy stream of parsed conversations</p> <p>Raises: - <code>FileNotFoundError</code>: If file doesn't exist - <code>ParseError</code>: If JSON syntax is invalid - <code>ValidationError</code>: If conversation data violates Pydantic schema</p> <p>Example: <pre><code>from pathlib import Path\nfrom echomine import ClaudeAdapter\n\nadapter = ClaudeAdapter()\n\n# Basic streaming\nfor conv in adapter.stream_conversations(Path(\"claude_export.json\")):\n    print(f\"{conv.title}: {len(conv.messages)} messages\")\n\n# With progress callback\ndef show_progress(count: int) -&gt; None:\n    print(f\"Processed {count} conversations\")\n\nfor conv in adapter.stream_conversations(\n    Path(\"claude_export.json\"),\n    progress_callback=show_progress\n):\n    process(conv)\n\n# Early termination (memory efficient)\nconversations = []\nfor i, conv in enumerate(adapter.stream_conversations(Path(\"claude_export.json\"))):\n    conversations.append(conv)\n    if i &gt;= 9:  # First 10 only\n        break\n</code></pre></p> <p>Performance: - Memory: O(1) for file size, O(N) for single conversation - Time: O(M) where M = total conversations in file - Lazy: Conversations yielded as parsed (no upfront loading)</p>"},{"location":"api/adapters/claude/#search","title":"search","text":"<p>Search conversations with BM25 relevance ranking.</p> <p>Signature: <pre><code>def search(\n    self,\n    file_path: Path,\n    query: SearchQuery,\n    *,\n    progress_callback: ProgressCallback | None = None,\n    on_skip: OnSkipCallback | None = None,\n) -&gt; Iterator[SearchResult[Conversation]]:\n</code></pre></p> <p>Parameters: - <code>file_path</code> (Path): Path to Claude export file - <code>query</code> (SearchQuery): Search query with keywords, filters, and limits - <code>progress_callback</code> (Optional[ProgressCallback]): Progress reporting callback - <code>on_skip</code> (Optional[OnSkipCallback]): Malformed entry callback</p> <p>Returns: - Iterator[SearchResult[Conversation]]: Ranked search results with scores</p> <p>Raises: - <code>FileNotFoundError</code>: If file doesn't exist - <code>ParseError</code>: If JSON syntax is invalid</p> <p>Example: <pre><code>from pathlib import Path\nfrom echomine import ClaudeAdapter, SearchQuery\n\nadapter = ClaudeAdapter()\nexport_file = Path(\"claude_export.json\")\n\n# Keyword search\nquery = SearchQuery(keywords=[\"python\", \"algorithm\"], limit=10)\nfor result in adapter.search(export_file, query):\n    print(f\"[{result.score:.2f}] {result.conversation.title}\")\n    print(f\"  Snippet: {result.snippet}\")\n\n# Advanced search with filters\nfrom datetime import date\n\nquery = SearchQuery(\n    keywords=[\"refactor\"],\n    phrases=[\"code review\"],\n    match_mode=\"all\",\n    exclude_keywords=[\"test\"],\n    role_filter=\"user\",\n    from_date=date(2025, 1, 1),\n    limit=5\n)\n\nfor result in adapter.search(export_file, query):\n    print(f\"{result.conversation.title}\")\n    print(f\"  Score: {result.score:.2f}\")\n    print(f\"  Matched messages: {len(result.matched_message_ids)}\")\n</code></pre></p> <p>Search Features: - BM25 relevance ranking - Exact phrase matching - Boolean keyword logic (AND/OR) - Keyword exclusion - Role filtering (user/assistant) - Date range filtering - Message count filtering - Automatic snippet extraction</p> <p>Performance: - Memory: O(N) where N = matching conversations (must score all for ranking) - Time: O(M) where M = total conversations in file</p>"},{"location":"api/adapters/claude/#get_conversation_by_id","title":"get_conversation_by_id","text":"<p>Retrieve a specific conversation by UUID.</p> <p>Signature: <pre><code>def get_conversation_by_id(\n    self,\n    file_path: Path,\n    conversation_id: str,\n) -&gt; Conversation | None:\n</code></pre></p> <p>Parameters: - <code>file_path</code> (Path): Path to Claude export file - <code>conversation_id</code> (str): Full or partial UUID (minimum 4 characters)</p> <p>Returns: - Conversation | None: Conversation if found, None otherwise</p> <p>Raises: - <code>FileNotFoundError</code>: If file doesn't exist - <code>ParseError</code>: If JSON syntax is invalid</p> <p>Example: <pre><code>from pathlib import Path\nfrom echomine import ClaudeAdapter\n\nadapter = ClaudeAdapter()\nexport_file = Path(\"claude_export.json\")\n\n# Full UUID match\nconv = adapter.get_conversation_by_id(\n    export_file,\n    \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\"\n)\n\n# Partial UUID match (minimum 4 characters)\nconv = adapter.get_conversation_by_id(export_file, \"a1b2\")\n\nif conv:\n    print(f\"Found: {conv.title}\")\n    print(f\"Messages: {len(conv.messages)}\")\nelse:\n    print(\"Conversation not found\")\n</code></pre></p> <p>Matching Rules: - Case-insensitive - Exact match takes precedence - Prefix match requires minimum 4 characters - Returns first match found (early termination)</p> <p>Performance: - Memory: O(1) for file size, O(M) for single conversation - Time: O(N) where N = conversations until match (early termination)</p>"},{"location":"api/adapters/claude/#get_message_by_id","title":"get_message_by_id","text":"<p>Retrieve a specific message by UUID with conversation context.</p> <p>Signature: <pre><code>def get_message_by_id(\n    self,\n    file_path: Path,\n    message_id: str,\n    *,\n    conversation_id: str | None = None,\n) -&gt; tuple[Message, Conversation] | None:\n</code></pre></p> <p>Parameters: - <code>file_path</code> (Path): Path to Claude export file - <code>message_id</code> (str): UUID of message to retrieve - <code>conversation_id</code> (Optional[str]): Conversation hint for faster lookup</p> <p>Returns: - tuple[Message, Conversation] | None: Message and parent conversation if found, None otherwise</p> <p>Raises: - <code>FileNotFoundError</code>: If file doesn't exist - <code>ParseError</code>: If JSON syntax is invalid</p> <p>Example: <pre><code>from pathlib import Path\nfrom echomine import ClaudeAdapter\n\nadapter = ClaudeAdapter()\nexport_file = Path(\"claude_export.json\")\n\n# Search with conversation hint (faster)\nresult = adapter.get_message_by_id(\n    export_file,\n    \"msg-123\",\n    conversation_id=\"conv-456\"\n)\n\n# Search all conversations (slower)\nresult = adapter.get_message_by_id(export_file, \"msg-123\")\n\nif result:\n    message, conversation = result\n    print(f\"Message: {message.content}\")\n    print(f\"From conversation: {conversation.title}\")\nelse:\n    print(\"Message not found\")\n</code></pre></p> <p>Performance: - With <code>conversation_id</code> hint:   - Time: O(N) where N = conversations until match   - Memory: O(1) for file size - Without hint:   - Time: O(N*M) where N = conversations, M = messages per conversation   - Memory: O(1) for file size - Early termination: Returns immediately when match found</p>"},{"location":"api/adapters/claude/#error-handling","title":"Error Handling","text":"<p>The adapter follows a fail-fast strategy for unrecoverable errors and graceful degradation for malformed entries:</p>"},{"location":"api/adapters/claude/#fail-fast-errors","title":"Fail-Fast Errors","text":"<p>Raised immediately, processing stops:</p> <pre><code>from echomine.exceptions import ParseError\nfrom pydantic import ValidationError\n\ntry:\n    conversations = list(adapter.stream_conversations(path))\nexcept FileNotFoundError:\n    print(\"Export file not found\")\nexcept ParseError:\n    print(\"Invalid JSON syntax\")\nexcept ValidationError:\n    print(\"Export schema violation\")\n</code></pre>"},{"location":"api/adapters/claude/#graceful-degradation","title":"Graceful Degradation","text":"<p>Malformed conversations/messages are logged and skipped, processing continues:</p> <pre><code>import logging\n\n# Enable logging to see skipped entries\nlogging.basicConfig(level=logging.WARNING)\n\nskipped_ids = []\n\ndef on_skip(item_id: str, reason: str) -&gt; None:\n    skipped_ids.append(item_id)\n\n# Processing continues despite malformed entries\nconversations = list(adapter.stream_conversations(\n    path,\n    on_skip=on_skip\n))\n\nprint(f\"Parsed {len(conversations)} conversations\")\nprint(f\"Skipped {len(skipped_ids)} malformed entries\")\n</code></pre>"},{"location":"api/adapters/claude/#empty-conversation-handling","title":"Empty Conversation Handling","text":"<p>Claude exports may contain conversations with zero messages. The adapter handles this gracefully:</p> <pre><code># Empty conversation in export\n{\n  \"uuid\": \"conv-123\",\n  \"name\": \"Empty Chat\",\n  \"created_at\": \"2025-10-01T18:42:27.303515Z\",\n  \"updated_at\": \"2025-10-01T18:42:27.303515Z\",\n  \"chat_messages\": []\n}\n\n# Adapter inserts placeholder message\nconversation = adapter.get_conversation_by_id(path, \"conv-123\")\nprint(len(conversation.messages))  # 1 (placeholder)\nprint(conversation.messages[0].content)  # \"(Empty conversation)\"\nprint(conversation.messages[0].metadata)  # {\"is_placeholder\": True}\n</code></pre> <p>This satisfies the <code>Conversation</code> model's requirement for at least one message while preserving data integrity.</p>"},{"location":"api/adapters/claude/#type-safety","title":"Type Safety","text":"<p>The adapter is fully type-checked with mypy --strict:</p> <pre><code>from pathlib import Path\nfrom echomine import ClaudeAdapter\nfrom echomine.models import Conversation, Message, SearchResult\nfrom typing import Iterator\n\nadapter = ClaudeAdapter()\n\n# Type inference works\nconversations: Iterator[Conversation] = adapter.stream_conversations(Path(\"export.json\"))\n\nfor conv in conversations:\n    # IDE autocomplete and type checking\n    title: str = conv.title\n    message_count: int = len(conv.messages)\n\n    # mypy catches errors at type-check time\n    # invalid = conv.nonexistent_field  # AttributeError caught by mypy\n</code></pre>"},{"location":"api/adapters/claude/#constitution-compliance","title":"Constitution Compliance","text":"<p>The <code>ClaudeAdapter</code> adheres to all project constitution principles:</p> <ul> <li>Principle I (Library-First): Importable adapter, no CLI dependencies</li> <li>Principle VI (Strict Typing): mypy --strict, no <code>Any</code> types in public API</li> <li>Principle VII (Multi-Provider): Stateless adapter implements ConversationProvider protocol</li> <li>Principle VIII (Memory Efficiency): O(1) memory via ijson streaming</li> </ul>"},{"location":"api/adapters/claude/#see-also","title":"See Also","text":"<ul> <li>OpenAIAdapter - Adapter for ChatGPT exports</li> <li>ConversationProvider Protocol - Adapter interface contract</li> <li>Search Ranking - BM25 ranking and relevance scoring</li> <li>SearchQuery Model - Search query parameters and filters</li> <li>Conversation Model - Unified conversation schema</li> </ul>"},{"location":"api/adapters/openai/","title":"OpenAI Adapter","text":"<p>Adapter for parsing ChatGPT conversation exports.</p>"},{"location":"api/adapters/openai/#overview","title":"Overview","text":"<p>The <code>OpenAIAdapter</code> implements the <code>ConversationProvider</code> protocol for OpenAI (ChatGPT) conversation exports. It provides stateless, streaming-based parsing with O(1) memory usage.</p>"},{"location":"api/adapters/openai/#api-reference","title":"API Reference","text":""},{"location":"api/adapters/openai/#echomine.adapters.openai.OpenAIAdapter","title":"OpenAIAdapter","text":"<p>Adapter for streaming OpenAI conversation exports.</p> <p>This adapter uses ijson to stream-parse OpenAI ChatGPT export files with O(1) memory complexity. Conversations are yielded one at a time, enabling processing of arbitrarily large export files on modest hardware.</p> Memory Model <ul> <li>Streaming parser state: ~10-50MB (ijson buffer)</li> <li>Per-conversation overhead: ~5MB (metadata + message tree)</li> <li>Total working set: &lt;100MB regardless of file size</li> </ul> Error Handling Strategy <ul> <li>FileNotFoundError: Raised for missing files (fail-fast)</li> <li>ParseError: Raised for invalid JSON syntax (fail-fast)</li> <li>ValidationError: Raised for schema violations (fail-fast during streaming)</li> <li>Malformed conversations: Logged and skipped (graceful degradation)</li> </ul> Example <pre><code>from pathlib import Path\nfrom echomine.adapters import OpenAIAdapter\n\nadapter = OpenAIAdapter()\n\n# Stream all conversations (lazy iteration)\nfor conversation in adapter.stream_conversations(Path(\"export.json\")):\n    print(f\"{conversation.title}: {conversation.message_count} messages\")\n\n# Process first N conversations only (memory-efficient)\nconversations = []\nfor i, conv in enumerate(adapter.stream_conversations(Path(\"export.json\"))):\n    conversations.append(conv)\n    if i &gt;= 9:  # First 10 conversations\n        break\n</code></pre> Requirements <ul> <li>FR-003: O(1) memory streaming implementation</li> <li>FR-018: Extract conversation metadata (id, title, timestamps)</li> <li>FR-122: Use ijson for incremental JSON parsing</li> <li>FR-281-285: Skip malformed entries with warning logs</li> <li>SC-001: Memory usage &lt;1GB for large exports</li> </ul>"},{"location":"api/adapters/openai/#echomine.adapters.openai.OpenAIAdapter.stream_conversations","title":"stream_conversations","text":"<pre><code>stream_conversations(\n    file_path: Path,\n    *,\n    progress_callback: ProgressCallback | None = None,\n    on_skip: OnSkipCallback | None = None,\n) -&gt; Iterator[Conversation]\n</code></pre> <p>Stream conversations from OpenAI export file with O(1) memory.</p> <p>This method uses ijson to incrementally parse the export file, yielding Conversation objects one at a time. The entire file is NEVER loaded into memory - only the current conversation being parsed.</p> Streaming Behavior <ul> <li>Returns iterator (lazy evaluation)</li> <li>Conversations yielded in file order</li> <li>Parser state bounded by ijson buffer (~50MB)</li> <li>No buffering between conversations</li> </ul> Error Handling <ul> <li>Invalid JSON: Raises ParseError immediately</li> <li>Missing file: Raises FileNotFoundError</li> <li>Schema violations: Raises ValidationError (Pydantic)</li> <li>Empty array: Succeeds, yields zero conversations</li> </ul> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to OpenAI export JSON file</p> required <code>progress_callback</code> <code>ProgressCallback | None</code> <p>Optional callback invoked every 100 conversations (FR-069)</p> <code>None</code> <code>on_skip</code> <code>OnSkipCallback | None</code> <p>Optional callback invoked when malformed entries skipped (FR-107)</p> <code>None</code> <p>Yields:</p> Type Description <code>Conversation</code> <p>Conversation objects parsed from export</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>ParseError</code> <p>If JSON is malformed (syntax errors)</p> <code>ValidationError</code> <p>If conversation data violates schema</p> Example <pre><code># Basic usage\nadapter = OpenAIAdapter()\nfor conv in adapter.stream_conversations(Path(\"export.json\")):\n    print(f\"Conversation: {conv.title}\")\n\n# Handle errors\ntry:\n    conversations = list(adapter.stream_conversations(path))\nexcept ParseError as e:\n    print(f\"Invalid export format: {e}\")\nexcept ValidationError as e:\n    print(f\"Schema violation: {e}\")\n</code></pre> <p>Memory Complexity: O(1) for file size, O(N) for single conversation Time Complexity: O(M) where M = total conversations in file</p> Source code in <code>src/echomine/adapters/openai.py</code> <pre><code>def stream_conversations(\n    self,\n    file_path: Path,\n    *,\n    progress_callback: ProgressCallback | None = None,\n    on_skip: OnSkipCallback | None = None,\n) -&gt; Iterator[Conversation]:\n    \"\"\"Stream conversations from OpenAI export file with O(1) memory.\n\n    This method uses ijson to incrementally parse the export file, yielding\n    Conversation objects one at a time. The entire file is NEVER loaded into\n    memory - only the current conversation being parsed.\n\n    Streaming Behavior:\n        - Returns iterator (lazy evaluation)\n        - Conversations yielded in file order\n        - Parser state bounded by ijson buffer (~50MB)\n        - No buffering between conversations\n\n    Error Handling:\n        - Invalid JSON: Raises ParseError immediately\n        - Missing file: Raises FileNotFoundError\n        - Schema violations: Raises ValidationError (Pydantic)\n        - Empty array: Succeeds, yields zero conversations\n\n    Args:\n        file_path: Path to OpenAI export JSON file\n        progress_callback: Optional callback invoked every 100 conversations (FR-069)\n        on_skip: Optional callback invoked when malformed entries skipped (FR-107)\n\n    Yields:\n        Conversation objects parsed from export\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        ParseError: If JSON is malformed (syntax errors)\n        ValidationError: If conversation data violates schema\n\n    Example:\n        ```python\n        # Basic usage\n        adapter = OpenAIAdapter()\n        for conv in adapter.stream_conversations(Path(\"export.json\")):\n            print(f\"Conversation: {conv.title}\")\n\n        # Handle errors\n        try:\n            conversations = list(adapter.stream_conversations(path))\n        except ParseError as e:\n            print(f\"Invalid export format: {e}\")\n        except ValidationError as e:\n            print(f\"Schema violation: {e}\")\n        ```\n\n    Memory Complexity: O(1) for file size, O(N) for single conversation\n    Time Complexity: O(M) where M = total conversations in file\n    \"\"\"\n    # Open file in binary mode for ijson (required for streaming)\n    # FileNotFoundError raised naturally by open() if file missing\n    try:\n        with open(file_path, \"rb\") as f:\n            # Stream top-level array items using ijson\n            # Memory: O(1) - ijson maintains bounded buffer\n            # Each \"item\" is a complete conversation object\n            try:\n                items = ijson.items(f, \"item\")\n                count = 0  # Track for progress_callback (FR-069)\n\n                for raw_conversation in items:\n                    # Parse individual conversation\n                    # Memory: O(N) where N = messages in this conversation\n                    try:\n                        conversation = self._parse_conversation(raw_conversation)\n                        count += 1\n\n                        # Invoke progress callback every 100 items (FR-069)\n                        if progress_callback and count % 100 == 0:\n                            progress_callback(count)\n\n                        yield conversation\n                    except PydanticValidationError as e:\n                        # Graceful degradation: skip malformed entries (FR-281)\n                        conversation_id = raw_conversation.get(\"id\", \"unknown\")\n                        reason = f\"Validation error: {e}\"\n\n                        # Invoke on_skip callback if provided (FR-107)\n                        if on_skip:\n                            on_skip(conversation_id, reason)\n\n                        # Log warning but continue processing (FR-281)\n                        logger.warning(\n                            \"Skipped malformed conversation\",\n                            extra={\n                                \"conversation_id\": conversation_id,\n                                \"reason\": reason,\n                            },\n                        )\n                        continue  # Skip this conversation, process next\n\n            except ijson.JSONError as e:\n                # ijson.JSONError raised for malformed JSON\n                # Convert to our ParseError for consistent error handling (FR-039, FR-041)\n                raise ParseError(\n                    f\"JSON parsing failed: {e}. \"\n                    f\"Verify export file '{file_path}' is valid JSON from OpenAI ChatGPT.\"\n                ) from e\n\n    except FileNotFoundError:\n        # Re-raise FileNotFoundError without wrapping\n        # This is a standard Python exception, no conversion needed\n        raise\n</code></pre>"},{"location":"api/adapters/openai/#echomine.adapters.openai.OpenAIAdapter.search","title":"search","text":"<pre><code>search(\n    file_path: Path,\n    query: SearchQuery,\n    *,\n    progress_callback: ProgressCallback | None = None,\n    on_skip: OnSkipCallback | None = None,\n) -&gt; Iterator[SearchResult[Conversation]]\n</code></pre> <p>Search conversations with BM25 relevance ranking.</p> <p>Algorithm: 1. Stream all conversations (O(1) memory per conversation) 2. Apply title filter if specified (metadata-only, fast) 3. Apply date range filter if specified 4. Build corpus and calculate BM25 scores 5. Rank by relevance (descending) 6. Apply limit if specified 7. Yield SearchResult objects one at a time</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to OpenAI export file</p> required <code>query</code> <code>SearchQuery</code> <p>SearchQuery with keywords, title_filter, limit</p> required <code>progress_callback</code> <code>ProgressCallback | None</code> <p>Optional callback invoked per conversation processed</p> <code>None</code> <code>on_skip</code> <code>OnSkipCallback | None</code> <p>Optional callback for malformed entries</p> <code>None</code> <p>Yields:</p> Type Description <code>SearchResult[Conversation]</code> <p>SearchResult[Conversation] with ranked results and scores</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>ParseError</code> <p>If JSON is malformed</p> Performance <ul> <li>Memory: O(N) where N = matching conversations</li> <li>Time: O(M) where M = total conversations in file</li> <li>Early termination: Not implemented (stream all for BM25)</li> </ul> Example <pre><code>adapter = OpenAIAdapter()\nquery = SearchQuery(keywords=[\"python\"], limit=10)\n\nfor result in adapter.search(Path(\"export.json\"), query):\n    print(f\"{result.score:.2f}: {result.conversation.title}\")\n</code></pre> Source code in <code>src/echomine/adapters/openai.py</code> <pre><code>def search(\n    self,\n    file_path: Path,\n    query: SearchQuery,\n    *,\n    progress_callback: ProgressCallback | None = None,\n    on_skip: OnSkipCallback | None = None,\n) -&gt; Iterator[SearchResult[Conversation]]:\n    \"\"\"Search conversations with BM25 relevance ranking.\n\n    Algorithm:\n    1. Stream all conversations (O(1) memory per conversation)\n    2. Apply title filter if specified (metadata-only, fast)\n    3. Apply date range filter if specified\n    4. Build corpus and calculate BM25 scores\n    5. Rank by relevance (descending)\n    6. Apply limit if specified\n    7. Yield SearchResult objects one at a time\n\n    Args:\n        file_path: Path to OpenAI export file\n        query: SearchQuery with keywords, title_filter, limit\n        progress_callback: Optional callback invoked per conversation processed\n        on_skip: Optional callback for malformed entries\n\n    Yields:\n        SearchResult[Conversation] with ranked results and scores\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        ParseError: If JSON is malformed\n\n    Performance:\n        - Memory: O(N) where N = matching conversations\n        - Time: O(M) where M = total conversations in file\n        - Early termination: Not implemented (stream all for BM25)\n\n    Example:\n        ```python\n        adapter = OpenAIAdapter()\n        query = SearchQuery(keywords=[\"python\"], limit=10)\n\n        for result in adapter.search(Path(\"export.json\"), query):\n            print(f\"{result.score:.2f}: {result.conversation.title}\")\n        ```\n    \"\"\"\n    # Stream conversations and apply filters\n    # Type: (conversation, filtered_messages) for snippet extraction\n    conversations: list[tuple[Conversation, list[Message]]] = []\n    corpus_texts: list[str] = []\n\n    count = 0\n    for conv in self.stream_conversations(file_path):\n        count += 1\n\n        # Progress callback (every 100 items per FR-069)\n        if progress_callback and count % 100 == 0:\n            progress_callback(count)\n\n        # Title filter (fast metadata check)\n        if query.has_title_filter():\n            assert query.title_filter is not None  # Type narrowing\n            if query.title_filter.lower() not in conv.title.lower():\n                continue  # Skip non-matching titles\n\n        # Date range filter\n        if query.has_date_filter():\n            conv_date = conv.created_at.date()\n\n            # Check from_date (inclusive)\n            if query.from_date is not None and conv_date &lt; query.from_date:\n                continue\n\n            # Check to_date (inclusive)\n            if query.to_date is not None and conv_date &gt; query.to_date:\n                continue\n\n        # FR-006: Message count filter (streaming approach for O(1) memory)\n        if query.has_message_count_filter():\n            msg_count = conv.message_count\n\n            # Check min_messages (inclusive)\n            if query.min_messages is not None and msg_count &lt; query.min_messages:\n                continue\n\n            # Check max_messages (inclusive)\n            if query.max_messages is not None and msg_count &gt; query.max_messages:\n                continue\n\n        # FR-018: Filter messages by role before text aggregation\n        if query.role_filter is not None:\n            filtered_messages = [m for m in conv.messages if m.role == query.role_filter]\n        else:\n            filtered_messages = list(conv.messages)\n\n        # Skip conversations with no messages matching the role filter\n        if query.role_filter is not None and not filtered_messages:\n            continue\n\n        # Build corpus text\n        # When role_filter is set, only search in filtered message content (not title)\n        # When role_filter is None, include title for metadata-based matching\n        if query.role_filter is not None:\n            conv_text = \" \".join(m.content for m in filtered_messages)\n        else:\n            conv_text = f\"{conv.title} \" + \" \".join(m.content for m in filtered_messages)\n\n        conversations.append((conv, filtered_messages))\n        corpus_texts.append(conv_text)\n\n    # Final progress callback\n    if progress_callback:\n        progress_callback(count)\n\n    # Handle empty results\n    if not conversations:\n        return  # Empty iterator\n\n    # Calculate average document length for BM25\n    # Use regex tokenization to match BM25Scorer's tokenization\n    import re\n\n    def tokenize_for_length(text: str) -&gt; int:\n        \"\"\"Count tokens using same method as BM25Scorer.\"\"\"\n        text_lower = text.lower()\n        count = len(re.findall(r\"[a-z0-9]+\", text_lower))\n        count += len(re.findall(r\"[^\\W\\d_a-z]\", text_lower))\n        return count\n\n    avg_doc_length = sum(tokenize_for_length(text) for text in corpus_texts) / len(corpus_texts)\n\n    # Initialize BM25 scorer\n    scorer = BM25Scorer(corpus=corpus_texts, avg_doc_length=avg_doc_length)\n\n    # Score all conversations\n    # Type: (conversation, score, matched_message_ids, filtered_messages)\n    scored_conversations: list[tuple[Conversation, float, list[str], list[Message]]] = []\n\n    for (conv, filtered_msgs), conv_text in zip(conversations, corpus_texts):\n        score = 0.0\n        matched_message_ids: list[str] = []\n        has_keyword_match = False\n        has_phrase_match = False\n\n        # Check keyword matches (BM25 scoring)\n        if query.has_keyword_search():\n            assert query.keywords is not None  # Type narrowing\n\n            # FR-009: match_mode='all' requires ALL keywords present\n            if query.match_mode == \"all\":\n                if all_terms_present(conv_text, query.keywords, scorer):\n                    # All keywords present - calculate score\n                    score = scorer.score(conv_text, query.keywords)\n                    matched_message_ids = self._find_matched_messages(\n                        filtered_msgs, query.keywords\n                    )\n                    has_keyword_match = True\n                # else: keywords don't all match, but may still match phrases (checked below)\n            else:\n                # Default 'any' mode: regular BM25 scoring\n                score = scorer.score(conv_text, query.keywords)\n                matched_message_ids = self._find_matched_messages(filtered_msgs, query.keywords)\n                if score &gt; 0.0:\n                    has_keyword_match = True\n\n        # Check phrase matches (exact substring matching)\n        # FR-002: Multiple phrases use OR logic\n        # FR-004: Phrases can be combined with keywords (OR logic)\n        if query.has_phrase_search():\n            assert query.phrases is not None  # Type narrowing\n            if phrase_matches(conv_text, query.phrases):\n                has_phrase_match = True\n                # If phrase matches but no keyword score, use 1.0\n                if score == 0.0:\n                    score = 1.0\n                # Find messages that match the phrases (from filtered messages only)\n                for message in filtered_msgs:\n                    if phrase_matches(message.content, query.phrases):\n                        if message.id not in matched_message_ids:\n                            matched_message_ids.append(message.id)\n\n        # Skip conversations with no matches (neither keyword nor phrase)\n        if not has_keyword_match and not has_phrase_match:\n            # If no keywords or phrases specified, include all (title/date filter only)\n            if not query.has_keyword_search() and not query.has_phrase_search():\n                score = 1.0\n            else:\n                continue\n\n        # FR-014: Apply exclude filter after matching, before ranking\n        if query.has_exclude_keywords():\n            assert query.exclude_keywords is not None  # Type narrowing\n            if exclude_filter(conv_text, query.exclude_keywords, scorer):\n                continue  # Skip conversations containing excluded terms\n\n        scored_conversations.append((conv, score, matched_message_ids, filtered_msgs))\n\n    # Handle no results after filtering\n    if not scored_conversations:\n        return  # Empty iterator\n\n    # Sort conversations based on query parameters (FR-043-048)\n    # FR-043a: Tie-breaking by conversation_id (ascending, lexicographic)\n    # FR-043b: Stable sort (Python's sort() is stable by default)\n    def get_sort_key(\n        item: tuple[Conversation, float, list[str], list[Message]],\n    ) -&gt; tuple[float | str | int, str]:\n        \"\"\"Get sort key based on query sort_by parameter.\n\n        Returns tuple for multi-level sorting:\n        - Primary: sort_by field value\n        - Secondary: conversation_id (tie-breaker, FR-043a)\n\n        FR-046a: For date sort, use updated_at or fall back to created_at if None\n        FR-047: Title sort is case-insensitive\n        \"\"\"\n        conv, score, _, _ = item\n\n        primary_key: float | str | int\n        if query.sort_by == \"score\":\n            # Sort by BM25 relevance score\n            primary_key = score\n        elif query.sort_by == \"date\":\n            # FR-046a: Use updated_at if present, otherwise created_at\n            sort_date = conv.updated_at if conv.updated_at is not None else conv.created_at\n            # Convert datetime to timestamp for numeric sorting\n            primary_key = sort_date.timestamp()\n        elif query.sort_by == \"title\":\n            # FR-047: Case-insensitive title sort\n            primary_key = conv.title.lower()\n        else:  # query.sort_by == \"messages\"\n            # Sort by message count\n            primary_key = conv.message_count\n\n        # FR-043a: Tie-breaking by conversation_id (ascending)\n        return (primary_key, conv.id)\n\n    # Apply sorting with reverse based on sort_order (FR-044)\n    reverse_sort = query.sort_order == \"desc\"\n    scored_conversations.sort(key=get_sort_key, reverse=reverse_sort)\n\n    # Normalize scores to [0.0, 1.0] range using BM25 normalization formula (FR-319)\n    # Formula: score_normalized = score_raw / (score_raw + 1)\n    # This ensures consistent score interpretation across queries\n    scored_conversations = [\n        (conv, score / (score + 1.0) if score &gt; 0 else 0.0, msg_ids, msgs)\n        for conv, score, msg_ids, msgs in scored_conversations\n    ]\n\n    # Apply limit (always positive integer per SearchQuery validation)\n    scored_conversations = scored_conversations[: query.limit]\n\n    # Yield SearchResult objects with snippet extraction (FR-021-025)\n    for conv, score, matched_message_ids, filtered_msgs in scored_conversations:\n        # Build keywords list for snippet extraction\n        snippet_keywords: list[str] = []\n        if query.keywords:\n            snippet_keywords.extend(query.keywords)\n        if query.phrases:\n            snippet_keywords.extend(query.phrases)\n\n        # Extract snippet from matched messages\n        snippet, _ = extract_snippet_from_messages(\n            filtered_msgs,\n            snippet_keywords,\n            matched_message_ids,\n        )\n\n        yield SearchResult[Conversation](\n            conversation=conv,\n            score=score,\n            matched_message_ids=matched_message_ids,\n            snippet=snippet,\n        )\n</code></pre>"},{"location":"api/adapters/openai/#echomine.adapters.openai.OpenAIAdapter.get_conversation_by_id","title":"get_conversation_by_id","text":"<pre><code>get_conversation_by_id(\n    file_path: Path, conversation_id: str\n) -&gt; Conversation | None\n</code></pre> <p>Retrieve specific conversation by UUID (FR-155, FR-217, FR-356).</p> <p>Uses streaming search for memory efficiency - O(N) time, O(1) memory. For large files with frequent ID lookups, consider building an index.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to OpenAI export JSON file</p> required <code>conversation_id</code> <code>str</code> <p>UUID of conversation to retrieve</p> required <p>Returns:</p> Type Description <code>Conversation | None</code> <p>Conversation object if found, None otherwise (FR-155)</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>ParseError</code> <p>If JSON is malformed</p> Example <pre><code>adapter = OpenAIAdapter()\nconv = adapter.get_conversation_by_id(\n    Path(\"export.json\"),\n    \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\"\n)\nif conv:\n    print(f\"Found: {conv.title}\")\nelse:\n    print(\"Conversation not found\")\n</code></pre> Performance <ul> <li>Time: O(N) where N = conversations in file (streaming search)</li> <li>Memory: O(1) for file size, O(M) for single conversation</li> <li>Early termination: Returns immediately when match found</li> </ul> Source code in <code>src/echomine/adapters/openai.py</code> <pre><code>def get_conversation_by_id(\n    self,\n    file_path: Path,\n    conversation_id: str,\n) -&gt; Conversation | None:\n    \"\"\"Retrieve specific conversation by UUID (FR-155, FR-217, FR-356).\n\n    Uses streaming search for memory efficiency - O(N) time, O(1) memory.\n    For large files with frequent ID lookups, consider building an index.\n\n    Args:\n        file_path: Path to OpenAI export JSON file\n        conversation_id: UUID of conversation to retrieve\n\n    Returns:\n        Conversation object if found, None otherwise (FR-155)\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        ParseError: If JSON is malformed\n\n    Example:\n        ```python\n        adapter = OpenAIAdapter()\n        conv = adapter.get_conversation_by_id(\n            Path(\"export.json\"),\n            \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\"\n        )\n        if conv:\n            print(f\"Found: {conv.title}\")\n        else:\n            print(\"Conversation not found\")\n        ```\n\n    Performance:\n        - Time: O(N) where N = conversations in file (streaming search)\n        - Memory: O(1) for file size, O(M) for single conversation\n        - Early termination: Returns immediately when match found\n    \"\"\"\n    # Stream conversations and return first match\n    for conversation in self.stream_conversations(file_path):\n        if conversation.id == conversation_id:\n            return conversation\n\n    # Not found - return None per FR-155\n    return None\n</code></pre>"},{"location":"api/adapters/openai/#echomine.adapters.openai.OpenAIAdapter.get_message_by_id","title":"get_message_by_id","text":"<pre><code>get_message_by_id(\n    file_path: Path, message_id: str, *, conversation_id: str | None = None\n) -&gt; tuple[Message, Conversation] | None\n</code></pre> <p>Retrieve specific message by UUID with parent conversation context.</p> <p>Searches for a message by ID, optionally scoped to a specific conversation for performance optimization. Returns both the message and its parent conversation to provide full context.</p> <p>Uses streaming search for memory efficiency - O(1) memory usage.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to OpenAI export JSON file</p> required <code>message_id</code> <code>str</code> <p>UUID of message to retrieve</p> required <code>conversation_id</code> <code>str | None</code> <p>Optional conversation UUID to scope search (performance hint)</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Message, Conversation] | None</code> <p>Tuple of (Message, Conversation) if found, None otherwise.</p> <code>tuple[Message, Conversation] | None</code> <p>The Conversation is the parent containing the message.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>ParseError</code> <p>If JSON is malformed</p> Example <pre><code>adapter = OpenAIAdapter()\n\n# Search with conversation hint (faster)\nresult = adapter.get_message_by_id(\n    Path(\"export.json\"),\n    \"msg-123\",\n    conversation_id=\"conv-456\"\n)\n\n# Search all conversations (slower)\nresult = adapter.get_message_by_id(\n    Path(\"export.json\"),\n    \"msg-123\"\n)\n\nif result:\n    message, conversation = result\n    print(f\"Message: {message.content}\")\n    print(f\"From conversation: {conversation.title}\")\nelse:\n    print(\"Message not found\")\n</code></pre> Performance <ul> <li>With conversation_id:<ul> <li>Time: O(N) where N = conversations until match</li> <li>Memory: O(1) for file size, O(M) for single conversation</li> </ul> </li> <li>Without conversation_id:<ul> <li>Time: O(N*M) where N = conversations, M = messages per conversation</li> <li>Memory: O(1) for file size, O(M) for single conversation</li> </ul> </li> <li>Early termination: Returns immediately when match found</li> </ul> Design Notes <p>Returns tuple instead of just Message to provide conversation context (title, timestamps, other messages) which is valuable for CLI display and analysis workflows.</p> Source code in <code>src/echomine/adapters/openai.py</code> <pre><code>def get_message_by_id(\n    self,\n    file_path: Path,\n    message_id: str,\n    *,\n    conversation_id: str | None = None,\n) -&gt; tuple[Message, Conversation] | None:\n    \"\"\"Retrieve specific message by UUID with parent conversation context.\n\n    Searches for a message by ID, optionally scoped to a specific conversation\n    for performance optimization. Returns both the message and its parent\n    conversation to provide full context.\n\n    Uses streaming search for memory efficiency - O(1) memory usage.\n\n    Args:\n        file_path: Path to OpenAI export JSON file\n        message_id: UUID of message to retrieve\n        conversation_id: Optional conversation UUID to scope search (performance hint)\n\n    Returns:\n        Tuple of (Message, Conversation) if found, None otherwise.\n        The Conversation is the parent containing the message.\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        ParseError: If JSON is malformed\n\n    Example:\n        ```python\n        adapter = OpenAIAdapter()\n\n        # Search with conversation hint (faster)\n        result = adapter.get_message_by_id(\n            Path(\"export.json\"),\n            \"msg-123\",\n            conversation_id=\"conv-456\"\n        )\n\n        # Search all conversations (slower)\n        result = adapter.get_message_by_id(\n            Path(\"export.json\"),\n            \"msg-123\"\n        )\n\n        if result:\n            message, conversation = result\n            print(f\"Message: {message.content}\")\n            print(f\"From conversation: {conversation.title}\")\n        else:\n            print(\"Message not found\")\n        ```\n\n    Performance:\n        - With conversation_id:\n            - Time: O(N) where N = conversations until match\n            - Memory: O(1) for file size, O(M) for single conversation\n        - Without conversation_id:\n            - Time: O(N*M) where N = conversations, M = messages per conversation\n            - Memory: O(1) for file size, O(M) for single conversation\n        - Early termination: Returns immediately when match found\n\n    Design Notes:\n        Returns tuple instead of just Message to provide conversation context\n        (title, timestamps, other messages) which is valuable for CLI display\n        and analysis workflows.\n    \"\"\"\n    # If conversation_id provided, search only that conversation\n    if conversation_id is not None:\n        conv = self.get_conversation_by_id(file_path, conversation_id)\n        if conv is not None:\n            msg = conv.get_message_by_id(message_id)\n            if msg is not None:\n                return (msg, conv)\n        return None\n\n    # Otherwise, stream all conversations and search each\n    for conv in self.stream_conversations(file_path):\n        msg = conv.get_message_by_id(message_id)\n        if msg is not None:\n            return (msg, conv)\n\n    # Not found in any conversation\n    return None\n</code></pre>"},{"location":"api/adapters/openai/#usage-examples","title":"Usage Examples","text":""},{"location":"api/adapters/openai/#basic-setup","title":"Basic Setup","text":"<pre><code>from echomine import OpenAIAdapter\nfrom pathlib import Path\n\n# Create adapter (stateless, reusable)\nadapter = OpenAIAdapter()\nexport_file = Path(\"conversations.json\")\n</code></pre>"},{"location":"api/adapters/openai/#stream-all-conversations","title":"Stream All Conversations","text":"<p>Memory-efficient iteration over all conversations:</p> <pre><code># Stream conversations (O(1) memory usage)\nfor conversation in adapter.stream_conversations(export_file):\n    print(f\"[{conversation.created_at.date()}] {conversation.title}\")\n    print(f\"  Messages: {len(conversation.messages)}\")\n    print(f\"  ID: {conversation.id}\")\n</code></pre>"},{"location":"api/adapters/openai/#search-with-keywords","title":"Search with Keywords","text":"<p>Full-text search with BM25 ranking:</p> <pre><code>from echomine.models import SearchQuery\n\n# Create search query\nquery = SearchQuery(\n    keywords=[\"algorithm\", \"leetcode\"],\n    limit=10\n)\n\n# Execute search\nfor result in adapter.search(export_file, query):\n    print(f\"[{result.score:.2f}] {result.conversation.title}\")\n</code></pre>"},{"location":"api/adapters/openai/#get-conversation-by-id","title":"Get Conversation by ID","text":"<p>Retrieve a specific conversation:</p> <pre><code># Get specific conversation\nconversation = adapter.get_conversation_by_id(export_file, \"conv-abc123\")\n\nif conversation:\n    print(f\"Found: {conversation.title}\")\n    print(f\"Messages: {len(conversation.messages)}\")\nelse:\n    print(\"Conversation not found\")\n</code></pre>"},{"location":"api/adapters/openai/#progress-reporting","title":"Progress Reporting","text":"<p>Track progress for long-running operations:</p> <pre><code>def progress_callback(count: int) -&gt; None:\n    \"\"\"Called periodically during processing.\"\"\"\n    if count % 100 == 0:\n        print(f\"Processed {count:,} conversations...\")\n\n# Stream with progress reporting\nfor conversation in adapter.stream_conversations(\n    export_file,\n    progress_callback=progress_callback\n):\n    process(conversation)\n</code></pre>"},{"location":"api/adapters/openai/#graceful-degradation","title":"Graceful Degradation","text":"<p>Handle malformed entries gracefully:</p> <pre><code>skipped_entries = []\n\ndef handle_skipped(conversation_id: str, reason: str) -&gt; None:\n    \"\"\"Called when malformed entry is skipped.\"\"\"\n    skipped_entries.append({\n        \"id\": conversation_id,\n        \"reason\": reason,\n    })\n\n# Stream with skip handler\nfor conversation in adapter.stream_conversations(\n    export_file,\n    on_skip=handle_skipped\n):\n    process(conversation)\n\nif skipped_entries:\n    print(f\"Skipped {len(skipped_entries)} malformed conversations\")\n</code></pre>"},{"location":"api/adapters/openai/#methods","title":"Methods","text":""},{"location":"api/adapters/openai/#stream_conversations","title":"stream_conversations()","text":"<p>Stream all conversations from export file.</p> <p>Signature:</p> <pre><code>def stream_conversations(\n    self,\n    file_path: Path,\n    *,\n    progress_callback: Optional[Callable[[int], None]] = None,\n    on_skip: Optional[Callable[[str, str], None]] = None,\n) -&gt; Iterator[Conversation]:\n    ...\n</code></pre> <p>Parameters:</p> <ul> <li><code>file_path</code>: Path to OpenAI export JSON file</li> <li><code>progress_callback</code>: Optional callback invoked periodically with conversation count</li> <li><code>on_skip</code>: Optional callback invoked when malformed entry is skipped</li> </ul> <p>Returns:</p> <p>Iterator yielding <code>Conversation</code> objects.</p> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If file does not exist</li> <li><code>PermissionError</code>: If file cannot be read</li> <li><code>ParseError</code>: If export format is invalid</li> <li><code>SchemaVersionError</code>: If export schema version is unsupported</li> </ul> <p>Memory Usage: O(1) - constant memory regardless of file size.</p>"},{"location":"api/adapters/openai/#search","title":"search()","text":"<p>Search conversations with BM25 ranking and filtering.</p> <p>Signature:</p> <pre><code>def search(\n    self,\n    file_path: Path,\n    query: SearchQuery,\n    *,\n    progress_callback: Optional[Callable[[int], None]] = None,\n    on_skip: Optional[Callable[[str, str], None]] = None,\n) -&gt; Iterator[SearchResult[Conversation]]:\n    ...\n</code></pre> <p>Parameters:</p> <ul> <li><code>file_path</code>: Path to OpenAI export JSON file</li> <li><code>query</code>: Search parameters (keywords, filters, limit)</li> <li><code>progress_callback</code>: Optional callback for progress reporting</li> <li><code>on_skip</code>: Optional callback for skipped entries</li> </ul> <p>Returns:</p> <p>Iterator yielding <code>SearchResult[Conversation]</code> objects, sorted by relevance score (descending).</p> <p>Raises:</p> <p>Same as <code>stream_conversations()</code>.</p> <p>Performance:</p> <ul> <li>Title-only search: &lt;5 seconds for 10K conversations (metadata-only)</li> <li>Keyword search: &lt;30 seconds for 1.6GB files (full-text with BM25)</li> </ul>"},{"location":"api/adapters/openai/#get_conversation_by_id","title":"get_conversation_by_id()","text":"<p>Retrieve a specific conversation by ID.</p> <p>Signature:</p> <pre><code>def get_conversation_by_id(\n    self,\n    file_path: Path,\n    conversation_id: str,\n) -&gt; Optional[Conversation]:\n    ...\n</code></pre> <p>Parameters:</p> <ul> <li><code>file_path</code>: Path to OpenAI export JSON file</li> <li><code>conversation_id</code>: ID of conversation to retrieve</li> </ul> <p>Returns:</p> <p><code>Conversation</code> if found, <code>None</code> otherwise.</p> <p>Raises:</p> <p>Same as <code>stream_conversations()</code>.</p> <p>Performance: Early termination - stops searching after finding conversation.</p>"},{"location":"api/adapters/openai/#adapter-pattern","title":"Adapter Pattern","text":""},{"location":"api/adapters/openai/#stateless-design","title":"Stateless Design","text":"<p><code>OpenAIAdapter</code> has no <code>__init__</code> parameters and maintains no internal state:</p> <pre><code># \u2705 CORRECT: Reusable adapter\nadapter = OpenAIAdapter()\n\nfor file in export_files:\n    for conv in adapter.stream_conversations(file):\n        process(conv)\n</code></pre> <p>Benefits:</p> <ul> <li>Thread-safe (no shared state)</li> <li>Reusable across multiple files</li> <li>Simple, predictable behavior</li> </ul>"},{"location":"api/adapters/openai/#protocol-implementation","title":"Protocol Implementation","text":"<p>Implements <code>ConversationProvider</code> protocol:</p> <pre><code>from echomine.protocols import ConversationProvider\n\n# Type-safe adapter usage\ndef process_export(\n    adapter: ConversationProvider,  # Works with ANY adapter\n    file_path: Path\n) -&gt; None:\n    for conv in adapter.stream_conversations(file_path):\n        print(conv.title)\n\n# OpenAIAdapter implements protocol\nprocess_export(OpenAIAdapter(), Path(\"export.json\"))\n</code></pre>"},{"location":"api/adapters/openai/#openai-specific-behavior","title":"OpenAI-Specific Behavior","text":""},{"location":"api/adapters/openai/#export-format","title":"Export Format","text":"<p>Expects OpenAI ChatGPT export JSON format:</p> <pre><code>[\n  {\n    \"id\": \"conv-uuid\",\n    \"title\": \"Conversation Title\",\n    \"create_time\": 1704974400.0,\n    \"update_time\": 1704974500.0,\n    \"mapping\": {\n      \"msg-uuid-1\": {\n        \"id\": \"msg-uuid-1\",\n        \"message\": {\n          \"id\": \"msg-uuid-1\",\n          \"author\": {\"role\": \"user\"},\n          \"content\": {\"content_type\": \"text\", \"parts\": [\"Hello\"]},\n          \"create_time\": 1704974410.0\n        },\n        \"parent\": null,\n        \"children\": [\"msg-uuid-2\"]\n      }\n    }\n  }\n]\n</code></pre>"},{"location":"api/adapters/openai/#metadata-mapping","title":"Metadata Mapping","text":"<p>Provider-specific fields stored in <code>conversation.metadata</code>:</p> <ul> <li><code>openai_model</code>: Model used (e.g., \"gpt-4\")</li> <li><code>openai_conversation_template_id</code>: Template ID</li> <li><code>openai_plugin_ids</code>: List of plugin IDs used</li> <li><code>openai_moderation_results</code>: Moderation results (if any)</li> </ul> <p>Example:</p> <pre><code>conversation = adapter.get_conversation_by_id(file_path, \"conv-123\")\nmodel = conversation.metadata.get(\"openai_model\", \"unknown\")\nprint(f\"Model: {model}\")\n</code></pre>"},{"location":"api/adapters/openai/#role-normalization","title":"Role Normalization","text":"<p>OpenAI roles are already normalized (no mapping needed):</p> <ul> <li>\"user\" \u2192 \"user\"</li> <li>\"assistant\" \u2192 \"assistant\"</li> <li>\"system\" \u2192 \"system\"</li> </ul>"},{"location":"api/adapters/openai/#error-handling","title":"Error Handling","text":""},{"location":"api/adapters/openai/#exceptions","title":"Exceptions","text":"<pre><code>from echomine import (\n    ParseError,          # Malformed JSON/structure\n    ValidationError,     # Invalid data\n    SchemaVersionError,  # Unsupported version\n)\n\ntry:\n    for conv in adapter.stream_conversations(file_path):\n        process(conv)\nexcept ParseError as e:\n    print(f\"Export file corrupted: {e}\")\nexcept SchemaVersionError as e:\n    print(f\"Unsupported export version: {e}\")\nexcept FileNotFoundError:\n    print(f\"File not found: {file_path}\")\n</code></pre>"},{"location":"api/adapters/openai/#graceful-degradation_1","title":"Graceful Degradation","text":"<p>Malformed conversations are skipped with warnings logged:</p> <pre><code># Skipped entries logged as WARNING\n# Processing continues for valid entries\nfor conv in adapter.stream_conversations(file_path):\n    # Only valid conversations yielded\n    process(conv)\n</code></pre>"},{"location":"api/adapters/openai/#concurrency","title":"Concurrency","text":""},{"location":"api/adapters/openai/#thread-safety","title":"Thread Safety","text":"<ul> <li>Adapter instances: Thread-safe (stateless)</li> <li>Iterators: NOT thread-safe (each thread needs its own)</li> </ul> <pre><code>from threading import Thread\n\nadapter = OpenAIAdapter()  # SAFE: Share adapter\n\ndef worker(thread_id):\n    # SAFE: Each thread creates its own iterator\n    for conv in adapter.stream_conversations(file_path):\n        process(conv, thread_id)\n\nthreads = [Thread(target=worker, args=(i,)) for i in range(4)]\n</code></pre>"},{"location":"api/adapters/openai/#multi-process-safety","title":"Multi-Process Safety","text":"<p>Multiple processes can read the same file concurrently:</p> <pre><code>from multiprocessing import Process\n\ndef worker(process_id):\n    adapter = OpenAIAdapter()  # Each process has its own adapter\n    for conv in adapter.stream_conversations(file_path):\n        process(conv, process_id)\n\nprocesses = [Process(target=worker, args=(i,)) for i in range(4)]\n</code></pre>"},{"location":"api/adapters/openai/#performance","title":"Performance","text":""},{"location":"api/adapters/openai/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>O(1) memory usage: Constant memory regardless of file size</li> <li>Streaming: Uses ijson for incremental parsing</li> <li>No buffering: Yields conversations as they're parsed</li> </ul>"},{"location":"api/adapters/openai/#speed","title":"Speed","text":"<ul> <li>10K conversations: &lt;5 seconds for listing (metadata-only)</li> <li>1.6GB file: &lt;30 seconds for keyword search</li> <li>Early termination: <code>get_conversation_by_id</code> stops after finding match</li> </ul>"},{"location":"api/adapters/openai/#related","title":"Related","text":"<ul> <li>ConversationProvider Protocol: Protocol definition</li> <li>Conversation Model: Result type</li> <li>SearchQuery: Search parameters</li> </ul>"},{"location":"api/adapters/openai/#see-also","title":"See Also","text":"<ul> <li>Library Usage Guide</li> <li>Architecture</li> </ul>"},{"location":"api/adapters/protocols/","title":"Conversation Provider Protocol","text":"<p>Protocol definition for conversation export adapters.</p>"},{"location":"api/adapters/protocols/#overview","title":"Overview","text":"<p><code>ConversationProvider</code> is a Protocol (PEP 544) that defines the interface all conversation adapters must implement. This enables type-safe, provider-agnostic code.</p>"},{"location":"api/adapters/protocols/#api-reference","title":"API Reference","text":""},{"location":"api/adapters/protocols/#echomine.models.protocols.ConversationProvider","title":"ConversationProvider","text":"<p>               Bases: <code>Protocol[ConversationT]</code></p> <p>Generic protocol for AI provider export parsers (per FR-151).</p> <p>All adapters (OpenAI, Anthropic, Google, etc.) MUST implement this protocol. This ensures a unified interface regardless of export format, enabling library consumers to write provider-agnostic code.</p> Type Parameter <p>ConversationT: Provider-specific conversation type (must implement BaseConversation)               Examples: Conversation (OpenAI), ClaudeConversation (Anthropic)</p> <p>Adapter Design Principles (per FR-113, FR-114, FR-115, FR-120):     - Stateless: No configuration parameters in init     - Reusable: Same adapter instance can process different files     - Lightweight: Instantiation should be instant (no I/O)     - NOT context managers: Adapters don't implement enter/exit</p> <p>Thread Safety (per FR-098, FR-099, FR-100, FR-101):     - Adapter instances MUST be thread-safe (safe to share across threads)     - Iterators returned by methods MUST NOT be shared across threads     - Each thread MUST create its own iterator by calling methods separately</p> <p>Iterator Lifecycle (per FR-116, FR-117, FR-118, FR-119):     - Iterators are single-use (exhausted after completion)     - Multiple calls return independent iterators (not resume)     - File handles closed even if iteration stops early     - Context managers guarantee cleanup in ALL scenarios</p> <p>Resource Management (per FR-130, FR-131, FR-132, FR-133):     - Methods use try/finally for cleanup guarantees     - File handles managed via context managers     - Cleanup occurs: normal completion, early break, exceptions, GC     - NO del methods for cleanup</p> <p>Backpressure (per FR-134, FR-135, FR-136, FR-137):     - NO explicit backpressure mechanisms     - Generators yield one item at a time     - Memory usage constant regardless of consumer speed     - Consumer controls parsing pace (pull-based)</p> Example <pre><code>from pathlib import Path\nfrom echomine import OpenAIAdapter, SearchQuery\n\nadapter = OpenAIAdapter()\n\n# List all conversations\nfor conversation in adapter.stream_conversations(Path(\"export.json\")):\n    print(conversation.title)\n\n# Search with filters\nquery = SearchQuery(keywords=[\"algorithm\"], limit=10)\nfor result in adapter.search(Path(\"export.json\"), query):\n    print(f\"{result.score:.2f}: {result.conversation.title}\")\n\n# Get specific conversation\nconv = adapter.get_conversation_by_id(Path(\"export.json\"), \"conv-uuid-123\")\nif conv:\n    print(conv.title)\n</code></pre> Requirements <ul> <li>FR-151: Generic protocol with ConversationT type parameter</li> <li>FR-215-221: Complete method signatures with proper types</li> <li>FR-027: All adapters must implement this protocol</li> </ul>"},{"location":"api/adapters/protocols/#echomine.models.protocols.ConversationProvider.stream_conversations","title":"stream_conversations","text":"<pre><code>stream_conversations(\n    file_path: Path,\n    *,\n    progress_callback: ProgressCallback | None = None,\n    on_skip: OnSkipCallback | None = None,\n) -&gt; Iterator[ConversationT]\n</code></pre> <p>Stream conversations one at a time from export file (per FR-151, FR-153).</p> <p>Memory Contract: MUST use streaming (ijson or equivalent) to avoid loading entire file into memory. Memory usage MUST be constant regardless of file size.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Absolute path to export file (e.g., /path/to/conversations.json)</p> required <code>progress_callback</code> <code>ProgressCallback | None</code> <p>Optional callback(count) called periodically with item count</p> <code>None</code> <code>on_skip</code> <code>OnSkipCallback | None</code> <p>Optional callback(conversation_id, reason) when malformed entries skipped</p> <code>None</code> <p>Yields:</p> Name Type Description <code>ConversationT</code> <code>ConversationT</code> <p>Provider-specific conversation objects one at a time</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file_path does not exist (per FR-049, FR-033)</p> <code>PermissionError</code> <p>If file_path is not readable (per FR-051, FR-033)</p> <code>ParseError</code> <p>If export format is invalid or corrupted (per FR-036)</p> <code>SchemaVersionError</code> <p>If export schema version is unsupported (per FR-036, FR-085)</p> <code>ValidationError</code> <p>If conversation data fails Pydantic validation (per FR-036, FR-054)</p> <p>Exception Handling (per FR-042, FR-045, FR-046, FR-047, FR-048):     - MUST fail fast, no retries     - MUST use context managers for file handle cleanup     - MUST include conversation index in exception messages     - MUST NOT raise StopIteration explicitly (use return)</p> <p>Progress Reporting (per FR-068, FR-069):     - progress_callback called every 100 items OR 100ms, whichever comes first     - Callback receives current item count (not percentage)</p> <p>Graceful Degradation (per FR-004, FR-105):     - Malformed entries skipped with WARNING log     - on_skip callback invoked with conversation_id and reason     - Processing continues after skip</p> Thread Safety <p>This iterator MUST NOT be shared across threads (per FR-099). Each thread must call this method to get its own iterator.</p> Requirements <ul> <li>FR-151: Generic return type (Iterator[ConversationT])</li> <li>FR-153: Streaming memory contract</li> <li>FR-076, FR-077: Progress callback support</li> <li>FR-106, FR-107: Skip callback support</li> </ul> Source code in <code>src/echomine/models/protocols.py</code> <pre><code>def stream_conversations(\n    self,\n    file_path: Path,\n    *,\n    progress_callback: ProgressCallback | None = None,\n    on_skip: OnSkipCallback | None = None,\n) -&gt; Iterator[ConversationT]:\n    \"\"\"Stream conversations one at a time from export file (per FR-151, FR-153).\n\n    Memory Contract: MUST use streaming (ijson or equivalent) to avoid loading\n    entire file into memory. Memory usage MUST be constant regardless of file size.\n\n    Args:\n        file_path: Absolute path to export file (e.g., /path/to/conversations.json)\n        progress_callback: Optional callback(count) called periodically with item count\n        on_skip: Optional callback(conversation_id, reason) when malformed entries skipped\n\n    Yields:\n        ConversationT: Provider-specific conversation objects one at a time\n\n    Raises:\n        FileNotFoundError: If file_path does not exist (per FR-049, FR-033)\n        PermissionError: If file_path is not readable (per FR-051, FR-033)\n        ParseError: If export format is invalid or corrupted (per FR-036)\n        SchemaVersionError: If export schema version is unsupported (per FR-036, FR-085)\n        ValidationError: If conversation data fails Pydantic validation (per FR-036, FR-054)\n\n    Exception Handling (per FR-042, FR-045, FR-046, FR-047, FR-048):\n        - MUST fail fast, no retries\n        - MUST use context managers for file handle cleanup\n        - MUST include conversation index in exception messages\n        - MUST NOT raise StopIteration explicitly (use return)\n\n    Progress Reporting (per FR-068, FR-069):\n        - progress_callback called every 100 items OR 100ms, whichever comes first\n        - Callback receives current item count (not percentage)\n\n    Graceful Degradation (per FR-004, FR-105):\n        - Malformed entries skipped with WARNING log\n        - on_skip callback invoked with conversation_id and reason\n        - Processing continues after skip\n\n    Thread Safety:\n        This iterator MUST NOT be shared across threads (per FR-099).\n        Each thread must call this method to get its own iterator.\n\n    Requirements:\n        - FR-151: Generic return type (Iterator[ConversationT])\n        - FR-153: Streaming memory contract\n        - FR-076, FR-077: Progress callback support\n        - FR-106, FR-107: Skip callback support\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/adapters/protocols/#echomine.models.protocols.ConversationProvider.search","title":"search","text":"<pre><code>search(\n    file_path: Path,\n    query: SearchQuery,\n    *,\n    progress_callback: ProgressCallback | None = None,\n    on_skip: OnSkipCallback | None = None,\n) -&gt; Iterator[SearchResult[ConversationT]]\n</code></pre> <p>Search conversations matching query criteria with relevance ranking.</p> <p>Ranking Contract: Results MUST be sorted by relevance_score (descending). BM25 or equivalent algorithm MUST be used for keyword ranking (FR-317).</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to export file</p> required <code>query</code> <code>SearchQuery</code> <p>Search parameters (keywords, title filter, date range, limit)</p> required <code>progress_callback</code> <code>ProgressCallback | None</code> <p>Optional callback(count) for progress reporting</p> <code>None</code> <code>on_skip</code> <code>OnSkipCallback | None</code> <p>Optional callback(conversation_id, reason) when entries skipped</p> <code>None</code> <p>Yields:</p> Type Description <code>SearchResult[ConversationT]</code> <p>SearchResult[ConversationT]: Matched conversations with provider-specific type,                          sorted by relevance (highest first)</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file_path does not exist (per FR-049)</p> <code>PermissionError</code> <p>If file_path is not readable (per FR-051)</p> <code>ParseError</code> <p>If export format is invalid (per FR-036)</p> <code>SchemaVersionError</code> <p>If schema version unsupported (per FR-036, FR-085)</p> <code>ValidationError</code> <p>If query or conversation data invalid (per FR-036, FR-054, FR-055)</p> Thread Safety <p>This iterator MUST NOT be shared across threads (per FR-099).</p> Requirements <ul> <li>FR-152: Generic return type (Iterator[SearchResult[ConversationT]])</li> <li>FR-153: Memory-efficient streaming</li> <li>FR-317-326: BM25 relevance ranking</li> </ul> Source code in <code>src/echomine/models/protocols.py</code> <pre><code>def search(\n    self,\n    file_path: Path,\n    query: SearchQuery,\n    *,\n    progress_callback: ProgressCallback | None = None,\n    on_skip: OnSkipCallback | None = None,\n) -&gt; Iterator[SearchResult[ConversationT]]:\n    \"\"\"Search conversations matching query criteria with relevance ranking.\n\n    Ranking Contract: Results MUST be sorted by relevance_score (descending).\n    BM25 or equivalent algorithm MUST be used for keyword ranking (FR-317).\n\n    Args:\n        file_path: Path to export file\n        query: Search parameters (keywords, title filter, date range, limit)\n        progress_callback: Optional callback(count) for progress reporting\n        on_skip: Optional callback(conversation_id, reason) when entries skipped\n\n    Yields:\n        SearchResult[ConversationT]: Matched conversations with provider-specific type,\n                                     sorted by relevance (highest first)\n\n    Raises:\n        FileNotFoundError: If file_path does not exist (per FR-049)\n        PermissionError: If file_path is not readable (per FR-051)\n        ParseError: If export format is invalid (per FR-036)\n        SchemaVersionError: If schema version unsupported (per FR-036, FR-085)\n        ValidationError: If query or conversation data invalid (per FR-036, FR-054, FR-055)\n\n    Thread Safety:\n        This iterator MUST NOT be shared across threads (per FR-099).\n\n    Requirements:\n        - FR-152: Generic return type (Iterator[SearchResult[ConversationT]])\n        - FR-153: Memory-efficient streaming\n        - FR-317-326: BM25 relevance ranking\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/adapters/protocols/#echomine.models.protocols.ConversationProvider.get_conversation_by_id","title":"get_conversation_by_id","text":"<pre><code>get_conversation_by_id(\n    file_path: Path, conversation_id: str\n) -&gt; ConversationT | None\n</code></pre> <p>Retrieve specific conversation by UUID (per FR-151, FR-153, FR-155).</p> <p>Performance Contract: MAY use streaming or index lookup. For large files, consider building an in-memory index (conversation_id -&gt; file offset) on first call.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to export file</p> required <code>conversation_id</code> <code>str</code> <p>Conversation UUID from export</p> required <p>Returns:</p> Name Type Description <code>ConversationT</code> <code>ConversationT | None</code> <p>Provider-specific conversation object if found</p> <code>None</code> <code>ConversationT | None</code> <p>If conversation_id not found in export (per FR-155)</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file_path does not exist (per FR-049)</p> <code>PermissionError</code> <p>If file_path is not readable (per FR-051)</p> <code>ParseError</code> <p>If export format is invalid (per FR-036)</p> <code>SchemaVersionError</code> <p>If schema version unsupported (per FR-036, FR-085)</p> Requirements <ul> <li>FR-151: Generic return type (Optional[ConversationT])</li> <li>FR-153: Memory-efficient (O(1) or streaming until found)</li> <li>FR-155: Returns None if not found (not exception)</li> </ul> Source code in <code>src/echomine/models/protocols.py</code> <pre><code>def get_conversation_by_id(\n    self,\n    file_path: Path,\n    conversation_id: str,\n) -&gt; ConversationT | None:\n    \"\"\"Retrieve specific conversation by UUID (per FR-151, FR-153, FR-155).\n\n    Performance Contract: MAY use streaming or index lookup. For large files,\n    consider building an in-memory index (conversation_id -&gt; file offset) on first call.\n\n    Args:\n        file_path: Path to export file\n        conversation_id: Conversation UUID from export\n\n    Returns:\n        ConversationT: Provider-specific conversation object if found\n        None: If conversation_id not found in export (per FR-155)\n\n    Raises:\n        FileNotFoundError: If file_path does not exist (per FR-049)\n        PermissionError: If file_path is not readable (per FR-051)\n        ParseError: If export format is invalid (per FR-036)\n        SchemaVersionError: If schema version unsupported (per FR-036, FR-085)\n\n    Requirements:\n        - FR-151: Generic return type (Optional[ConversationT])\n        - FR-153: Memory-efficient (O(1) or streaming until found)\n        - FR-155: Returns None if not found (not exception)\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/adapters/protocols/#usage-examples","title":"Usage Examples","text":""},{"location":"api/adapters/protocols/#type-safe-adapter-usage","title":"Type-Safe Adapter Usage","text":"<p>Write code that works with any adapter:</p> <pre><code>from echomine.protocols import ConversationProvider\nfrom echomine.models import Conversation\nfrom pathlib import Path\n\ndef process_export(\n    adapter: ConversationProvider,  # Works with ANY adapter\n    file_path: Path\n) -&gt; int:\n    \"\"\"Process export file using any provider adapter.\"\"\"\n    count = 0\n    for conv in adapter.stream_conversations(file_path):\n        print(f\"{conv.title}: {len(conv.messages)} messages\")\n        count += 1\n    return count\n\n\n# Works with OpenAI\nfrom echomine import OpenAIAdapter\ncount = process_export(OpenAIAdapter(), Path(\"chatgpt.json\"))\n\n# Works with future providers\n# from echomine import ClaudeAdapter\n# count = process_export(ClaudeAdapter(), Path(\"claude.jsonl\"))\n</code></pre>"},{"location":"api/adapters/protocols/#adapter-registry-pattern","title":"Adapter Registry Pattern","text":"<p>Build multi-provider systems:</p> <pre><code>from echomine import OpenAIAdapter\nfrom echomine.protocols import ConversationProvider\n\n# Adapter registry\nADAPTERS: dict[str, type[ConversationProvider]] = {\n    \"openai\": OpenAIAdapter,\n    # Future providers:\n    # \"anthropic\": ClaudeAdapter,\n    # \"google\": GeminiAdapter,\n}\n\ndef ingest_ai_export(provider: str, export_file: Path):\n    \"\"\"Ingest any AI provider export.\"\"\"\n    adapter_class = ADAPTERS.get(provider)\n    if not adapter_class:\n        raise ValueError(f\"Unknown provider: {provider}\")\n\n    adapter = adapter_class()\n\n    # Same logic works for all providers!\n    count = 0\n    for conv in adapter.stream_conversations(export_file):\n        knowledge_base.add(conv)\n        count += 1\n\n    return count\n\n\n# Usage\ningest_ai_export(\"openai\", Path(\"chatgpt_export.json\"))\n# Future: ingest_ai_export(\"anthropic\", Path(\"claude_export.jsonl\"))\n</code></pre>"},{"location":"api/adapters/protocols/#protocol-methods","title":"Protocol Methods","text":""},{"location":"api/adapters/protocols/#stream_conversations","title":"stream_conversations()","text":"<p>Stream all conversations from export file.</p> <p>Signature:</p> <pre><code>def stream_conversations(\n    self,\n    file_path: Path,\n    *,\n    progress_callback: Optional[Callable[[int], None]] = None,\n    on_skip: Optional[Callable[[str, str], None]] = None,\n) -&gt; Iterator[ConversationT]:\n    ...\n</code></pre> <p>Required for all adapters.</p>"},{"location":"api/adapters/protocols/#search","title":"search()","text":"<p>Search conversations with filtering and ranking.</p> <p>Signature:</p> <pre><code>def search(\n    self,\n    file_path: Path,\n    query: SearchQuery,\n    *,\n    progress_callback: Optional[Callable[[int], None]] = None,\n    on_skip: Optional[Callable[[str, str], None]] = None,\n) -&gt; Iterator[SearchResult[ConversationT]]:\n    ...\n</code></pre> <p>Required for all adapters.</p>"},{"location":"api/adapters/protocols/#get_conversation_by_id","title":"get_conversation_by_id()","text":"<p>Retrieve specific conversation by ID.</p> <p>Signature:</p> <pre><code>def get_conversation_by_id(\n    self,\n    file_path: Path,\n    conversation_id: str,\n) -&gt; Optional[ConversationT]:\n    ...\n</code></pre> <p>Required for all adapters.</p>"},{"location":"api/adapters/protocols/#implementing-custom-adapters","title":"Implementing Custom Adapters","text":""},{"location":"api/adapters/protocols/#step-1-define-adapter-class","title":"Step 1: Define Adapter Class","text":"<pre><code>from typing import Iterator, Optional\nfrom pathlib import Path\nfrom echomine.models import Conversation, SearchQuery, SearchResult\n\nclass ClaudeAdapter:\n    \"\"\"Adapter for Anthropic Claude exports.\"\"\"\n\n    def stream_conversations(\n        self,\n        file_path: Path,\n        *,\n        progress_callback: Optional[Callable[[int], None]] = None,\n        on_skip: Optional[Callable[[str, str], None]] = None,\n    ) -&gt; Iterator[Conversation]:\n        \"\"\"Stream conversations from Claude export.\"\"\"\n        # Implementation here\n        pass\n\n    def search(\n        self,\n        file_path: Path,\n        query: SearchQuery,\n        *,\n        progress_callback: Optional[Callable[[int], None]] = None,\n        on_skip: Optional[Callable[[str, str], None]] = None,\n    ) -&gt; Iterator[SearchResult[Conversation]]:\n        \"\"\"Search Claude conversations.\"\"\"\n        # Implementation here\n        pass\n\n    def get_conversation_by_id(\n        self,\n        file_path: Path,\n        conversation_id: str,\n    ) -&gt; Optional[Conversation]:\n        \"\"\"Get Claude conversation by ID.\"\"\"\n        # Implementation here\n        pass\n</code></pre>"},{"location":"api/adapters/protocols/#step-2-normalize-provider-specific-data","title":"Step 2: Normalize Provider-Specific Data","text":"<p>Map provider-specific fields to standard <code>Conversation</code> model:</p> <pre><code># Claude-specific parsing\ndef _parse_claude_conversation(self, raw_data: dict) -&gt; Conversation:\n    \"\"\"Parse Claude export format.\"\"\"\n    return Conversation(\n        id=raw_data[\"conversation_id\"],\n        title=raw_data[\"name\"],\n        created_at=self._parse_timestamp(raw_data[\"created_at\"]),\n        messages=self._parse_messages(raw_data[\"messages\"]),\n        metadata={\n            \"claude_model\": raw_data.get(\"model\", \"unknown\"),\n            \"claude_workspace_id\": raw_data.get(\"workspace_id\"),\n        }\n    )\n</code></pre>"},{"location":"api/adapters/protocols/#step-3-normalize-roles","title":"Step 3: Normalize Roles","text":"<p>Map provider-specific roles to standard roles:</p> <pre><code>def _normalize_role(self, claude_role: str) -&gt; Literal[\"user\", \"assistant\", \"system\"]:\n    \"\"\"Normalize Claude roles to standard roles.\"\"\"\n    role_mapping = {\n        \"human\": \"user\",\n        \"assistant\": \"assistant\",\n        # Claude doesn't have system role\n    }\n    return role_mapping.get(claude_role, \"user\")\n</code></pre>"},{"location":"api/adapters/protocols/#step-4-implement-streaming","title":"Step 4: Implement Streaming","text":"<p>Use generators for memory efficiency:</p> <pre><code>import ijson\n\ndef stream_conversations(\n    self,\n    file_path: Path,\n    *,\n    progress_callback: Optional[Callable[[int], None]] = None,\n    on_skip: Optional[Callable[[str, str], None]] = None,\n) -&gt; Iterator[Conversation]:\n    \"\"\"Stream conversations with O(1) memory.\"\"\"\n    with open(file_path, \"rb\") as f:\n        parser = ijson.items(f, \"item\")  # Streaming parser\n        count = 0\n\n        for item in parser:\n            try:\n                conversation = self._parse_claude_conversation(item)\n                yield conversation\n                count += 1\n\n                # Progress reporting\n                if progress_callback and count % 100 == 0:\n                    progress_callback(count)\n\n            except ValidationError as e:\n                # Graceful degradation\n                if on_skip:\n                    on_skip(item.get(\"conversation_id\", \"unknown\"), str(e))\n                continue\n</code></pre>"},{"location":"api/adapters/protocols/#step-5-type-checking","title":"Step 5: Type Checking","text":"<p>Verify protocol compliance with mypy:</p> <pre><code>from echomine.protocols import ConversationProvider\n\n# This line verifies ClaudeAdapter implements the protocol\nadapter: ConversationProvider = ClaudeAdapter()  # Type-checks!\n</code></pre>"},{"location":"api/adapters/protocols/#design-guidelines","title":"Design Guidelines","text":""},{"location":"api/adapters/protocols/#stateless-design","title":"Stateless Design","text":"<p>Adapters should have no <code>__init__</code> parameters:</p> <pre><code># \u2705 CORRECT: Stateless\nclass ClaudeAdapter:\n    def stream_conversations(self, file_path: Path) -&gt; Iterator[Conversation]:\n        pass\n\n# \u274c WRONG: Stateful\nclass ClaudeAdapter:\n    def __init__(self, file_path: Path):  # NO!\n        self.file_path = file_path\n</code></pre>"},{"location":"api/adapters/protocols/#memory-efficiency","title":"Memory Efficiency","text":"<p>Always use streaming (generators, not lists):</p> <pre><code># \u2705 CORRECT: Generator\ndef stream_conversations(self, file_path: Path) -&gt; Iterator[Conversation]:\n    for item in parser:\n        yield conversation\n\n# \u274c WRONG: List (loads entire file)\ndef stream_conversations(self, file_path: Path) -&gt; list[Conversation]:\n    return [conversation for item in parser]\n</code></pre>"},{"location":"api/adapters/protocols/#error-handling","title":"Error Handling","text":"<ul> <li>Fail fast on unrecoverable errors (file not found, unsupported version)</li> <li>Graceful degradation on data errors (skip malformed entries)</li> </ul>"},{"location":"api/adapters/protocols/#progress-reporting","title":"Progress Reporting","text":"<p>Invoke callbacks every 100 items OR 100ms (whichever comes first):</p> <pre><code>import time\n\nlast_progress_time = time.monotonic()\ncount = 0\n\nfor item in parser:\n    count += 1\n    current_time = time.monotonic()\n\n    if progress_callback and (count % 100 == 0 or current_time - last_progress_time &gt;= 0.1):\n        progress_callback(count)\n        last_progress_time = current_time\n</code></pre>"},{"location":"api/adapters/protocols/#protocol-benefits","title":"Protocol Benefits","text":"<ol> <li>Type Safety: mypy validates adapter implementations</li> <li>Interchangeability: Swap providers without code changes</li> <li>Testability: Mock adapters for testing</li> <li>Documentation: Self-documenting interface</li> </ol>"},{"location":"api/adapters/protocols/#related","title":"Related","text":"<ul> <li>OpenAI Adapter: Reference implementation</li> <li>Conversation Model: Standard conversation model</li> <li>SearchQuery: Search parameters</li> </ul>"},{"location":"api/adapters/protocols/#see-also","title":"See Also","text":"<ul> <li>Architecture</li> <li>Library Usage</li> </ul>"},{"location":"api/cli/commands/","title":"CLI Commands","text":"<p>Command-line interface built on the Echomine library.</p>"},{"location":"api/cli/commands/#overview","title":"Overview","text":"<p>The Echomine CLI provides a thin wrapper over the library API for terminal use. All CLI commands use the same library functions available programmatically.</p> <p>See CLI Usage Guide for comprehensive command reference.</p>"},{"location":"api/cli/commands/#architecture","title":"Architecture","text":"<p>The CLI follows the library-first architecture principle:</p> <pre><code>User Input (CLI)\n    \u2193\nTyper Command Handler\n    \u2193\nLibrary Function (OpenAIAdapter)\n    \u2193\nRich Formatter (Terminal Output)\n    \u2193\nstdout/stderr\n</code></pre> <p>Key Points:</p> <ul> <li>CLI contains NO business logic</li> <li>All operations delegated to library</li> <li>Rich formatting for human-readable output</li> <li>JSON mode for programmatic use</li> </ul>"},{"location":"api/cli/commands/#commands","title":"Commands","text":""},{"location":"api/cli/commands/#list","title":"list","text":"<p>List all conversations in an export file.</p> <p>Usage:</p> <pre><code>echomine list [OPTIONS] FILE_PATH\n</code></pre> <p>Library Equivalent:</p> <pre><code>from echomine import OpenAIAdapter\n\nadapter = OpenAIAdapter()\nfor conversation in adapter.stream_conversations(file_path):\n    print(f\"{conversation.title}: {len(conversation.messages)} messages\")\n</code></pre> <p>See: CLI Usage - list</p>"},{"location":"api/cli/commands/#search","title":"search","text":"<p>Search conversations with keyword matching and relevance ranking.</p> <p>Usage:</p> <pre><code>echomine search [OPTIONS] FILE_PATH\n</code></pre> <p>Library Equivalent:</p> <pre><code>from echomine import OpenAIAdapter, SearchQuery\n\nadapter = OpenAIAdapter()\nquery = SearchQuery(keywords=[\"python\"], limit=10)\nfor result in adapter.search(file_path, query):\n    print(f\"[{result.score:.2f}] {result.conversation.title}\")\n</code></pre> <p>See: CLI Usage - search</p>"},{"location":"api/cli/commands/#export","title":"export","text":"<p>Export a specific conversation to markdown or JSON format.</p> <p>Usage:</p> <pre><code>echomine export [OPTIONS] FILE_PATH CONVERSATION_ID\n</code></pre> <p>Options:</p> <ul> <li><code>--output PATH</code>: Output file path (if not specified, prints to stdout)</li> <li><code>--format TEXT</code>: Export format: <code>markdown</code> (default) or <code>json</code></li> </ul> <p>Library Equivalent:</p> <pre><code>from echomine import OpenAIAdapter\nfrom echomine.exporters import MarkdownExporter, JSONExporter\n\nadapter = OpenAIAdapter()\nconversation = adapter.get_conversation_by_id(file_path, conversation_id)\n\nif conversation:\n    # Markdown export (default)\n    markdown_exporter = MarkdownExporter()\n    markdown = markdown_exporter.export(conversation)\n    print(markdown)\n\n    # JSON export\n    json_exporter = JSONExporter()\n    json_output = json_exporter.export(conversation)\n    print(json_output)\n</code></pre> <p>See: CLI Usage - export</p>"},{"location":"api/cli/commands/#output-formats","title":"Output Formats","text":""},{"location":"api/cli/commands/#human-readable-default","title":"Human-Readable (Default)","text":"<p>Uses Rich library for formatted terminal output:</p> <pre><code>echomine list export.json\n\n# Output:\n# Conversations in export.json\n#\n# [2024-01-15] Python Async Best Practices\n#   Messages: 42\n#   ID: conv-abc123\n# ...\n</code></pre>"},{"location":"api/cli/commands/#json-json-flag","title":"JSON (--json flag)","text":"<p>Machine-readable JSON on stdout:</p> <pre><code>echomine list export.json --json\n\n# Output:\n# {\"conversations\": [...], \"total\": 145}\n</code></pre>"},{"location":"api/cli/commands/#exit-codes","title":"Exit Codes","text":"<p>Standard UNIX exit codes:</p> <ul> <li>0: Success</li> <li>1: Operational error (file not found, parsing error)</li> <li>2: Usage error (invalid arguments)</li> </ul>"},{"location":"api/cli/commands/#implementation","title":"Implementation","text":""},{"location":"api/cli/commands/#typer-application","title":"Typer Application","text":""},{"location":"api/cli/commands/#echomine.cli.app","title":"app","text":"<p>Main CLI application entry point.</p> <p>This module defines the Typer application and main() entry point for the echomine CLI tool.</p> Architecture <ul> <li>Typer application with registered commands</li> <li>main() function as entry point (referenced in pyproject.toml)</li> <li>Minimal error handling (commands handle their own errors)</li> </ul> Constitution Compliance <ul> <li>Principle I: Library-first (CLI delegates to library)</li> <li>CHK031: stdout/stderr separation</li> <li>CHK032: Exit codes 0/1/2</li> </ul> <p>Entry Point Configuration (pyproject.toml):     [project.scripts]     echomine = \"echomine.cli.app:main\"</p> Usage"},{"location":"api/cli/commands/#echomine.cli.app--as-installed-script","title":"As installed script","text":"<p>$ echomine list export.json</p>"},{"location":"api/cli/commands/#echomine.cli.app--as-python-module-development","title":"As Python module (development)","text":"<p>$ python -m echomine.cli.app list export.json</p>"},{"location":"api/cli/commands/#echomine.cli.app.callback","title":"callback","text":"<pre><code>callback(\n    ctx: Context,\n    version: Annotated[\n        bool,\n        Option(\"--version\", \"-v\", help=\"Show version and exit\", is_eager=True),\n    ] = False,\n) -&gt; None\n</code></pre> <p>Echomine CLI - Library-first tool for parsing AI conversation exports.</p> Source code in <code>src/echomine/cli/app.py</code> <pre><code>@app.callback(invoke_without_command=True)\ndef callback(\n    ctx: typer.Context,\n    version: Annotated[\n        bool,\n        typer.Option(\n            \"--version\",\n            \"-v\",\n            help=\"Show version and exit\",\n            is_eager=True,\n        ),\n    ] = False,\n) -&gt; None:\n    \"\"\"Echomine CLI - Library-first tool for parsing AI conversation exports.\"\"\"\n    if version:\n        typer.echo(f\"echomine version {__version__}\")\n        raise typer.Exit(0)\n\n    # Show help if no command provided (unless --version was used)\n    if ctx.invoked_subcommand is None:\n        typer.echo(ctx.get_help())\n        raise typer.Exit(0)\n</code></pre>"},{"location":"api/cli/commands/#echomine.cli.app.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Entry point for CLI application.</p> <p>This function is referenced in pyproject.toml as the console script entry point. It invokes the Typer app and handles any uncaught exceptions (though commands should handle their own errors).</p> Exit Codes <p>0: Success 1: Error (user error, file not found, parse error, etc.) 2: Invalid arguments (Typer handles this)</p> Requirements <ul> <li>CHK032: Consistent exit codes</li> <li>Entry point for installed script</li> </ul> Source code in <code>src/echomine/cli/app.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Entry point for CLI application.\n\n    This function is referenced in pyproject.toml as the console script\n    entry point. It invokes the Typer app and handles any uncaught\n    exceptions (though commands should handle their own errors).\n\n    Exit Codes:\n        0: Success\n        1: Error (user error, file not found, parse error, etc.)\n        2: Invalid arguments (Typer handles this)\n\n    Requirements:\n        - CHK032: Consistent exit codes\n        - Entry point for installed script\n    \"\"\"\n    # Configure UTF-8 encoding for Windows compatibility\n    _configure_encoding()\n\n    try:\n        app()\n    except typer.Exit:  # pragma: no cover\n        # typer.Exit exceptions are raised by commands to set exit codes\n        # Re-raise to preserve exit code\n        raise\n    except KeyboardInterrupt:  # pragma: no cover\n        # User interrupted with Ctrl+C (FR-062)\n        # Exit with code 130 (standard for SIGINT: 128 + 2)\n        typer.echo(\"\", err=True)\n        sys.exit(130)\n    except Exception as e:  # pragma: no cover\n        # Unexpected error not caught by command\n        # This is a safety net - commands should handle their own errors\n        typer.echo(f\"Error: {e}\", err=True)\n        sys.exit(1)\n</code></pre>"},{"location":"api/cli/commands/#command-handlers","title":"Command Handlers","text":"<p>Command handlers in <code>echomine/cli/commands/</code>:</p> <ul> <li><code>list.py</code>: List command handler</li> <li><code>search.py</code>: Search command handler</li> <li><code>export.py</code>: Export command handler</li> </ul>"},{"location":"api/cli/commands/#formatters","title":"Formatters","text":"<p>Terminal formatters in <code>echomine/cli/formatters.py</code>:</p> <ul> <li><code>format_conversation_list()</code>: Format list output</li> <li><code>format_search_results()</code>: Format search output</li> <li><code>format_conversation_export()</code>: Format export output</li> </ul>"},{"location":"api/cli/commands/#design-patterns","title":"Design Patterns","text":""},{"location":"api/cli/commands/#library-first-pattern","title":"Library-First Pattern","text":"<pre><code># CLI command handler (thin wrapper)\ndef list_command(file_path: Path, limit: Optional[int] = None, json: bool = False):\n    # Delegate to library\n    adapter = OpenAIAdapter()\n    conversations = adapter.stream_conversations(file_path)\n\n    if limit:\n        conversations = itertools.islice(conversations, limit)\n\n    # Format output\n    if json:\n        print_json(conversations)  # JSON formatter\n    else:\n        print_table(conversations)  # Rich formatter\n</code></pre>"},{"location":"api/cli/commands/#stdoutstderr-contract","title":"Stdout/Stderr Contract","text":"<pre><code>import sys\n\n# Results to stdout\nprint(json.dumps(results))  # stdout\n\n# Progress to stderr\nprint(\"Processing...\", file=sys.stderr)  # stderr\n\n# Errors to stderr\nprint(f\"Error: {error}\", file=sys.stderr)  # stderr\nsys.exit(1)  # Exit code 1\n</code></pre>"},{"location":"api/cli/commands/#progress-reporting","title":"Progress Reporting","text":"<pre><code>from rich.progress import track\n\ndef list_command(file_path: Path):\n    adapter = OpenAIAdapter()\n\n    # Show progress bar (stderr)\n    conversations = list(adapter.stream_conversations(file_path))\n\n    for conv in track(conversations, description=\"Listing...\"):\n        # Process\n        pass\n</code></pre>"},{"location":"api/cli/commands/#testing-cli","title":"Testing CLI","text":""},{"location":"api/cli/commands/#contract-tests","title":"Contract Tests","text":"<p>CLI contract tests verify stdout/stderr behavior:</p> <pre><code>def test_list_command_json_output(tmp_export_file):\n    \"\"\"List command outputs valid JSON to stdout.\"\"\"\n    result = subprocess.run(\n        [\"echomine\", \"list\", str(tmp_export_file), \"--json\"],\n        capture_output=True,\n        text=True\n    )\n\n    assert result.returncode == 0\n    data = json.loads(result.stdout)  # Validates JSON\n    assert \"conversations\" in data\n</code></pre>"},{"location":"api/cli/commands/#exit-code-tests","title":"Exit Code Tests","text":"<pre><code>def test_list_command_file_not_found():\n    \"\"\"List command exits with code 1 on file not found.\"\"\"\n    result = subprocess.run(\n        [\"echomine\", \"list\", \"nonexistent.json\"],\n        capture_output=True\n    )\n\n    assert result.returncode == 1\n    assert \"not found\" in result.stderr.lower()\n</code></pre>"},{"location":"api/cli/commands/#related","title":"Related","text":"<ul> <li>CLI Usage Guide: Complete CLI reference</li> <li>OpenAI Adapter: Library backend</li> <li>Library Usage: Programmatic usage</li> </ul>"},{"location":"api/cli/commands/#see-also","title":"See Also","text":"<ul> <li>Architecture</li> <li>Contributing</li> </ul>"},{"location":"api/models/conversation/","title":"Conversation Model","text":"<p>Represents a complete AI conversation with messages and metadata.</p>"},{"location":"api/models/conversation/#overview","title":"Overview","text":"<p>The <code>Conversation</code> model is the primary data structure for representing AI conversations. It includes metadata (title, timestamps) and a collection of messages organized in a tree structure.</p>"},{"location":"api/models/conversation/#api-reference","title":"API Reference","text":""},{"location":"api/models/conversation/#echomine.models.conversation.Conversation","title":"Conversation","text":"<p>               Bases: <code>BaseModel</code></p> <p>Immutable conversation structure with tree navigation (per FR-222, FR-227, FR-278).</p> <p>This model represents a complete AI conversation with metadata and all messages. Messages form a tree structure via parent_id references, enabling branching dialogue paths and multi-turn interactions.</p> Immutability <p>This model is FROZEN - attempting to modify fields will raise ValidationError. Use .model_copy(update={...}) to create modified instances.</p> Tree Navigation <p>Conversations support tree navigation via helper methods: - get_root_messages(): Entry points for conversation traversal - get_message_by_id(): Fast lookup for specific messages - get_children(): Get direct replies to a message - get_thread(): Get message and all ancestors up to root - get_all_threads(): Get all root-to-leaf paths</p> Example <pre><code>from datetime import datetime, UTC\n\nmessages = [\n    Message(id=\"1\", content=\"Hello\", role=\"user\", timestamp=datetime.now(UTC), parent_id=None),\n    Message(id=\"2\", content=\"Hi!\", role=\"assistant\", timestamp=datetime.now(UTC), parent_id=\"1\"),\n    Message(id=\"3\", content=\"Alt response\", role=\"assistant\", timestamp=datetime.now(UTC), parent_id=\"1\"),\n]\n\nconversation = Conversation(\n    id=\"conv-001\",\n    title=\"Greeting\",\n    created_at=datetime.now(UTC),\n    updated_at=datetime.now(UTC),\n    messages=messages\n)\n\n# Tree navigation\nroots = conversation.get_root_messages()  # [msg-1]\nchildren = conversation.get_children(\"1\")  # [msg-2, msg-3]\nthreads = conversation.get_all_threads()  # [[msg-1, msg-2], [msg-1, msg-3]]\n</code></pre> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique conversation identifier (non-empty string)</p> <code>title</code> <code>str</code> <p>Conversation title (non-empty, any UTF-8)</p> <code>created_at</code> <code>datetime</code> <p>Conversation creation timestamp (timezone-aware UTC, REQUIRED)</p> <code>updated_at</code> <code>datetime | None</code> <p>Last modification timestamp (timezone-aware UTC, None if never updated)</p> <code>messages</code> <code>list[Message]</code> <p>All messages in conversation (non-empty list)</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Provider-specific fields (e.g., moderation_results, plugin_ids)</p> Computed Properties <p>updated_at_or_created: Returns updated_at if set, else created_at (never None) message_count: Number of messages in conversation</p>"},{"location":"api/models/conversation/#echomine.models.conversation.Conversation.updated_at_or_created","title":"updated_at_or_created  <code>property</code>","text":"<pre><code>updated_at_or_created: datetime\n</code></pre> <p>Get the last update timestamp, falling back to created_at if not set.</p> <p>This property ensures downstream code always has a valid \"last modified\" timestamp without needing to handle Optional[datetime]. If the conversation has never been updated (updated_at is None), returns created_at.</p> <p>Returns:</p> Type Description <code>datetime</code> <p>Last update timestamp (updated_at if set, else created_at)</p> Example <pre><code># Conversation never updated\nconv = Conversation(..., created_at=ts1, updated_at=None)\nassert conv.updated_at_or_created == ts1\n\n# Conversation updated\nconv2 = Conversation(..., created_at=ts1, updated_at=ts2)\nassert conv2.updated_at_or_created == ts2\n</code></pre> Usage Notes <ul> <li>Prefer this property over direct <code>updated_at</code> access for display/sorting</li> <li>Use direct <code>updated_at</code> field when you need to distinguish null vs. set</li> <li>Guaranteed non-null return value (mypy --strict compliant)</li> </ul>"},{"location":"api/models/conversation/#echomine.models.conversation.Conversation.message_count","title":"message_count  <code>property</code>","text":"<pre><code>message_count: int\n</code></pre> <p>Get the total number of messages in the conversation.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of messages in the conversation</p> Example <pre><code>conversation = Conversation(\n    id=\"conv-001\",\n    title=\"Test\",\n    created_at=datetime.now(UTC),\n    updated_at=datetime.now(UTC),\n    messages=[msg1, msg2, msg3]\n)\nassert conversation.message_count == 3\n</code></pre> Requirements <ul> <li>FR-018: Metadata includes message count</li> </ul>"},{"location":"api/models/conversation/#echomine.models.conversation.Conversation.validate_created_at_timezone_aware","title":"validate_created_at_timezone_aware  <code>classmethod</code>","text":"<pre><code>validate_created_at_timezone_aware(v: datetime) -&gt; datetime\n</code></pre> <p>Ensure created_at is timezone-aware and normalized to UTC.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>datetime</code> <p>Timestamp value to validate</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>Timezone-aware datetime normalized to UTC</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If timestamp is timezone-naive</p> Requirements <ul> <li>FR-244: Timestamps must be timezone-aware</li> <li>FR-245: Timestamps normalized to UTC</li> <li>FR-246: Validation enforced at parse time</li> </ul> Source code in <code>src/echomine/models/conversation.py</code> <pre><code>@field_validator(\"created_at\")\n@classmethod\ndef validate_created_at_timezone_aware(cls, v: datetime) -&gt; datetime:\n    \"\"\"Ensure created_at is timezone-aware and normalized to UTC.\n\n    Args:\n        v: Timestamp value to validate\n\n    Returns:\n        Timezone-aware datetime normalized to UTC\n\n    Raises:\n        ValueError: If timestamp is timezone-naive\n\n    Requirements:\n        - FR-244: Timestamps must be timezone-aware\n        - FR-245: Timestamps normalized to UTC\n        - FR-246: Validation enforced at parse time\n    \"\"\"\n    if v.tzinfo is None or v.tzinfo.utcoffset(v) is None:\n        msg = f\"created_at must be timezone-aware: {v}\"\n        raise ValueError(msg)\n    return v.astimezone(UTC)  # Normalize to UTC\n</code></pre>"},{"location":"api/models/conversation/#echomine.models.conversation.Conversation.validate_updated_at_timezone_aware","title":"validate_updated_at_timezone_aware  <code>classmethod</code>","text":"<pre><code>validate_updated_at_timezone_aware(\n    v: datetime | None, info: Any\n) -&gt; datetime | None\n</code></pre> <p>Ensure updated_at is timezone-aware and &gt;= created_at (if provided).</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>datetime | None</code> <p>updated_at value to validate (may be None)</p> required <code>info</code> <code>Any</code> <p>Field validation info containing created_at</p> required <p>Returns:</p> Type Description <code>datetime | None</code> <p>Timezone-aware datetime normalized to UTC, or None if not provided</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If timestamp is timezone-naive or &lt; created_at</p> Requirements <ul> <li>FR-244: Timestamps must be timezone-aware (when provided)</li> <li>FR-245: Timestamps normalized to UTC</li> <li>FR-246: Validation enforced at parse time</li> <li>FR-273: updated_at must be &gt;= created_at (when provided)</li> </ul> Source code in <code>src/echomine/models/conversation.py</code> <pre><code>@field_validator(\"updated_at\")\n@classmethod\ndef validate_updated_at_timezone_aware(cls, v: datetime | None, info: Any) -&gt; datetime | None:\n    \"\"\"Ensure updated_at is timezone-aware and &gt;= created_at (if provided).\n\n    Args:\n        v: updated_at value to validate (may be None)\n        info: Field validation info containing created_at\n\n    Returns:\n        Timezone-aware datetime normalized to UTC, or None if not provided\n\n    Raises:\n        ValueError: If timestamp is timezone-naive or &lt; created_at\n\n    Requirements:\n        - FR-244: Timestamps must be timezone-aware (when provided)\n        - FR-245: Timestamps normalized to UTC\n        - FR-246: Validation enforced at parse time\n        - FR-273: updated_at must be &gt;= created_at (when provided)\n    \"\"\"\n    # Handle None (optional field)\n    if v is None:\n        return None\n\n    # Validate timezone-aware\n    if v.tzinfo is None or v.tzinfo.utcoffset(v) is None:\n        msg = f\"updated_at must be timezone-aware: {v}\"\n        raise ValueError(msg)\n\n    # Normalize to UTC\n    v_utc = v.astimezone(UTC)\n\n    # Validate updated_at &gt;= created_at\n    created_at = info.data.get(\"created_at\")\n    if created_at and v_utc &lt; created_at:\n        msg = f\"updated_at ({v_utc}) must be &gt;= created_at ({created_at})\"\n        raise ValueError(msg)\n\n    return v_utc\n</code></pre>"},{"location":"api/models/conversation/#echomine.models.conversation.Conversation.get_message_by_id","title":"get_message_by_id","text":"<pre><code>get_message_by_id(message_id: str) -&gt; Message | None\n</code></pre> <p>Find message by ID.</p> <p>Parameters:</p> Name Type Description Default <code>message_id</code> <code>str</code> <p>Message identifier to find</p> required <p>Returns:</p> Type Description <code>Message | None</code> <p>Message if found, None otherwise</p> Example <pre><code>msg = conversation.get_message_by_id(\"msg-001\")\nif msg:\n    print(msg.content)\n</code></pre> Requirements <ul> <li>FR-278: Support message lookup by ID</li> </ul> Source code in <code>src/echomine/models/conversation.py</code> <pre><code>def get_message_by_id(self, message_id: str) -&gt; Message | None:\n    \"\"\"Find message by ID.\n\n    Args:\n        message_id: Message identifier to find\n\n    Returns:\n        Message if found, None otherwise\n\n    Example:\n        ```python\n        msg = conversation.get_message_by_id(\"msg-001\")\n        if msg:\n            print(msg.content)\n        ```\n\n    Requirements:\n        - FR-278: Support message lookup by ID\n    \"\"\"\n    return next((m for m in self.messages if m.id == message_id), None)\n</code></pre>"},{"location":"api/models/conversation/#echomine.models.conversation.Conversation.get_root_messages","title":"get_root_messages","text":"<pre><code>get_root_messages() -&gt; list[Message]\n</code></pre> <p>Get all root messages (parent_id is None).</p> <p>Root messages are entry points for conversation tree traversal.</p> <p>Returns:</p> Type Description <code>list[Message]</code> <p>List of root messages (may be empty if malformed data)</p> Example <pre><code>roots = conversation.get_root_messages()\nfor root in roots:\n    print(f\"Thread starting with: {root.content}\")\n</code></pre> Requirements <ul> <li>FR-278: Support identifying conversation entry points</li> </ul> Source code in <code>src/echomine/models/conversation.py</code> <pre><code>def get_root_messages(self) -&gt; list[Message]:\n    \"\"\"Get all root messages (parent_id is None).\n\n    Root messages are entry points for conversation tree traversal.\n\n    Returns:\n        List of root messages (may be empty if malformed data)\n\n    Example:\n        ```python\n        roots = conversation.get_root_messages()\n        for root in roots:\n            print(f\"Thread starting with: {root.content}\")\n        ```\n\n    Requirements:\n        - FR-278: Support identifying conversation entry points\n    \"\"\"\n    return [m for m in self.messages if m.parent_id is None]\n</code></pre>"},{"location":"api/models/conversation/#echomine.models.conversation.Conversation.get_children","title":"get_children","text":"<pre><code>get_children(message_id: str) -&gt; list[Message]\n</code></pre> <p>Get all direct children of a message.</p> <p>Parameters:</p> Name Type Description Default <code>message_id</code> <code>str</code> <p>Parent message identifier</p> required <p>Returns:</p> Type Description <code>list[Message]</code> <p>List of messages with parent_id == message_id (may be empty)</p> Example <pre><code>children = conversation.get_children(\"msg-001\")\nif children:\n    print(f\"Found {len(children)} replies\")\n</code></pre> Requirements <ul> <li>FR-278: Support tree navigation via parent-child relationships</li> </ul> Source code in <code>src/echomine/models/conversation.py</code> <pre><code>def get_children(self, message_id: str) -&gt; list[Message]:\n    \"\"\"Get all direct children of a message.\n\n    Args:\n        message_id: Parent message identifier\n\n    Returns:\n        List of messages with parent_id == message_id (may be empty)\n\n    Example:\n        ```python\n        children = conversation.get_children(\"msg-001\")\n        if children:\n            print(f\"Found {len(children)} replies\")\n        ```\n\n    Requirements:\n        - FR-278: Support tree navigation via parent-child relationships\n    \"\"\"\n    return [m for m in self.messages if m.parent_id == message_id]\n</code></pre>"},{"location":"api/models/conversation/#echomine.models.conversation.Conversation.get_thread","title":"get_thread","text":"<pre><code>get_thread(message_id: str) -&gt; list[Message]\n</code></pre> <p>Get message and all ancestors up to root.</p> <p>Returns messages in chronological order (oldest first), representing the conversation path from root to the specified message.</p> <p>Parameters:</p> Name Type Description Default <code>message_id</code> <code>str</code> <p>Target message identifier</p> required <p>Returns:</p> Type Description <code>list[Message]</code> <p>List of messages from root to target (empty if message_id not found)</p> Example <pre><code>thread = conversation.get_thread(\"msg-005\")\nfor msg in thread:\n    print(f\"{msg.role}: {msg.content}\")\n</code></pre> Requirements <ul> <li>FR-278: Support retrieving conversation context for a message</li> </ul> Source code in <code>src/echomine/models/conversation.py</code> <pre><code>def get_thread(self, message_id: str) -&gt; list[Message]:\n    \"\"\"Get message and all ancestors up to root.\n\n    Returns messages in chronological order (oldest first), representing\n    the conversation path from root to the specified message.\n\n    Args:\n        message_id: Target message identifier\n\n    Returns:\n        List of messages from root to target (empty if message_id not found)\n\n    Example:\n        ```python\n        thread = conversation.get_thread(\"msg-005\")\n        for msg in thread:\n            print(f\"{msg.role}: {msg.content}\")\n        ```\n\n    Requirements:\n        - FR-278: Support retrieving conversation context for a message\n    \"\"\"\n    thread: list[Message] = []\n    current = self.get_message_by_id(message_id)\n\n    while current:\n        thread.insert(0, current)  # Prepend (oldest first)\n        current = self.get_message_by_id(current.parent_id) if current.parent_id else None\n\n    return thread\n</code></pre>"},{"location":"api/models/conversation/#echomine.models.conversation.Conversation.get_all_threads","title":"get_all_threads","text":"<pre><code>get_all_threads() -&gt; list[list[Message]]\n</code></pre> <p>Get all conversation threads (root-to-leaf paths).</p> <p>Returns list of threads, where each thread is a list of messages from root to leaf in chronological order. Useful for understanding all conversation branches.</p> <p>Returns:</p> Type Description <code>list[list[Message]]</code> <p>List of message threads (each thread is a list of messages)</p> Example <pre><code># Conversation structure:\n#   msg-1 (user: \"Hello\")\n#   \u251c\u2500\u2500 msg-2 (assistant: \"Hi! How can I help?\")\n#   \u2514\u2500\u2500 msg-3 (assistant: \"Alternative response\")\n#\n# Result: [[msg-1, msg-2], [msg-1, msg-3]]\n\nthreads = conversation.get_all_threads()\nfor i, thread in enumerate(threads):\n    print(f\"Thread {i+1}: {len(thread)} messages\")\n</code></pre> Requirements <ul> <li>FR-278: Support comprehensive tree traversal</li> <li>FR-280: Enable analysis of all conversation branches</li> </ul> Source code in <code>src/echomine/models/conversation.py</code> <pre><code>def get_all_threads(self) -&gt; list[list[Message]]:\n    \"\"\"Get all conversation threads (root-to-leaf paths).\n\n    Returns list of threads, where each thread is a list of messages\n    from root to leaf in chronological order. Useful for understanding\n    all conversation branches.\n\n    Returns:\n        List of message threads (each thread is a list of messages)\n\n    Example:\n        ```python\n        # Conversation structure:\n        #   msg-1 (user: \"Hello\")\n        #   \u251c\u2500\u2500 msg-2 (assistant: \"Hi! How can I help?\")\n        #   \u2514\u2500\u2500 msg-3 (assistant: \"Alternative response\")\n        #\n        # Result: [[msg-1, msg-2], [msg-1, msg-3]]\n\n        threads = conversation.get_all_threads()\n        for i, thread in enumerate(threads):\n            print(f\"Thread {i+1}: {len(thread)} messages\")\n        ```\n\n    Requirements:\n        - FR-278: Support comprehensive tree traversal\n        - FR-280: Enable analysis of all conversation branches\n    \"\"\"\n    threads: list[list[Message]] = []\n\n    def build_threads(msg: Message, path: list[Message]) -&gt; None:\n        \"\"\"Recursive helper to build all threads from a message.\n\n        Args:\n            msg: Current message in traversal\n            path: Accumulated messages from root to current\n        \"\"\"\n        path = [*path, msg]  # Extend path with current message\n        children = self.get_children(msg.id)\n\n        if not children:\n            # Leaf node: complete thread\n            threads.append(path)\n        else:\n            # Branch node: recurse into children\n            for child in children:\n                build_threads(child, path)\n\n    # Start traversal from each root\n    for root in self.get_root_messages():\n        build_threads(root, [])\n\n    return threads\n</code></pre>"},{"location":"api/models/conversation/#echomine.models.conversation.Conversation.flatten_messages","title":"flatten_messages","text":"<pre><code>flatten_messages() -&gt; str\n</code></pre> <p>Flatten all message content to single string for search.</p> <p>Concatenates all message content with space separation, useful for full-text search operations.</p> <p>Returns:</p> Type Description <code>str</code> <p>Single string containing all message content</p> Example <pre><code>text = conversation.flatten_messages()\nif \"algorithm\" in text.lower():\n    print(\"Conversation mentions algorithms\")\n</code></pre> Requirements <ul> <li>FR-322: Enable full-text search across conversation content</li> </ul> Source code in <code>src/echomine/models/conversation.py</code> <pre><code>def flatten_messages(self) -&gt; str:\n    \"\"\"Flatten all message content to single string for search.\n\n    Concatenates all message content with space separation, useful for\n    full-text search operations.\n\n    Returns:\n        Single string containing all message content\n\n    Example:\n        ```python\n        text = conversation.flatten_messages()\n        if \"algorithm\" in text.lower():\n            print(\"Conversation mentions algorithms\")\n        ```\n\n    Requirements:\n        - FR-322: Enable full-text search across conversation content\n    \"\"\"\n    return \" \".join(msg.content for msg in self.messages)\n</code></pre>"},{"location":"api/models/conversation/#usage-examples","title":"Usage Examples","text":""},{"location":"api/models/conversation/#basic-access","title":"Basic Access","text":"<pre><code>from echomine import OpenAIAdapter\nfrom pathlib import Path\n\nadapter = OpenAIAdapter()\nconversation = adapter.get_conversation_by_id(Path(\"export.json\"), \"conv-abc123\")\n\n# Access metadata\nprint(f\"Title: {conversation.title}\")\nprint(f\"Created: {conversation.created_at}\")\nprint(f\"Messages: {len(conversation.messages)}\")\n\n# Access messages\nfor message in conversation.messages:\n    print(f\"{message.role}: {message.content[:50]}...\")\n</code></pre>"},{"location":"api/models/conversation/#message-tree-navigation","title":"Message Tree Navigation","text":"<p>Conversations can have branching message trees (e.g., regenerated AI responses):</p> <pre><code># Get all threads (root-to-leaf paths)\nthreads = conversation.get_all_threads()\nprint(f\"Conversation has {len(threads)} branches\")\n\nfor i, thread in enumerate(threads, 1):\n    print(f\"\\nThread {i} ({len(thread)} messages):\")\n    for msg in thread:\n        print(f\"  {msg.role}: {msg.content[:50]}...\")\n\n# Get specific thread by leaf message ID\nthread = conversation.get_thread(\"msg-xyz-789\")\n\n# Get root messages (conversation starters)\nroots = conversation.get_root_messages()\n\n# Get children of a specific message\nchildren = conversation.get_children(\"msg-abc-123\")\n\n# Check if message has children (branches)\nhas_branches = conversation.has_children(\"msg-abc-123\")\n</code></pre>"},{"location":"api/models/conversation/#immutability","title":"Immutability","text":"<p>Models are frozen (immutable) to prevent accidental data corruption:</p> <pre><code>from pydantic import ValidationError\n\n# \u274c This raises ValidationError (frozen model)\ntry:\n    conversation.title = \"New Title\"\nexcept ValidationError:\n    print(\"Error: Cannot modify frozen model\")\n\n# \u2705 Create modified copy instead\nupdated = conversation.model_copy(update={\"title\": \"New Title\"})\nprint(f\"Original: {conversation.title}\")\nprint(f\"Updated: {updated.title}\")\n</code></pre>"},{"location":"api/models/conversation/#validation","title":"Validation","text":"<p>All fields are strictly validated:</p> <pre><code>from echomine.models import Conversation\nfrom datetime import datetime, timezone\nfrom pydantic import ValidationError\n\n# \u274c Invalid: missing required fields\ntry:\n    invalid = Conversation(\n        id=\"conv-123\",\n        title=\"Test\"\n        # Missing: created_at, messages\n    )\nexcept ValidationError as e:\n    print(f\"Validation error: {e}\")\n\n# \u2705 Valid: all required fields provided\nvalid = Conversation(\n    id=\"conv-123\",\n    title=\"Test Conversation\",\n    created_at=datetime.now(timezone.utc),\n    messages=[],\n    metadata={}\n)\n</code></pre>"},{"location":"api/models/conversation/#model-fields","title":"Model Fields","text":""},{"location":"api/models/conversation/#required-fields","title":"Required Fields","text":"<ul> <li>id (<code>str</code>): Unique conversation identifier</li> <li>title (<code>str</code>): Conversation title (1-2000 characters)</li> <li>created_at (<code>datetime</code>): Creation timestamp (UTC, timezone-aware)</li> <li>messages (<code>list[Message]</code>): List of conversation messages</li> </ul>"},{"location":"api/models/conversation/#optional-fields","title":"Optional Fields","text":"<ul> <li>updated_at (<code>datetime | None</code>): Last update timestamp (UTC, timezone-aware), <code>None</code> if never updated</li> <li>metadata (<code>dict[str, Any]</code>): Provider-specific metadata (default: empty dict)</li> </ul>"},{"location":"api/models/conversation/#computed-properties","title":"Computed Properties","text":"<ul> <li>message_count (<code>int</code>): Number of messages in conversation</li> </ul>"},{"location":"api/models/conversation/#message-tree-structure","title":"Message Tree Structure","text":"<p>Conversations are organized as trees, not linear sequences:</p> <ul> <li>Each message has an optional <code>parent_id</code> pointing to its predecessor</li> <li>Messages with <code>parent_id=None</code> are root messages (conversation starters)</li> <li>Messages can have multiple children (branches from regenerated responses)</li> </ul> <p>Example Tree:</p> <pre><code>Root (parent_id=None)\n\u251c\u2500\u2500 Child 1 (parent_id=Root)\n\u2502   \u2514\u2500\u2500 Child 1.1 (parent_id=Child 1)\n\u2514\u2500\u2500 Child 2 (parent_id=Root)  # Branch from regeneration\n    \u2514\u2500\u2500 Child 2.1 (parent_id=Child 2)\n</code></pre>"},{"location":"api/models/conversation/#thread-extraction","title":"Thread Extraction","text":"<p>Thread: A root-to-leaf path through the message tree.</p> <pre><code># Get all threads (all possible conversation paths)\nthreads = conversation.get_all_threads()\n\n# Each thread is a list of messages in chronological order\nfor thread in threads:\n    for msg in thread:\n        print(f\"{msg.role}: {msg.content}\")\n</code></pre>"},{"location":"api/models/conversation/#metadata","title":"Metadata","text":"<p>Provider-specific data is stored in the <code>metadata</code> dictionary:</p> <pre><code># OpenAI-specific metadata\nconversation.metadata.get(\"openai_model\", \"unknown\")\nconversation.metadata.get(\"openai_conversation_template_id\")\nconversation.metadata.get(\"openai_plugin_ids\", [])\n\n# Check if metadata exists\nif \"openai_model\" in conversation.metadata:\n    print(f\"Model: {conversation.metadata['openai_model']}\")\n</code></pre>"},{"location":"api/models/conversation/#related-models","title":"Related Models","text":"<ul> <li>Message: Individual message model</li> <li>SearchResult: Search result containing a conversation</li> </ul>"},{"location":"api/models/conversation/#see-also","title":"See Also","text":"<ul> <li>Library Usage Guide</li> <li>OpenAI Adapter</li> </ul>"},{"location":"api/models/message/","title":"Message Model","text":"<p>Represents an individual message within a conversation.</p>"},{"location":"api/models/message/#overview","title":"Overview","text":"<p>The <code>Message</code> model represents a single message in a conversation, including the role (user/assistant/system), content, timestamp, and parent relationship for tree navigation.</p>"},{"location":"api/models/message/#api-reference","title":"API Reference","text":""},{"location":"api/models/message/#echomine.models.message.Message","title":"Message","text":"<p>               Bases: <code>BaseModel</code></p> <p>Immutable message structure from conversation export (per FR-223, FR-227).</p> <p>This model represents a single message in an AI conversation, supporting message threading via parent_id references. Messages form a tree structure within conversations, enabling branching dialogue paths.</p> Immutability <p>This model is FROZEN - attempting to modify fields will raise ValidationError. Use .model_copy(update={...}) to create modified instances.</p> Example <pre><code>from datetime import datetime, UTC\n\nmessage = Message(\n    id=\"msg-001\",\n    content=\"Hello, world!\",\n    role=\"user\",\n    timestamp=datetime.now(UTC),\n    parent_id=None  # Root message\n)\n\n# Create a reply\nreply = Message(\n    id=\"msg-002\",\n    content=\"Hi! How can I help?\",\n    role=\"assistant\",\n    timestamp=datetime.now(UTC),\n    parent_id=\"msg-001\"  # References parent\n)\n</code></pre> Role Normalization <p>The <code>role</code> field is normalized to one of three standard values for multi-provider consistency. Provider-specific roles are mapped as follows:</p> <p>OpenAI role mappings:     - \"user\" \u2192 \"user\" (human input)     - \"assistant\" \u2192 \"assistant\" (AI response)     - \"system\" \u2192 \"system\" (system messages)     - \"tool\" \u2192 \"assistant\" (tool execution is assistant action)     - unknown roles \u2192 \"assistant\" (safe fallback)</p> <p>The original provider-specific role is preserved in metadata[\"original_role\"] for debugging and provider-specific workflows.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique message identifier within conversation (non-empty string)</p> <code>content</code> <code>str</code> <p>Message text content (may be empty for deleted messages)</p> <code>role</code> <code>Literal['user', 'assistant', 'system']</code> <p>Normalized message role (user, assistant, or system)</p> <code>timestamp</code> <code>datetime</code> <p>Message creation time (timezone-aware UTC)</p> <code>parent_id</code> <code>str | None</code> <p>Parent message ID for threading (None for root messages)</p> <code>images</code> <code>list[ImageRef]</code> <p>Image attachments extracted from multimodal content (empty for text-only)</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Provider-specific fields (e.g., original_role, token_count)</p>"},{"location":"api/models/message/#echomine.models.message.Message.validate_timezone_aware","title":"validate_timezone_aware  <code>classmethod</code>","text":"<pre><code>validate_timezone_aware(v: datetime) -&gt; datetime\n</code></pre> <p>Ensure timestamp is timezone-aware and normalized to UTC.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>datetime</code> <p>Timestamp value to validate</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>Timezone-aware datetime normalized to UTC</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If timestamp is timezone-naive</p> Requirements <ul> <li>FR-244: Timestamps must be timezone-aware</li> <li>FR-245: Timestamps normalized to UTC</li> <li>FR-246: Validation enforced at parse time</li> </ul> Source code in <code>src/echomine/models/message.py</code> <pre><code>@field_validator(\"timestamp\")\n@classmethod\ndef validate_timezone_aware(cls, v: datetime) -&gt; datetime:\n    \"\"\"Ensure timestamp is timezone-aware and normalized to UTC.\n\n    Args:\n        v: Timestamp value to validate\n\n    Returns:\n        Timezone-aware datetime normalized to UTC\n\n    Raises:\n        ValueError: If timestamp is timezone-naive\n\n    Requirements:\n        - FR-244: Timestamps must be timezone-aware\n        - FR-245: Timestamps normalized to UTC\n        - FR-246: Validation enforced at parse time\n    \"\"\"\n    if v.tzinfo is None or v.tzinfo.utcoffset(v) is None:\n        msg = f\"Timestamp must be timezone-aware: {v}\"\n        raise ValueError(msg)\n    return v.astimezone(UTC)  # Normalize to UTC\n</code></pre>"},{"location":"api/models/message/#echomine.models.message.Message.is_root","title":"is_root","text":"<pre><code>is_root() -&gt; bool\n</code></pre> <p>Check if message is conversation root (no parent).</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if parent_id is None, False otherwise</p> Example <pre><code>root_msg = Message(id=\"1\", content=\"Hello\", role=\"user\", timestamp=now, parent_id=None)\nreply_msg = Message(id=\"2\", content=\"Hi\", role=\"assistant\", timestamp=now, parent_id=\"1\")\n\nassert root_msg.is_root() is True\nassert reply_msg.is_root() is False\n</code></pre> Source code in <code>src/echomine/models/message.py</code> <pre><code>def is_root(self) -&gt; bool:\n    \"\"\"Check if message is conversation root (no parent).\n\n    Returns:\n        True if parent_id is None, False otherwise\n\n    Example:\n        ```python\n        root_msg = Message(id=\"1\", content=\"Hello\", role=\"user\", timestamp=now, parent_id=None)\n        reply_msg = Message(id=\"2\", content=\"Hi\", role=\"assistant\", timestamp=now, parent_id=\"1\")\n\n        assert root_msg.is_root() is True\n        assert reply_msg.is_root() is False\n        ```\n    \"\"\"\n    return self.parent_id is None\n</code></pre>"},{"location":"api/models/message/#usage-examples","title":"Usage Examples","text":""},{"location":"api/models/message/#basic-access","title":"Basic Access","text":"<pre><code>from echomine import OpenAIAdapter\nfrom pathlib import Path\n\nadapter = OpenAIAdapter()\nconversation = adapter.get_conversation_by_id(Path(\"export.json\"), \"conv-abc123\")\n\n# Access messages\nfor message in conversation.messages:\n    print(f\"[{message.timestamp}] {message.role}: {message.content[:50]}...\")\n</code></pre>"},{"location":"api/models/message/#role-types","title":"Role Types","text":"<p>Messages have normalized roles:</p> <pre><code>from typing import Literal\n\n# Message.role is Literal[\"user\", \"assistant\", \"system\"]\nfor message in conversation.messages:\n    if message.role == \"user\":\n        print(f\"User: {message.content}\")\n    elif message.role == \"assistant\":\n        print(f\"AI: {message.content}\")\n    elif message.role == \"system\":\n        print(f\"System: {message.content}\")\n    # No other values possible - type safety guaranteed!\n</code></pre>"},{"location":"api/models/message/#timestamp-handling","title":"Timestamp Handling","text":"<p>All timestamps are timezone-aware UTC datetimes:</p> <pre><code>from datetime import timezone\n\nfor message in conversation.messages:\n    # Guaranteed to be UTC and timezone-aware\n    assert message.timestamp.tzinfo == timezone.utc\n\n    # Safe to compare and serialize\n    print(f\"{message.timestamp.isoformat()}: {message.content}\")\n\n# Convert to local timezone for display\nimport datetime\nlocal_tz = datetime.datetime.now().astimezone().tzinfo\nfor msg in conversation.messages:\n    local_time = msg.timestamp.astimezone(local_tz)\n    print(f\"[{local_time}] {msg.role}: {msg.content[:30]}...\")\n</code></pre>"},{"location":"api/models/message/#parent-child-relationships","title":"Parent-Child Relationships","text":"<p>Messages are organized in a tree structure:</p> <pre><code># Check if message is root (conversation starter)\nis_root = message.parent_id is None\n\n# Find message's parent\nparent_id = message.parent_id\nif parent_id:\n    parent = next((m for m in conversation.messages if m.id == parent_id), None)\n    if parent:\n        print(f\"Parent: {parent.content[:50]}...\")\n\n# Find message's children\nchildren = [m for m in conversation.messages if m.parent_id == message.id]\nprint(f\"Message has {len(children)} children\")\n</code></pre>"},{"location":"api/models/message/#immutability","title":"Immutability","text":"<p>Messages are frozen (immutable):</p> <pre><code>from pydantic import ValidationError\n\n# \u274c This raises ValidationError\ntry:\n    message.content = \"New content\"\nexcept ValidationError:\n    print(\"Error: Cannot modify frozen model\")\n\n# \u2705 Create modified copy instead\nupdated = message.model_copy(update={\"content\": \"New content\"})\n</code></pre>"},{"location":"api/models/message/#validation","title":"Validation","text":"<p>All fields are strictly validated:</p> <pre><code>from echomine.models import Message\nfrom datetime import datetime, timezone\nfrom pydantic import ValidationError\n\n# \u274c Invalid: naive timestamp (no timezone)\ntry:\n    invalid = Message(\n        id=\"msg-123\",\n        content=\"Hello\",\n        role=\"user\",\n        timestamp=datetime.now(),  # \u274c Not timezone-aware!\n        parent_id=None\n    )\nexcept ValidationError as e:\n    print(f\"Error: {e}\")\n\n# \u2705 Valid: timezone-aware UTC timestamp\nvalid = Message(\n    id=\"msg-123\",\n    content=\"Hello\",\n    role=\"user\",\n    timestamp=datetime.now(timezone.utc),\n    parent_id=None\n)\n</code></pre>"},{"location":"api/models/message/#model-fields","title":"Model Fields","text":""},{"location":"api/models/message/#required-fields","title":"Required Fields","text":"<ul> <li>id (<code>str</code>): Unique message identifier (non-empty)</li> <li>content (<code>str</code>): Message text content</li> <li>role (<code>Literal[\"user\", \"assistant\", \"system\"]</code>): Message role (normalized)</li> <li>timestamp (<code>datetime</code>): Message timestamp (UTC, timezone-aware)</li> </ul>"},{"location":"api/models/message/#optional-fields","title":"Optional Fields","text":"<ul> <li>parent_id (<code>str | None</code>): ID of parent message, <code>None</code> for root messages</li> <li>metadata (<code>dict[str, Any]</code>): Message-specific metadata (default: empty dict)</li> </ul>"},{"location":"api/models/message/#role-normalization","title":"Role Normalization","text":"<p>All provider-specific roles are normalized to three standard values:</p> Provider Source Role Normalized Role OpenAI \"user\" \"user\" OpenAI \"assistant\" \"assistant\" OpenAI \"system\" \"system\" Anthropic (future) \"human\" \"user\" Anthropic (future) \"assistant\" \"assistant\" Google (future) \"user\" \"user\" Google (future) \"model\" \"assistant\""},{"location":"api/models/message/#timestamp-format","title":"Timestamp Format","text":"<p>All timestamps follow these rules:</p> <ol> <li>Timezone-aware: Must have <code>tzinfo</code> set</li> <li>UTC: Normalized to UTC timezone</li> <li>ISO 8601: Serialized as ISO 8601 strings</li> </ol> <p>Example:</p> <pre><code>from datetime import datetime, timezone\n\n# Create message with current UTC time\nmessage = Message(\n    id=\"msg-123\",\n    content=\"Hello\",\n    role=\"user\",\n    timestamp=datetime.now(timezone.utc),\n    parent_id=None\n)\n\n# Serialize to ISO 8601\niso_timestamp = message.timestamp.isoformat()\n# Output: \"2024-01-15T10:30:00+00:00\"\n</code></pre>"},{"location":"api/models/message/#metadata","title":"Metadata","text":"<p>Message-specific metadata (e.g., image attachments, code blocks):</p> <pre><code># Access metadata\nimage_urls = message.metadata.get(\"image_urls\", [])\ncode_blocks = message.metadata.get(\"code_blocks\", [])\n\n# Check if metadata exists\nif \"image_urls\" in message.metadata:\n    print(f\"Message has {len(message.metadata['image_urls'])} images\")\n</code></pre>"},{"location":"api/models/message/#related-models","title":"Related Models","text":"<ul> <li>Conversation: Container for messages</li> <li>Image: Image attachment model (if present in metadata)</li> </ul>"},{"location":"api/models/message/#see-also","title":"See Also","text":"<ul> <li>Library Usage Guide</li> <li>Data Validation</li> </ul>"},{"location":"api/models/search/","title":"Search Models","text":"<p>Models for search queries and results.</p>"},{"location":"api/models/search/#overview","title":"Overview","text":"<p>Search models provide type-safe interfaces for searching conversations with BM25 ranking, date filtering, and title matching.</p>"},{"location":"api/models/search/#searchquery","title":"SearchQuery","text":""},{"location":"api/models/search/#echomine.models.search.SearchQuery","title":"SearchQuery","text":"<p>               Bases: <code>BaseModel</code></p> <p>Search query parameters with filters.</p> <p>Encapsulates all search parameters including keywords, title filtering, date range filtering, and result limits. All filters are optional but at least one should be provided for meaningful results.</p> Immutability <p>This model is FROZEN - attempting to modify fields will raise ValidationError. Use .model_copy(update={...}) to create modified instances.</p> Example <pre><code>from datetime import date\n\n# Keyword search\nquery = SearchQuery(keywords=[\"algorithm\", \"design\"], limit=10)\n\n# Title filter only (fast, metadata-only)\nquery = SearchQuery(title_filter=\"Project\")\n\n# Combined filters\nquery = SearchQuery(\n    keywords=[\"refactor\"],\n    title_filter=\"Project\",\n    from_date=date(2024, 1, 1),\n    to_date=date(2024, 3, 31),\n    limit=20\n)\n\n# Check filter types\nif query.has_keyword_search():\n    print(\"Performing full-text search\")\n</code></pre> <p>Attributes:</p> Name Type Description <code>keywords</code> <code>list[str] | None</code> <p>Keywords for full-text search (OR logic, case-insensitive)</p> <code>title_filter</code> <code>str | None</code> <p>Partial match on conversation title (metadata-only, fast)</p> <code>from_date</code> <code>date | None</code> <p>Start date for date range filter (inclusive)</p> <code>to_date</code> <code>date | None</code> <p>End date for date range filter (inclusive)</p> <code>limit</code> <code>int</code> <p>Maximum results to return (1-1000, default: 10)</p>"},{"location":"api/models/search/#echomine.models.search.SearchQuery.validate_message_count_bounds","title":"validate_message_count_bounds","text":"<pre><code>validate_message_count_bounds() -&gt; SearchQuery\n</code></pre> <p>Validate min_messages &lt;= max_messages when both are set (FR-005).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If min_messages &gt; max_messages</p> Example <pre><code># Valid: min &lt;= max\nquery = SearchQuery(min_messages=5, max_messages=20)\n\n# Invalid: min &gt; max\ntry:\n    query = SearchQuery(min_messages=20, max_messages=5)\nexcept ValidationError as e:\n    print(e)  # \"min_messages (20) must be &lt;= max_messages (5)\"\n</code></pre> Source code in <code>src/echomine/models/search.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_message_count_bounds(self) -&gt; SearchQuery:\n    \"\"\"Validate min_messages &lt;= max_messages when both are set (FR-005).\n\n    Raises:\n        ValueError: If min_messages &gt; max_messages\n\n    Example:\n        ```python\n        # Valid: min &lt;= max\n        query = SearchQuery(min_messages=5, max_messages=20)\n\n        # Invalid: min &gt; max\n        try:\n            query = SearchQuery(min_messages=20, max_messages=5)\n        except ValidationError as e:\n            print(e)  # \"min_messages (20) must be &lt;= max_messages (5)\"\n        ```\n    \"\"\"\n    if self.min_messages is not None and self.max_messages is not None:\n        if self.min_messages &gt; self.max_messages:\n            raise ValueError(\n                f\"min_messages ({self.min_messages}) must be &lt;= max_messages ({self.max_messages})\"\n            )\n    return self\n</code></pre>"},{"location":"api/models/search/#echomine.models.search.SearchQuery.has_keyword_search","title":"has_keyword_search","text":"<pre><code>has_keyword_search() -&gt; bool\n</code></pre> <p>Check if keyword search is requested.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if keywords provided and non-empty, False otherwise</p> Example <pre><code>query = SearchQuery(keywords=[\"algorithm\"])\nassert query.has_keyword_search() is True\n</code></pre> Source code in <code>src/echomine/models/search.py</code> <pre><code>def has_keyword_search(self) -&gt; bool:\n    \"\"\"Check if keyword search is requested.\n\n    Returns:\n        True if keywords provided and non-empty, False otherwise\n\n    Example:\n        ```python\n        query = SearchQuery(keywords=[\"algorithm\"])\n        assert query.has_keyword_search() is True\n        ```\n    \"\"\"\n    return self.keywords is not None and len(self.keywords) &gt; 0\n</code></pre>"},{"location":"api/models/search/#echomine.models.search.SearchQuery.has_title_filter","title":"has_title_filter","text":"<pre><code>has_title_filter() -&gt; bool\n</code></pre> <p>Check if title filtering is requested.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if title_filter provided and non-empty, False otherwise</p> Example <pre><code>query = SearchQuery(title_filter=\"Project\")\nassert query.has_title_filter() is True\n</code></pre> Source code in <code>src/echomine/models/search.py</code> <pre><code>def has_title_filter(self) -&gt; bool:\n    \"\"\"Check if title filtering is requested.\n\n    Returns:\n        True if title_filter provided and non-empty, False otherwise\n\n    Example:\n        ```python\n        query = SearchQuery(title_filter=\"Project\")\n        assert query.has_title_filter() is True\n        ```\n    \"\"\"\n    return self.title_filter is not None and len(self.title_filter.strip()) &gt; 0\n</code></pre>"},{"location":"api/models/search/#echomine.models.search.SearchQuery.has_date_filter","title":"has_date_filter","text":"<pre><code>has_date_filter() -&gt; bool\n</code></pre> <p>Check if date range filtering is requested.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if either from_date or to_date provided, False otherwise</p> Example <pre><code>from datetime import date\n\nquery = SearchQuery(from_date=date(2024, 1, 1))\nassert query.has_date_filter() is True\n</code></pre> Source code in <code>src/echomine/models/search.py</code> <pre><code>def has_date_filter(self) -&gt; bool:\n    \"\"\"Check if date range filtering is requested.\n\n    Returns:\n        True if either from_date or to_date provided, False otherwise\n\n    Example:\n        ```python\n        from datetime import date\n\n        query = SearchQuery(from_date=date(2024, 1, 1))\n        assert query.has_date_filter() is True\n        ```\n    \"\"\"\n    return self.from_date is not None or self.to_date is not None\n</code></pre>"},{"location":"api/models/search/#echomine.models.search.SearchQuery.has_phrase_search","title":"has_phrase_search","text":"<pre><code>has_phrase_search() -&gt; bool\n</code></pre> <p>Check if phrase search is requested.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if phrases provided and non-empty, False otherwise</p> Example <pre><code>query = SearchQuery(phrases=[\"algo-insights\"])\nassert query.has_phrase_search() is True\n</code></pre> Source code in <code>src/echomine/models/search.py</code> <pre><code>def has_phrase_search(self) -&gt; bool:\n    \"\"\"Check if phrase search is requested.\n\n    Returns:\n        True if phrases provided and non-empty, False otherwise\n\n    Example:\n        ```python\n        query = SearchQuery(phrases=[\"algo-insights\"])\n        assert query.has_phrase_search() is True\n        ```\n    \"\"\"\n    return self.phrases is not None and len(self.phrases) &gt; 0\n</code></pre>"},{"location":"api/models/search/#echomine.models.search.SearchQuery.has_exclude_keywords","title":"has_exclude_keywords","text":"<pre><code>has_exclude_keywords() -&gt; bool\n</code></pre> <p>Check if exclude keywords filtering is requested.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if exclude_keywords provided and non-empty, False otherwise</p> Example <pre><code>query = SearchQuery(keywords=[\"python\"], exclude_keywords=[\"django\"])\nassert query.has_exclude_keywords() is True\n</code></pre> Source code in <code>src/echomine/models/search.py</code> <pre><code>def has_exclude_keywords(self) -&gt; bool:\n    \"\"\"Check if exclude keywords filtering is requested.\n\n    Returns:\n        True if exclude_keywords provided and non-empty, False otherwise\n\n    Example:\n        ```python\n        query = SearchQuery(keywords=[\"python\"], exclude_keywords=[\"django\"])\n        assert query.has_exclude_keywords() is True\n        ```\n    \"\"\"\n    return self.exclude_keywords is not None and len(self.exclude_keywords) &gt; 0\n</code></pre>"},{"location":"api/models/search/#echomine.models.search.SearchQuery.has_message_count_filter","title":"has_message_count_filter","text":"<pre><code>has_message_count_filter() -&gt; bool\n</code></pre> <p>Check if message count filtering is requested (FR-004).</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if either min_messages or max_messages is set, False otherwise</p> Example <pre><code>query = SearchQuery(min_messages=10)\nassert query.has_message_count_filter() is True\n\nquery2 = SearchQuery(max_messages=50)\nassert query2.has_message_count_filter() is True\n\nquery3 = SearchQuery()\nassert query3.has_message_count_filter() is False\n</code></pre> Source code in <code>src/echomine/models/search.py</code> <pre><code>def has_message_count_filter(self) -&gt; bool:\n    \"\"\"Check if message count filtering is requested (FR-004).\n\n    Returns:\n        True if either min_messages or max_messages is set, False otherwise\n\n    Example:\n        ```python\n        query = SearchQuery(min_messages=10)\n        assert query.has_message_count_filter() is True\n\n        query2 = SearchQuery(max_messages=50)\n        assert query2.has_message_count_filter() is True\n\n        query3 = SearchQuery()\n        assert query3.has_message_count_filter() is False\n        ```\n    \"\"\"\n    return self.min_messages is not None or self.max_messages is not None\n</code></pre>"},{"location":"api/models/search/#searchresult","title":"SearchResult","text":""},{"location":"api/models/search/#echomine.models.search.SearchResult","title":"SearchResult","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[ConversationT]</code></p> <p>Generic search result with relevance scoring.</p> <p>Represents a conversation match from a search query with relevance metadata. Results are typically sorted by score (descending) before being returned to the user.</p> Generic Type <p>ConversationT: Provider-specific conversation type (e.g., Conversation for OpenAI)</p> Immutability <p>This model is FROZEN - attempting to modify fields will raise ValidationError. Use .model_copy(update={...}) to create modified instances.</p> Example <pre><code>from echomine.models import Conversation, SearchResult\n\nresult: SearchResult[Conversation] = SearchResult(\n    conversation=conversation,\n    score=0.85,\n    matched_message_ids=[\"msg-001\", \"msg-005\"]\n)\n\nprint(f\"Relevance: {result.score:.2f}\")\nprint(f\"Title: {result.conversation.title}\")\nprint(f\"Matched {len(result.matched_message_ids)} messages\")\n\n# Sort results by relevance\nresults = sorted(results, reverse=True)  # Uses __lt__\n</code></pre> <p>Attributes:</p> Name Type Description <code>conversation</code> <code>ConversationT</code> <p>Matched conversation object (full conversation, not just ID)</p> <code>score</code> <code>float</code> <p>Relevance score (0.0-1.0, higher = better match)</p> <code>matched_message_ids</code> <code>list[str]</code> <p>Message IDs containing keyword matches</p>"},{"location":"api/models/search/#echomine.models.search.SearchResult.__lt__","title":"__lt__","text":"<pre><code>__lt__(other: SearchResult[ConversationT]) -&gt; bool\n</code></pre> <p>Enable sorting by relevance (descending).</p> <p>When using sorted() or .sort(), results will be ordered by relevance score in descending order (highest score first).</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SearchResult[ConversationT]</code> <p>Another SearchResult to compare against</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if self.score &gt; other.score (reversed for descending sort)</p> Example <pre><code>results = [\n    SearchResult(conversation=c1, score=0.5, matched_message_ids=[]),\n    SearchResult(conversation=c2, score=0.9, matched_message_ids=[]),\n    SearchResult(conversation=c3, score=0.7, matched_message_ids=[]),\n]\n\n# Sort descending by relevance\nsorted_results = sorted(results, reverse=True)\n# Order: [0.9, 0.7, 0.5]\n</code></pre> Source code in <code>src/echomine/models/search.py</code> <pre><code>def __lt__(self, other: SearchResult[ConversationT]) -&gt; bool:\n    \"\"\"Enable sorting by relevance (descending).\n\n    When using sorted() or .sort(), results will be ordered by\n    relevance score in descending order (highest score first).\n\n    Args:\n        other: Another SearchResult to compare against\n\n    Returns:\n        True if self.score &gt; other.score (reversed for descending sort)\n\n    Example:\n        ```python\n        results = [\n            SearchResult(conversation=c1, score=0.5, matched_message_ids=[]),\n            SearchResult(conversation=c2, score=0.9, matched_message_ids=[]),\n            SearchResult(conversation=c3, score=0.7, matched_message_ids=[]),\n        ]\n\n        # Sort descending by relevance\n        sorted_results = sorted(results, reverse=True)\n        # Order: [0.9, 0.7, 0.5]\n        ```\n    \"\"\"\n    return self.score &gt; other.score  # Reverse for descending sort\n</code></pre>"},{"location":"api/models/search/#usage-examples","title":"Usage Examples","text":""},{"location":"api/models/search/#basic-keyword-search","title":"Basic Keyword Search","text":"<pre><code>from echomine import OpenAIAdapter, SearchQuery\nfrom pathlib import Path\n\nadapter = OpenAIAdapter()\nexport_file = Path(\"conversations.json\")\n\n# Create search query\nquery = SearchQuery(\n    keywords=[\"algorithm\", \"leetcode\"],\n    limit=10\n)\n\n# Execute search\nfor result in adapter.search(export_file, query):\n    print(f\"[{result.score:.2f}] {result.conversation.title}\")\n    print(f\"  Snippet: {result.snippet}\")  # v1.1.0: automatic preview\n    print(f\"  Matches: {len(result.matched_message_ids)} messages\")\n</code></pre>"},{"location":"api/models/search/#advanced-search-features-v110","title":"Advanced Search Features (v1.1.0+)","text":"<pre><code># Exact phrase matching\nquery = SearchQuery(phrases=[\"algo-insights\", \"data pipeline\"])\nfor result in adapter.search(export_file, query):\n    print(f\"{result.conversation.title}: {result.snippet}\")\n\n# Boolean match mode (require ALL keywords)\nquery = SearchQuery(\n    keywords=[\"python\", \"async\", \"testing\"],\n    match_mode=\"all\"  # AND logic\n)\n\n# Exclude unwanted results\nquery = SearchQuery(\n    keywords=[\"python\"],\n    exclude_keywords=[\"django\", \"flask\"]\n)\n\n# Role filtering\nquery = SearchQuery(\n    keywords=[\"refactor\"],\n    role_filter=\"user\"  # Search only your messages\n)\n\n# Combine all features\nquery = SearchQuery(\n    keywords=[\"optimization\"],\n    phrases=[\"algo-insights\"],\n    match_mode=\"all\",\n    exclude_keywords=[\"test\"],\n    role_filter=\"user\",\n    limit=10\n)\n</code></pre>"},{"location":"api/models/search/#title-filtering-fast","title":"Title Filtering (Fast)","text":"<p>Title filtering is metadata-only, much faster than full-text search:</p> <pre><code># Search by title (partial match, case-insensitive)\nquery = SearchQuery(\n    title_filter=\"Project Alpha\",\n    limit=10\n)\n\nfor result in adapter.search(export_file, query):\n    print(result.conversation.title)\n</code></pre>"},{"location":"api/models/search/#date-range-filtering","title":"Date Range Filtering","text":"<pre><code>from datetime import date\n\n# Filter by creation date\nquery = SearchQuery(\n    from_date=date(2024, 1, 1),\n    to_date=date(2024, 3, 31),\n    limit=20\n)\n\nfor result in adapter.search(export_file, query):\n    print(f\"{result.conversation.title} - {result.conversation.created_at.date()}\")\n</code></pre>"},{"location":"api/models/search/#combined-filtering","title":"Combined Filtering","text":"<p>Combine multiple filters for precision:</p> <pre><code>query = SearchQuery(\n    keywords=[\"python\", \"async\"],\n    title_filter=\"Tutorial\",\n    from_date=date(2024, 1, 1),\n    to_date=date(2024, 12, 31),\n    limit=5\n)\n\nfor result in adapter.search(export_file, query):\n    print(f\"[{result.score:.2f}] {result.conversation.title}\")\n    print(f\"  Created: {result.conversation.created_at.date()}\")\n    print(f\"  Messages: {len(result.conversation.messages)}\")\n</code></pre>"},{"location":"api/models/search/#working-with-results","title":"Working with Results","text":"<pre><code># Collect results\nresults = list(adapter.search(export_file, query))\n\n# Results are sorted by relevance (descending)\nassert results[0].score &gt;= results[1].score\n\n# Access conversation data\nfor result in results:\n    conv = result.conversation\n    print(f\"Title: {conv.title}\")\n    print(f\"Score: {result.score:.2f}\")\n    print(f\"Messages: {len(conv.messages)}\")\n</code></pre>"},{"location":"api/models/search/#validation","title":"Validation","text":"<p>SearchQuery validates constraints automatically:</p> <pre><code>from pydantic import ValidationError\n\n# \u274c Invalid: from_date &gt; to_date\ntry:\n    invalid = SearchQuery(\n        from_date=date(2024, 12, 31),\n        to_date=date(2024, 1, 1),\n        keywords=[\"test\"]\n    )\nexcept ValidationError as e:\n    print(f\"Error: {e}\")\n\n# \u274c Invalid: limit &lt; 1\ntry:\n    invalid = SearchQuery(\n        keywords=[\"test\"],\n        limit=0\n    )\nexcept ValidationError as e:\n    print(f\"Error: limit must be &gt;= 1\")\n\n# \u2705 Valid: all constraints met\nvalid = SearchQuery(\n    keywords=[\"test\"],\n    from_date=date(2024, 1, 1),\n    to_date=date(2024, 12, 31),\n    limit=10\n)\n</code></pre>"},{"location":"api/models/search/#searchquery-fields","title":"SearchQuery Fields","text":""},{"location":"api/models/search/#content-matching-fields-v110","title":"Content Matching Fields (v1.1.0+)","text":"<ul> <li>keywords (<code>list[str] | None</code>): Keywords for BM25 full-text search</li> <li>phrases (<code>list[str] | None</code>): Exact phrases to match (preserves special characters)</li> <li>match_mode (<code>Literal[\"any\", \"all\"]</code>): Keyword matching logic (default: \"any\")</li> <li><code>\"any\"</code>: OR logic - match if ANY keyword present</li> <li><code>\"all\"</code>: AND logic - match if ALL keywords present</li> <li>exclude_keywords (<code>list[str] | None</code>): Terms to exclude (OR logic - excludes if ANY present)</li> <li>role_filter (<code>Literal[\"user\", \"assistant\", \"system\"] | None</code>): Filter by message author role</li> </ul>"},{"location":"api/models/search/#legacy-filter-fields","title":"Legacy Filter Fields","text":"<ul> <li>title_filter (<code>str | None</code>): Partial title match (case-insensitive)</li> <li>from_date (<code>date | None</code>): Minimum creation date (inclusive)</li> <li>to_date (<code>date | None</code>): Maximum creation date (inclusive)</li> </ul>"},{"location":"api/models/search/#output-control","title":"Output Control","text":"<ul> <li>limit (<code>int</code>): Maximum results to return (default: 10, min: 1)</li> </ul>"},{"location":"api/models/search/#validation-rules","title":"Validation Rules","text":"<ol> <li>At least one filter: Must specify keywords, phrases, or title_filter</li> <li>Date range: If both dates specified, <code>from_date &lt;= to_date</code></li> <li>Limit: Must be &gt;= 1</li> <li>Match mode: Only affects keywords (phrases always use OR logic)</li> <li>Role filter: Must be one of: \"user\", \"assistant\", \"system\" (case-insensitive)</li> </ol>"},{"location":"api/models/search/#searchresult-fields","title":"SearchResult Fields","text":""},{"location":"api/models/search/#fields","title":"Fields","text":"<ul> <li>conversation (<code>Conversation</code>): The matched conversation</li> <li>score (<code>float</code>): Relevance score (0.0 to 1.0, higher is better)</li> <li>matched_message_ids (<code>list[str]</code>): IDs of messages that matched the search query (v1.1.0+)</li> <li>snippet (<code>str</code>): Preview text from first matching message, ~100 characters (v1.1.0+)</li> </ul>"},{"location":"api/models/search/#score-interpretation","title":"Score Interpretation","text":"<ul> <li>1.0: Perfect match (all keywords present, high frequency)</li> <li>0.8-0.9: Excellent match (most keywords, good frequency)</li> <li>0.6-0.7: Good match (some keywords, moderate frequency)</li> <li>0.4-0.5: Fair match (few keywords, low frequency)</li> <li>&lt;0.4: Weak match</li> </ul> <p>Note: Title filtering and date filtering do not affect score. Score is based on BM25 ranking when keywords or phrases are specified.</p>"},{"location":"api/models/search/#snippet-features-v110","title":"Snippet Features (v1.1.0+)","text":"<ul> <li>Extracted from first matching message</li> <li>Truncated to ~100 characters with \"...\" suffix</li> <li>Multiple matches indicated by \"(+N more)\" in CLI output</li> <li>Fallback text for empty/malformed content</li> <li>Always present (never None)</li> </ul>"},{"location":"api/models/search/#working-with-matched-messages","title":"Working with Matched Messages","text":"<pre><code>for result in adapter.search(export_file, query):\n    conversation = result.conversation\n    matched_ids = result.matched_message_ids\n\n    # Find actual matched messages\n    matched_messages = [\n        msg for msg in conversation.messages\n        if msg.id in matched_ids\n    ]\n\n    print(f\"Found {len(matched_messages)} matching messages\")\n    for msg in matched_messages:\n        print(f\"  [{msg.role}] {msg.content[:50]}...\")\n</code></pre>"},{"location":"api/models/search/#search-behavior","title":"Search Behavior","text":""},{"location":"api/models/search/#two-stage-matching-process-v110","title":"Two-Stage Matching Process (v1.1.0+)","text":"<p>Stage 1: Content Matching (OR relationship)</p> <p>Conversations match if ANY of these are true: - Phrases: ANY phrase is found (exact match, case-insensitive) - Keywords: Match according to <code>match_mode</code>   - <code>match_mode=\"any\"</code> (default): ANY keyword present   - <code>match_mode=\"all\"</code>: ALL keywords present</p> <p>Key insight: Phrases and keywords are alternatives, not cumulative. If both specified, matches if EITHER phrases match OR keywords match.</p> <p>Stage 2: Post-Match Filters (AND relationship)</p> <p>After Stage 1, results are filtered by ALL of these: - <code>exclude_keywords</code>: Remove if ANY excluded term found - <code>role_filter</code>: Only messages from specified role - <code>title_filter</code>: Only conversations with matching title - <code>from_date</code> / <code>to_date</code>: Only in date range</p>"},{"location":"api/models/search/#filter-combination-examples","title":"Filter Combination Examples","text":"<pre><code># Phrase OR keyword (matches either)\nSearchQuery(phrases=[\"api\"], keywords=[\"python\"])\n\n# Multiple keywords with ALL mode (requires both)\nSearchQuery(keywords=[\"python\", \"async\"], match_mode=\"all\")\n\n# Content + exclusion\nSearchQuery(phrases=[\"api\"], keywords=[\"python\"], exclude_keywords=[\"java\"])\n\n# Role-specific search\nSearchQuery(keywords=[\"python\"], role_filter=\"user\")\n</code></pre>"},{"location":"api/models/search/#legacy-behavior-v10x","title":"Legacy Behavior (v1.0.x)","text":"<p>For backward compatibility, v1.0.x behavior is preserved:</p> <ol> <li>Date range filter (if specified)</li> <li>Title filter (if specified) - metadata-only</li> <li>Keyword search (if specified) - full-text with BM25</li> <li>Limit results</li> </ol>"},{"location":"api/models/search/#keyword-search-bm25","title":"Keyword Search (BM25)","text":"<p>When keywords or phrases are specified:</p> <ol> <li>Full-text search across message content</li> <li>BM25 relevance ranking</li> <li>Results sorted by score (descending)</li> <li>Snippet extraction from first match</li> </ol> <p>Performance: Scans all conversation content. Slower but comprehensive.</p>"},{"location":"api/models/search/#title-filtering","title":"Title Filtering","text":"<p>When only title_filter is specified:</p> <ol> <li>Metadata-only search (no message content scan)</li> <li>Partial match, case-insensitive</li> <li>Results returned in file order</li> </ol> <p>Performance: Fast (metadata-only). Use when you remember the title.</p>"},{"location":"api/models/search/#date-filtering","title":"Date Filtering","text":"<p>When date range is specified:</p> <ol> <li>Filters by <code>conversation.created_at</code></li> <li>Inclusive range (from_date &lt;= created_at &lt;= to_date)</li> <li>Can be combined with keyword, phrase, or title search</li> </ol>"},{"location":"api/models/search/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use title filtering when possible: 10-100x faster than keyword search</li> <li>Limit results: Use <code>limit</code> to avoid processing thousands of matches</li> <li>Narrow date ranges: Reduces conversations to search</li> <li>Specific keywords: More specific keywords = better ranking</li> </ol>"},{"location":"api/models/search/#related-models","title":"Related Models","text":"<ul> <li>Conversation: Result conversation model</li> <li>Message: Message model within conversations</li> </ul>"},{"location":"api/models/search/#see-also","title":"See Also","text":"<ul> <li>Library Usage Guide</li> <li>BM25 Ranking</li> <li>OpenAI Adapter</li> </ul>"},{"location":"api/search/ranking/","title":"BM25 Ranking","text":"<p>BM25 relevance ranking algorithm for full-text search.</p>"},{"location":"api/search/ranking/#overview","title":"Overview","text":"<p>Echomine uses BM25 (Best Matching 25) algorithm for relevance ranking when searching conversations by keywords. BM25 is a probabilistic ranking function used by search engines to estimate the relevance of documents to a given query.</p>"},{"location":"api/search/ranking/#api-reference","title":"API Reference","text":""},{"location":"api/search/ranking/#echomine.search.ranking","title":"ranking","text":"<p>BM25 relevance ranking for conversation search.</p> <p>BM25 (Best Matching 25) is a probabilistic relevance ranking function. Used to score documents (conversations) based on keyword queries.</p> <p>Parameters (from FR-317):     k1 = 1.5  # Term frequency saturation parameter     b = 0.75  # Length normalization parameter</p> Algorithm <p>score(D, Q) = \u03a3 IDF(qi) * (f(qi, D) * (k1 + 1)) / (f(qi, D) + k1 * (1 - b + b * |D|/avgdl))</p> <p>Where: - D: Document (conversation) - Q: Query (keywords) - f(qi, D): Frequency of keyword qi in document D - |D|: Length of document D (word count) - avgdl: Average document length across corpus - IDF(qi): Inverse document frequency of qi</p> Constitution Compliance <ul> <li>Principle VI: Strict typing with mypy --strict</li> <li>FR-317-326: BM25 algorithm with standard parameters</li> <li>FR-319: Case-insensitive keyword matching</li> </ul> <p>Advanced Search Features (v1.1.0):     - FR-001-006: phrase_matches() for exact phrase matching     - FR-012-016: exclude_filter() for exclusion filtering</p>"},{"location":"api/search/ranking/#echomine.search.ranking.BM25Scorer","title":"BM25Scorer","text":"<pre><code>BM25Scorer(corpus: list[str], avg_doc_length: float)\n</code></pre> <p>BM25 relevance scorer for conversations.</p> <p>Scores conversations based on keyword matches using BM25 algorithm. Normalizes scores to [0.0, 1.0] range for consistency.</p> Algorithm Details <ul> <li>k1 = 1.5 (term frequency saturation)</li> <li>b = 0.75 (length normalization)</li> <li>IDF: log((N - df + 0.5) / (df + 0.5) + 1)</li> <li>Score normalization: score_normalized = score_raw / (score_raw + 1) per FR-319</li> </ul> Example <pre><code># Build corpus\ncorpus = [\n    \"Python is a great programming language\",\n    \"I love Python and JavaScript\",\n    \"Go is fast but Python is easier\"\n]\n\n# Initialize scorer\nscorer = BM25Scorer(corpus=corpus, avg_doc_length=7.0)\n\n# Score documents\nscore = scorer.score(\"Python is a great programming language\", [\"python\"])\n# Returns BM25 score (higher = more relevant)\n</code></pre> Requirements <ul> <li>FR-317: BM25 algorithm implementation</li> <li>FR-318: k1 = 1.5, b = 0.75</li> <li>FR-319: Case-insensitive matching</li> <li>FR-326: Score normalization to [0.0, 1.0]</li> </ul> <p>Initialize BM25 scorer with corpus statistics.</p> <p>Parameters:</p> Name Type Description Default <code>corpus</code> <code>list[str]</code> <p>List of document texts (conversation content)</p> required <code>avg_doc_length</code> <code>float</code> <p>Average document length (for normalization)</p> required Example <pre><code>corpus = [\"doc one content\", \"doc two content\"]\navg_len = sum(len(doc.split()) for doc in corpus) / len(corpus)\nscorer = BM25Scorer(corpus=corpus, avg_doc_length=avg_len)\n</code></pre> Source code in <code>src/echomine/search/ranking.py</code> <pre><code>def __init__(self, corpus: list[str], avg_doc_length: float) -&gt; None:\n    \"\"\"Initialize BM25 scorer with corpus statistics.\n\n    Args:\n        corpus: List of document texts (conversation content)\n        avg_doc_length: Average document length (for normalization)\n\n    Example:\n        ```python\n        corpus = [\"doc one content\", \"doc two content\"]\n        avg_len = sum(len(doc.split()) for doc in corpus) / len(corpus)\n        scorer = BM25Scorer(corpus=corpus, avg_doc_length=avg_len)\n        ```\n    \"\"\"\n    self.corpus_size = len(corpus)\n    self.avg_doc_length = avg_doc_length\n\n    # Calculate IDF scores for all terms\n    self.idf_scores: dict[str, float] = self._calculate_idf(corpus)\n</code></pre>"},{"location":"api/search/ranking/#echomine.search.ranking.BM25Scorer.score","title":"score","text":"<pre><code>score(document: str, keywords: list[str]) -&gt; float\n</code></pre> <p>Score a document for given keywords using BM25.</p> <p>Tokenizes both document and keywords using the same method to ensure consistent matching. Multi-character keywords (e.g., Chinese \"\u7f16\u7a0b\") are split into individual character tokens.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>str</code> <p>Document text (conversation content)</p> required <code>keywords</code> <code>list[str]</code> <p>List of query keywords (will be tokenized)</p> required <p>Returns:</p> Type Description <code>float</code> <p>BM25 score (higher = more relevant, unnormalized)</p> Example <pre><code>doc = \"Python is a great programming language\"\nkeywords = [\"python\", \"programming\"]\nscore = scorer.score(doc, keywords)\n# Returns sum of BM25 scores for each keyword token\n</code></pre> Algorithm <p>For each keyword token qi:     IDF(qi) * (f(qi, D) * (k1 + 1)) / (f(qi, D) + k1 * (1 - b + b * |D|/avgdl))</p> Source code in <code>src/echomine/search/ranking.py</code> <pre><code>def score(self, document: str, keywords: list[str]) -&gt; float:\n    \"\"\"Score a document for given keywords using BM25.\n\n    Tokenizes both document and keywords using the same method to ensure\n    consistent matching. Multi-character keywords (e.g., Chinese \"\u7f16\u7a0b\")\n    are split into individual character tokens.\n\n    Args:\n        document: Document text (conversation content)\n        keywords: List of query keywords (will be tokenized)\n\n    Returns:\n        BM25 score (higher = more relevant, unnormalized)\n\n    Example:\n        ```python\n        doc = \"Python is a great programming language\"\n        keywords = [\"python\", \"programming\"]\n        score = scorer.score(doc, keywords)\n        # Returns sum of BM25 scores for each keyword token\n        ```\n\n    Algorithm:\n        For each keyword token qi:\n            IDF(qi) * (f(qi, D) * (k1 + 1)) / (f(qi, D) + k1 * (1 - b + b * |D|/avgdl))\n    \"\"\"\n    # Tokenize document using improved tokenization\n    doc_terms = self._tokenize(document)\n    doc_length = len(doc_terms)\n\n    # Count term frequencies\n    tf_counter: CounterType[str] = Counter(doc_terms)\n\n    # Tokenize keywords (handles multi-character keywords like \"\u7f16\u7a0b\")\n    keyword_tokens: list[str] = []\n    for keyword in keywords:\n        keyword_tokens.extend(self._tokenize(keyword))\n\n    # Calculate BM25 score\n    score = 0.0\n\n    for kw_token in keyword_tokens:\n        # Get IDF score (0 if term not in corpus)\n        idf = self.idf_scores.get(kw_token, 0.0)\n\n        # Get term frequency in document\n        tf = tf_counter.get(kw_token, 0)\n\n        # BM25 formula\n        numerator = tf * (self.K1 + 1.0)\n\n        # Guard against division by zero when avg_doc_length is 0\n        # This can happen with sparse corpora (e.g., role_filter=system with few system messages)\n        # When avg_doc_length is 0, skip length normalization (use ratio of 1.0)\n        length_ratio = doc_length / self.avg_doc_length if self.avg_doc_length &gt; 0 else 1.0\n\n        denominator = tf + self.K1 * (1.0 - self.B + self.B * length_ratio)\n\n        score += idf * (numerator / denominator)\n\n    return score\n</code></pre>"},{"location":"api/search/ranking/#echomine.search.ranking.phrase_matches","title":"phrase_matches","text":"<pre><code>phrase_matches(text: str, phrases: list[str]) -&gt; bool\n</code></pre> <p>Check if any phrase matches in the text (case-insensitive substring).</p> <p>This function implements exact phrase matching without tokenization. Phrases are matched as-is, preserving hyphens, underscores, and spaces.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to search in (e.g., conversation content)</p> required <code>phrases</code> <code>list[str]</code> <p>List of phrases to match</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if any phrase is found in text, False otherwise</p> Example <pre><code>text = \"We use algo-insights for data analysis\"\nassert phrase_matches(text, [\"algo-insights\"]) is True\nassert phrase_matches(text, [\"algorithm\"]) is False\n</code></pre> Requirements <ul> <li>FR-001: Exact phrase matching (no tokenization)</li> <li>FR-003: Case-insensitive matching</li> <li>FR-006: Special characters matched literally</li> </ul> Source code in <code>src/echomine/search/ranking.py</code> <pre><code>def phrase_matches(text: str, phrases: list[str]) -&gt; bool:\n    \"\"\"Check if any phrase matches in the text (case-insensitive substring).\n\n    This function implements exact phrase matching without tokenization.\n    Phrases are matched as-is, preserving hyphens, underscores, and spaces.\n\n    Args:\n        text: Text to search in (e.g., conversation content)\n        phrases: List of phrases to match\n\n    Returns:\n        True if any phrase is found in text, False otherwise\n\n    Example:\n        ```python\n        text = \"We use algo-insights for data analysis\"\n        assert phrase_matches(text, [\"algo-insights\"]) is True\n        assert phrase_matches(text, [\"algorithm\"]) is False\n        ```\n\n    Requirements:\n        - FR-001: Exact phrase matching (no tokenization)\n        - FR-003: Case-insensitive matching\n        - FR-006: Special characters matched literally\n    \"\"\"\n    # Empty phrases list means no matches possible\n    if not phrases:\n        return False\n\n    # Empty text means no matches possible\n    if not text:\n        return False\n\n    # Case-insensitive substring matching (OR logic for multiple phrases)\n    text_lower = text.lower()\n    return any(phrase.lower() in text_lower for phrase in phrases)\n</code></pre>"},{"location":"api/search/ranking/#echomine.search.ranking.all_terms_present","title":"all_terms_present","text":"<pre><code>all_terms_present(text: str, keywords: list[str], scorer: BM25Scorer) -&gt; bool\n</code></pre> <p>Check if ALL keyword tokens are present in the text.</p> <p>Uses the same tokenization as BM25Scorer to ensure consistent matching behavior. All keyword tokens must be present in the text tokens for match_mode='all' to succeed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to check</p> required <code>keywords</code> <code>list[str]</code> <p>Keywords to find (will be tokenized)</p> required <code>scorer</code> <code>BM25Scorer</code> <p>BM25Scorer instance for tokenization</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if ALL keyword tokens are present in text,</p> <code>bool</code> <p>False otherwise</p> Example <pre><code>scorer = BM25Scorer(corpus=[\"test\"], avg_doc_length=1.0)\ntext = \"Python and Java programming\"\nassert all_terms_present(text, [\"python\", \"java\"], scorer) is True\nassert all_terms_present(text, [\"python\", \"rust\"], scorer) is False\n</code></pre> Requirements <ul> <li>FR-009: All keywords must be present (AND logic)</li> <li>FR-010: Uses same tokenization as BM25</li> </ul> Source code in <code>src/echomine/search/ranking.py</code> <pre><code>def all_terms_present(text: str, keywords: list[str], scorer: BM25Scorer) -&gt; bool:\n    \"\"\"Check if ALL keyword tokens are present in the text.\n\n    Uses the same tokenization as BM25Scorer to ensure consistent matching\n    behavior. All keyword tokens must be present in the text tokens for\n    match_mode='all' to succeed.\n\n    Args:\n        text: Text to check\n        keywords: Keywords to find (will be tokenized)\n        scorer: BM25Scorer instance for tokenization\n\n    Returns:\n        True if ALL keyword tokens are present in text,\n        False otherwise\n\n    Example:\n        ```python\n        scorer = BM25Scorer(corpus=[\"test\"], avg_doc_length=1.0)\n        text = \"Python and Java programming\"\n        assert all_terms_present(text, [\"python\", \"java\"], scorer) is True\n        assert all_terms_present(text, [\"python\", \"rust\"], scorer) is False\n        ```\n\n    Requirements:\n        - FR-009: All keywords must be present (AND logic)\n        - FR-010: Uses same tokenization as BM25\n    \"\"\"\n    # Empty keywords list is vacuously true (all of nothing is present)\n    if not keywords:\n        return True\n\n    # Tokenize the text using BM25Scorer's tokenization\n    text_tokens = set(scorer._tokenize(text))  # noqa: SLF001\n\n    # Tokenize all keywords and check if each token is present\n    for keyword in keywords:\n        keyword_tokens = scorer._tokenize(keyword)  # noqa: SLF001\n        # All tokens from this keyword must be present\n        for token in keyword_tokens:\n            if token not in text_tokens:\n                return False\n\n    return True\n</code></pre>"},{"location":"api/search/ranking/#echomine.search.ranking.exclude_filter","title":"exclude_filter","text":"<pre><code>exclude_filter(\n    text: str, exclude_keywords: list[str], scorer: BM25Scorer\n) -&gt; bool\n</code></pre> <p>Check if text contains any excluded keywords.</p> <p>Uses the same tokenization as BM25Scorer to ensure consistent matching behavior between inclusion and exclusion.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to check for excluded terms</p> required <code>exclude_keywords</code> <code>list[str]</code> <p>Keywords to exclude (will be tokenized)</p> required <code>scorer</code> <code>BM25Scorer</code> <p>BM25Scorer instance for tokenization</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if text should be EXCLUDED (contains any excluded term),</p> <code>bool</code> <p>False if text should be KEPT (no excluded terms found)</p> Example <pre><code>scorer = BM25Scorer(corpus=[\"test\"], avg_doc_length=1.0)\ntext = \"Python with Django framework\"\nassert exclude_filter(text, [\"django\"], scorer) is True  # Exclude\nassert exclude_filter(text, [\"flask\"], scorer) is False  # Keep\n</code></pre> Requirements <ul> <li>FR-014: Applied after matching, before ranking</li> <li>FR-015: Uses same tokenization as keywords</li> </ul> Source code in <code>src/echomine/search/ranking.py</code> <pre><code>def exclude_filter(text: str, exclude_keywords: list[str], scorer: BM25Scorer) -&gt; bool:\n    \"\"\"Check if text contains any excluded keywords.\n\n    Uses the same tokenization as BM25Scorer to ensure consistent matching\n    behavior between inclusion and exclusion.\n\n    Args:\n        text: Text to check for excluded terms\n        exclude_keywords: Keywords to exclude (will be tokenized)\n        scorer: BM25Scorer instance for tokenization\n\n    Returns:\n        True if text should be EXCLUDED (contains any excluded term),\n        False if text should be KEPT (no excluded terms found)\n\n    Example:\n        ```python\n        scorer = BM25Scorer(corpus=[\"test\"], avg_doc_length=1.0)\n        text = \"Python with Django framework\"\n        assert exclude_filter(text, [\"django\"], scorer) is True  # Exclude\n        assert exclude_filter(text, [\"flask\"], scorer) is False  # Keep\n        ```\n\n    Requirements:\n        - FR-014: Applied after matching, before ranking\n        - FR-015: Uses same tokenization as keywords\n    \"\"\"\n    # Empty exclusions means keep all\n    if not exclude_keywords:\n        return False\n\n    # Empty text has no tokens to match exclusions\n    if not text:\n        return False\n\n    # Tokenize the text using BM25Scorer's tokenization\n    text_tokens = set(scorer._tokenize(text))  # noqa: SLF001\n\n    # Check if ANY excluded token is present (OR logic for exclusion)\n    for keyword in exclude_keywords:\n        keyword_tokens = scorer._tokenize(keyword)  # noqa: SLF001\n        # If any token from this keyword is present, exclude\n        for token in keyword_tokens:\n            if token in text_tokens:\n                return True\n\n    return False\n</code></pre>"},{"location":"api/search/ranking/#how-bm25-works","title":"How BM25 Works","text":""},{"location":"api/search/ranking/#algorithm","title":"Algorithm","text":"<p>BM25 calculates a relevance score based on:</p> <ol> <li>Term Frequency (TF): How often keywords appear in the conversation</li> <li>Inverse Document Frequency (IDF): How rare keywords are across all conversations</li> <li>Document Length Normalization: Adjusts for conversation length</li> </ol> <p>Formula:</p> <pre><code>score(D, Q) = \u03a3 IDF(qi) \u00b7 (f(qi, D) \u00b7 (k1 + 1)) / (f(qi, D) + k1 \u00b7 (1 - b + b \u00b7 |D| / avgdl))\n</code></pre> <p>Where: - <code>D</code> = Conversation (document) - <code>Q</code> = Search query keywords - <code>f(qi, D)</code> = Frequency of keyword qi in conversation D - <code>|D|</code> = Length of conversation (number of words) - <code>avgdl</code> = Average conversation length across all conversations - <code>k1</code> = Term saturation parameter (default: 1.5) - <code>b</code> = Length normalization parameter (default: 0.75)</p>"},{"location":"api/search/ranking/#parameters","title":"Parameters","text":"<p>Echomine uses standard BM25 parameters:</p> <ul> <li>k1 = 1.5: Controls term saturation (higher = more weight to term frequency)</li> <li>b = 0.75: Controls length normalization (higher = more penalty for long documents)</li> </ul>"},{"location":"api/search/ranking/#usage-examples","title":"Usage Examples","text":""},{"location":"api/search/ranking/#basic-search-with-ranking","title":"Basic Search with Ranking","text":"<pre><code>from echomine import OpenAIAdapter, SearchQuery\nfrom pathlib import Path\n\nadapter = OpenAIAdapter()\nexport_file = Path(\"conversations.json\")\n\n# Search with keywords\nquery = SearchQuery(keywords=[\"python\", \"async\"], limit=10)\n\n# Results are ranked by BM25 score\nfor result in adapter.search(export_file, query):\n    print(f\"[{result.score:.2f}] {result.conversation.title}\")\n</code></pre>"},{"location":"api/search/ranking/#score-interpretation","title":"Score Interpretation","text":"<pre><code>for result in adapter.search(export_file, query):\n    score = result.score\n\n    if score &gt;= 0.8:\n        quality = \"Excellent match\"\n    elif score &gt;= 0.6:\n        quality = \"Good match\"\n    elif score &gt;= 0.4:\n        quality = \"Fair match\"\n    else:\n        quality = \"Weak match\"\n\n    print(f\"[{score:.2f}] {quality}: {result.conversation.title}\")\n</code></pre>"},{"location":"api/search/ranking/#filtering-by-score-threshold","title":"Filtering by Score Threshold","text":"<pre><code># Only show high-quality matches\nmin_score = 0.6\n\nfor result in adapter.search(export_file, query):\n    if result.score &gt;= min_score:\n        print(f\"[{result.score:.2f}] {result.conversation.title}\")\n</code></pre>"},{"location":"api/search/ranking/#score-normalization","title":"Score Normalization","text":"<p>Scores are normalized to 0.0-1.0 range:</p> <ul> <li>1.0: Perfect match (all keywords present, high frequency)</li> <li>0.8-0.9: Excellent match (most keywords, good frequency)</li> <li>0.6-0.7: Good match (some keywords, moderate frequency)</li> <li>0.4-0.5: Fair match (few keywords, low frequency)</li> <li>&lt;0.4: Weak match</li> </ul> <p>Note: Scores are relative. A score of 0.8 doesn't mean \"80% match\" - it means the conversation is in the top tier of matches for the query.</p>"},{"location":"api/search/ranking/#search-behavior","title":"Search Behavior","text":""},{"location":"api/search/ranking/#keyword-processing","title":"Keyword Processing","text":"<p>Keywords are processed as follows:</p> <ol> <li>Case-insensitive: \"Python\" matches \"python\", \"PYTHON\"</li> <li>Whole words: \"python\" doesn't match \"pythonic\" (unless stemming is enabled)</li> <li>Multiple keywords: All keywords contribute to score (OR logic)</li> </ol>"},{"location":"api/search/ranking/#content-searched","title":"Content Searched","text":"<p>BM25 searches across:</p> <ul> <li>Message content (all messages in conversation)</li> <li>Conversation title (weighted higher)</li> </ul>"},{"location":"api/search/ranking/#ranking-order","title":"Ranking Order","text":"<p>Results are sorted by score in descending order:</p> <pre><code>results = list(adapter.search(export_file, query))\n\n# Results are pre-sorted by score\nassert results[0].score &gt;= results[1].score &gt;= results[2].score\n</code></pre>"},{"location":"api/search/ranking/#performance","title":"Performance","text":""},{"location":"api/search/ranking/#time-complexity","title":"Time Complexity","text":"<ul> <li>O(n \u00b7 m): Where n = number of conversations, m = average messages per conversation</li> <li>Early termination: Stops after finding <code>limit</code> top matches</li> </ul>"},{"location":"api/search/ranking/#memory-complexity","title":"Memory Complexity","text":"<ul> <li>O(1): Constant memory (streaming-based)</li> <li>No full conversation buffering</li> </ul>"},{"location":"api/search/ranking/#speed","title":"Speed","text":"<ul> <li>1.6GB file: &lt;30 seconds for keyword search</li> <li>10K conversations: &lt;10 seconds</li> </ul>"},{"location":"api/search/ranking/#comparison-with-other-ranking-methods","title":"Comparison with Other Ranking Methods","text":""},{"location":"api/search/ranking/#bm25-vs-tf-idf","title":"BM25 vs TF-IDF","text":"Feature BM25 TF-IDF Saturation Yes (k1 parameter) No Length normalization Yes (b parameter) Limited Performance Better for varied-length documents Better for uniform documents Echomine choice \u2705 Used \u274c Not used <p>Why BM25? Conversations vary widely in length. BM25's length normalization prevents long conversations from dominating results.</p>"},{"location":"api/search/ranking/#bm25-vs-semantic-search","title":"BM25 vs Semantic Search","text":"Feature BM25 Semantic Search Query type Keywords Natural language Speed Fast (no ML) Slower (embedding models) Setup No dependencies Requires embedding models Accuracy Good for exact terms Better for synonyms/concepts Echomine v1.0 \u2705 Used \u274c Not used <p>Future: Semantic search may be added in v2.0 as an optional feature.</p>"},{"location":"api/search/ranking/#limitations","title":"Limitations","text":"<ol> <li>No fuzzy matching: \"python\" doesn't match \"pythonic\"</li> <li>No synonym matching: \"car\" doesn't match \"automobile\"</li> <li>Keyword-only: Doesn't understand semantic meaning</li> <li>No phrase matching: \"machine learning\" treated as two separate keywords</li> </ol>"},{"location":"api/search/ranking/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/search/ranking/#multi-keyword-queries","title":"Multi-Keyword Queries","text":"<pre><code># All keywords contribute to score (OR logic)\nquery = SearchQuery(\n    keywords=[\"python\", \"async\", \"asyncio\", \"coroutine\"],\n    limit=10\n)\n\n# Conversations with more keyword matches rank higher\nfor result in adapter.search(export_file, query):\n    print(f\"[{result.score:.2f}] {result.conversation.title}\")\n</code></pre>"},{"location":"api/search/ranking/#combining-with-filters","title":"Combining with Filters","text":"<pre><code>from datetime import date\n\n# BM25 ranking + date filtering\nquery = SearchQuery(\n    keywords=[\"algorithm\", \"design\"],\n    from_date=date(2024, 1, 1),\n    to_date=date(2024, 3, 31),\n    limit=10\n)\n\n# Date filter reduces search space, then BM25 ranks\nfor result in adapter.search(export_file, query):\n    print(f\"[{result.score:.2f}] {result.conversation.title}\")\n</code></pre>"},{"location":"api/search/ranking/#implementation-details","title":"Implementation Details","text":""},{"location":"api/search/ranking/#tokenization","title":"Tokenization","text":"<p>Messages are tokenized by:</p> <ol> <li>Lowercase conversion</li> <li>Split on whitespace and punctuation</li> <li>Filter stop words (optional, not implemented in v1.0)</li> </ol>"},{"location":"api/search/ranking/#idf-calculation","title":"IDF Calculation","text":"<pre><code>import math\n\ndef calculate_idf(term: str, total_docs: int, docs_with_term: int) -&gt; float:\n    \"\"\"Calculate inverse document frequency.\"\"\"\n    return math.log((total_docs - docs_with_term + 0.5) / (docs_with_term + 0.5) + 1.0)\n</code></pre>"},{"location":"api/search/ranking/#score-calculation","title":"Score Calculation","text":"<pre><code>def calculate_bm25_score(\n    term_freq: int,\n    doc_length: int,\n    avg_doc_length: float,\n    idf: float,\n    k1: float = 1.5,\n    b: float = 0.75,\n) -&gt; float:\n    \"\"\"Calculate BM25 score for a single term.\"\"\"\n    numerator = term_freq * (k1 + 1)\n    denominator = term_freq + k1 * (1 - b + b * (doc_length / avg_doc_length))\n    return idf * (numerator / denominator)\n</code></pre>"},{"location":"api/search/ranking/#related","title":"Related","text":"<ul> <li>SearchQuery: Search parameters</li> <li>SearchResult: Result model with score</li> <li>OpenAI Adapter: Adapter using BM25</li> </ul>"},{"location":"api/search/ranking/#see-also","title":"See Also","text":"<ul> <li>Library Usage</li> <li>Architecture</li> </ul>"},{"location":"api/search/ranking/#references","title":"References","text":"<ul> <li>BM25 on Wikipedia</li> <li>Elasticsearch BM25</li> </ul>"},{"location":"development/","title":"Development Guide","text":"<p>Welcome to the Echomine development documentation! This section contains detailed guides for contributors working on the project.</p>"},{"location":"development/#quick-navigation","title":"Quick Navigation","text":""},{"location":"development/#getting-started","title":"Getting Started","text":"<ul> <li>Setup Guide: Environment setup and installation</li> <li>Workflows: Common development workflows</li> </ul>"},{"location":"development/#development-standards","title":"Development Standards","text":"<ul> <li>Testing Guide: Writing and running tests (TDD)</li> <li>Type Checking: mypy --strict usage and patterns</li> <li>Documentation: Writing docs and docstrings</li> </ul>"},{"location":"development/#overview","title":"Overview","text":"<p>Echomine follows strict development practices to ensure quality, type safety, and maintainability. All contributors must follow these standards.</p>"},{"location":"development/#core-principles","title":"Core Principles","text":"<ol> <li>Test-Driven Development (TDD)</li> <li>Write tests FIRST (RED phase)</li> <li>Implement minimal code to pass (GREEN phase)</li> <li> <p>Refactor while keeping tests green (REFACTOR phase)</p> </li> <li> <p>Type Safety</p> </li> <li>mypy --strict must pass with zero errors</li> <li>All functions must have type hints</li> <li> <p>No <code>Any</code> types in public API</p> </li> <li> <p>Library-First Architecture</p> </li> <li>Core functionality in <code>src/echomine/</code> (importable library)</li> <li>CLI wraps library, never the reverse</li> <li> <p>All features available programmatically</p> </li> <li> <p>Memory Efficiency</p> </li> <li>O(1) memory usage via streaming</li> <li>ijson for parsing large files</li> <li>Generator patterns for all iterations</li> </ol> <p>For complete development guidelines, see CONTRIBUTING.md.</p>"},{"location":"development/#development-workflow-overview","title":"Development Workflow Overview","text":"<pre><code>graph TD\n    A[Create Feature Branch] --&gt; B[Write Failing Test RED]\n    B --&gt; C[Run Test - Verify Failure]\n    C --&gt; D[Implement Code GREEN]\n    D --&gt; E[Run Test - Verify Pass]\n    E --&gt; F[Refactor]\n    F --&gt; G{More Tests?}\n    G --&gt;|Yes| B\n    G --&gt;|No| H[Run Quality Checks]\n    H --&gt; I[mypy --strict]\n    H --&gt; J[ruff check/format]\n    H --&gt; K[pytest --cov]\n    I --&gt; L{All Pass?}\n    J --&gt; L\n    K --&gt; L\n    L --&gt;|No| M[Fix Issues]\n    M --&gt; H\n    L --&gt;|Yes| N[Commit &amp; Push]\n    N --&gt; O[Create PR]\n</code></pre>"},{"location":"development/#quick-reference-commands","title":"Quick Reference Commands","text":""},{"location":"development/#testing","title":"Testing","text":"<pre><code># All tests with coverage\npytest --cov=echomine --cov-report=term-missing\n\n# Unit tests only (fast)\npytest tests/unit/ -v\n\n# Watch mode (requires pytest-watch)\nptw -- tests/unit/\n</code></pre>"},{"location":"development/#type-checking","title":"Type Checking","text":"<pre><code># Strict type checking\nmypy --strict src/echomine/\n\n# Check with tests too\nmypy --strict src/echomine/ tests/\n</code></pre>"},{"location":"development/#linting-and-formatting","title":"Linting and Formatting","text":"<pre><code># Auto-fix linting issues\nruff check --fix src/ tests/\n\n# Format code\nruff format src/ tests/\n\n# Check without making changes\nruff check src/ tests/\nruff format --check src/ tests/\n</code></pre>"},{"location":"development/#pre-commit","title":"Pre-Commit","text":"<pre><code># Run all hooks\npre-commit run --all-files\n\n# Run specific hook\npre-commit run mypy --all-files\n</code></pre>"},{"location":"development/#documentation","title":"Documentation","text":"<pre><code># Build docs locally\nmkdocs build\n\n# Serve docs with live reload\nmkdocs serve\n\n# Deploy to GitHub Pages\nmkdocs gh-deploy\n</code></pre>"},{"location":"development/#project-structure","title":"Project Structure","text":"<pre><code>echomine/\n\u251c\u2500\u2500 src/echomine/           # Library (importable, reusable)\n\u2502   \u251c\u2500\u2500 models/             # Pydantic data models\n\u2502   \u251c\u2500\u2500 protocols/          # Protocol definitions\n\u2502   \u251c\u2500\u2500 adapters/           # Provider implementations\n\u2502   \u251c\u2500\u2500 search/             # Search and ranking\n\u2502   \u251c\u2500\u2500 utils/              # Utilities (logging, exceptions)\n\u2502   \u2514\u2500\u2500 cli/                # CLI commands (wraps library)\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 unit/               # Fast, isolated tests (70%)\n\u2502   \u251c\u2500\u2500 integration/        # Component interaction (20%)\n\u2502   \u251c\u2500\u2500 contract/           # Protocol compliance (5%)\n\u2502   \u251c\u2500\u2500 performance/        # Benchmarks (5%)\n\u2502   \u2514\u2500\u2500 fixtures/           # Test data\n\u251c\u2500\u2500 docs/                   # Documentation (mkdocs)\n\u251c\u2500\u2500 specs/                  # Feature specifications\n\u2514\u2500\u2500 .claude/                # AI agent coordination\n</code></pre>"},{"location":"development/#development-standards_1","title":"Development Standards","text":""},{"location":"development/#code-quality-gates","title":"Code Quality Gates","text":"<p>All code must pass these checks before merging:</p> <ul> <li>\u2705 Tests: <code>pytest</code> passes with &gt;80% coverage</li> <li>\u2705 Type Checking: <code>mypy --strict</code> passes with zero errors</li> <li>\u2705 Linting: <code>ruff check</code> passes</li> <li>\u2705 Formatting: <code>ruff format --check</code> passes</li> <li>\u2705 Documentation: All public APIs have docstrings</li> <li>\u2705 Conventional Commits: Commit messages follow format</li> </ul>"},{"location":"development/#constitution-principles","title":"Constitution Principles","text":"<p>The project follows 8 constitution principles (non-negotiable):</p> <ol> <li>Library-First Architecture</li> <li>CLI Interface Contract (stdout/stderr separation)</li> <li>Test-Driven Development (TDD mandatory)</li> <li>Observability &amp; Debuggability (JSON logs)</li> <li>Simplicity &amp; YAGNI</li> <li>Strict Typing Mandatory (mypy --strict)</li> <li>Multi-Provider Adapter Pattern</li> <li>Memory Efficiency &amp; Streaming</li> </ol> <p>For details, see Architecture or <code>specs/001-ai-chat-parser/constitution.md</code>.</p>"},{"location":"development/#getting-help","title":"Getting Help","text":"<ul> <li>General Development: Read CONTRIBUTING.md</li> <li>Setup Issues: See Setup Guide</li> <li>Testing Questions: See Testing Guide</li> <li>Type Errors: See Type Checking Guide</li> <li>Other Questions: Open an issue or start a discussion</li> </ul>"},{"location":"development/#next-steps","title":"Next Steps","text":"<ol> <li>New to the project? Start with Setup Guide</li> <li>Ready to code? Review Workflows</li> <li>Writing tests? See Testing Guide</li> <li>Type errors? Check Type Checking Guide</li> <li>Updating docs? Read Documentation Guide</li> </ol> <p>Last Updated: 2025-11-28</p>"},{"location":"development/adr-timestamp-handling/","title":"ADR: Timestamp Handling","text":"<p>Date: 2025-11-22 Status: Accepted Decision: Use semantic optionality (<code>Optional[datetime]</code>) instead of Unix epoch fallback for <code>updated_at</code></p>"},{"location":"development/adr-timestamp-handling/#overview","title":"Overview","text":"<p>This document describes the design decisions and best practices for handling potentially null timestamps in the echomine Conversation model.</p>"},{"location":"development/adr-timestamp-handling/#design-decision-optional-updated_at-with-semantic-fallback","title":"Design Decision: Optional <code>updated_at</code> with Semantic Fallback","text":""},{"location":"development/adr-timestamp-handling/#the-problem","title":"The Problem","text":"<p>OpenAI conversation exports have <code>create_time</code> and <code>update_time</code> fields at the conversation level. While these fields are present in all real-world exports (verified with 280+ conversations), the schema doesn't guarantee they'll always be present.</p> <p>Anti-Pattern: Unix Epoch Fallback <pre><code># WRONG: Creates false data\nupdated_at = (\n    datetime.fromtimestamp(float(update_time), tz=UTC)\n    if update_time is not None\n    else datetime.fromtimestamp(0, tz=UTC)  # 1970-01-01 is FALSE DATA\n)\n</code></pre></p> <p>Problems with Unix epoch fallback: - Creates false data that never existed - Corrupts analytics and search results - Misleads users (conversation from 1970?) - Violates data integrity principles - Difficult to distinguish \"unknown\" from \"actually 1970\"</p>"},{"location":"development/adr-timestamp-handling/#the-solution-semantic-optionality","title":"The Solution: Semantic Optionality","text":"<pre><code>class Conversation(BaseModel):\n    created_at: datetime  # REQUIRED - every conversation must have creation time\n    updated_at: Optional[datetime] = None  # OPTIONAL - None means \"never updated\"\n\n    @property\n    def updated_at_or_created(self) -&gt; datetime:\n        \"\"\"Fallback to created_at if never updated.\"\"\"\n        return self.updated_at if self.updated_at is not None else self.created_at\n</code></pre> <p>Why this works: 1. Semantic Correctness: <code>None</code> explicitly means \"never modified\" (not \"unknown\") 2. Data Integrity: Never invents false data 3. Type Safety: Fully mypy --strict compliant 4. Backward Compatibility: JSON output always includes both timestamps via <code>updated_at_or_created</code> 5. Flexible Access: Direct field for null-checking, property for guaranteed non-null value</p>"},{"location":"development/adr-timestamp-handling/#implementation-details","title":"Implementation Details","text":""},{"location":"development/adr-timestamp-handling/#1-pydantic-model","title":"1. Pydantic Model","text":"<p>File: <code>src/echomine/models/conversation.py</code></p> <pre><code>from datetime import UTC, datetime\nfrom typing import Optional\nfrom pydantic import BaseModel, Field, field_validator\n\nclass Conversation(BaseModel):\n    created_at: datetime = Field(\n        ...,\n        description=\"Conversation creation timestamp (timezone-aware UTC, REQUIRED)\",\n    )\n    updated_at: Optional[datetime] = Field(\n        default=None,\n        description=\"Last modification timestamp (timezone-aware UTC, None if never updated)\",\n    )\n\n    @field_validator(\"created_at\")\n    @classmethod\n    def validate_created_at_timezone_aware(cls, v: datetime) -&gt; datetime:\n        \"\"\"Ensure created_at is timezone-aware and normalized to UTC.\"\"\"\n        if v.tzinfo is None or v.tzinfo.utcoffset(v) is None:\n            msg = f\"created_at must be timezone-aware: {v}\"\n            raise ValueError(msg)\n        return v.astimezone(UTC)\n\n    @field_validator(\"updated_at\")\n    @classmethod\n    def validate_updated_at_timezone_aware(cls, v: Optional[datetime], info: Any) -&gt; Optional[datetime]:\n        \"\"\"Ensure updated_at is timezone-aware and &gt;= created_at (if provided).\"\"\"\n        if v is None:\n            return None\n\n        if v.tzinfo is None or v.tzinfo.utcoffset(v) is None:\n            msg = f\"updated_at must be timezone-aware: {v}\"\n            raise ValueError(msg)\n\n        v_utc = v.astimezone(UTC)\n\n        # Validate updated_at &gt;= created_at\n        created_at = info.data.get(\"created_at\")\n        if created_at and v_utc &lt; created_at:\n            msg = f\"updated_at ({v_utc}) must be &gt;= created_at ({created_at})\"\n            raise ValueError(msg)\n\n        return v_utc\n\n    @property\n    def updated_at_or_created(self) -&gt; datetime:\n        \"\"\"Get last update timestamp, falling back to created_at if not set.\"\"\"\n        return self.updated_at if self.updated_at is not None else self.created_at\n</code></pre>"},{"location":"development/adr-timestamp-handling/#2-adapter-validation","title":"2. Adapter Validation","text":"<p>File: <code>src/echomine/adapters/openai.py</code></p> <pre><code>from pydantic import ValidationError as PydanticValidationError\n\n# Validate created_at is present (REQUIRED)\nif create_time is None:\n    raise PydanticValidationError.from_exception_data(\n        \"Conversation\",\n        [\n            {\n                \"type\": \"missing\",\n                \"loc\": (\"create_time\",),\n                \"input\": raw_data,\n            }\n        ],\n    )\n\ncreated_at = datetime.fromtimestamp(float(create_time), tz=UTC)\n\n# Handle optional updated_at\nupdated_at: Optional[datetime] = (\n    datetime.fromtimestamp(float(update_time), tz=UTC)\n    if update_time is not None\n    else None\n)\n</code></pre> <p>Validation Strategy: - <code>created_at</code>: REQUIRED - if null, raise ValidationError (malformed data) - <code>updated_at</code>: OPTIONAL - if null, set to None (semantically valid)</p>"},{"location":"development/adr-timestamp-handling/#3-json-serialization","title":"3. JSON Serialization","text":"<p>File: <code>src/echomine/cli/formatters.py</code></p> <pre><code>def format_json(conversations: list[Conversation]) -&gt; str:\n    \"\"\"Format conversations as JSON with guaranteed timestamps.\"\"\"\n    conv_dicts = []\n    for conv in conversations:\n        conv_dict = {\n            \"id\": conv.id,\n            \"title\": conv.title,\n            \"created_at\": conv.created_at.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n            \"updated_at\": conv.updated_at_or_created.strftime(\"%Y-%m-%dT%H:%M:%S\"),  # Never null\n            \"message_count\": conv.message_count,\n        }\n        conv_dicts.append(conv_dict)\n\n    return json.dumps(conv_dicts, separators=(',', ':'), ensure_ascii=False) + \"\\n\"\n</code></pre> <p>JSON Output Schema: <pre><code>{\n  \"id\": \"conv-123\",\n  \"title\": \"Python best practices\",\n  \"created_at\": \"2024-03-15T14:23:11\",\n  \"updated_at\": \"2024-03-15T14:23:11\",\n  \"message_count\": 47\n}\n</code></pre></p> <p>Key Points: - JSON consumers ALWAYS get both timestamps (never null) - If conversation never updated, <code>created_at == updated_at</code> (semantically correct) - No need for null handling in downstream JSON processors</p>"},{"location":"development/adr-timestamp-handling/#usage-patterns","title":"Usage Patterns","text":""},{"location":"development/adr-timestamp-handling/#pattern-1-display-timestamps","title":"Pattern 1: Display Timestamps","text":"<pre><code># For display (CLI table, JSON output)\nprint(f\"Last modified: {conv.updated_at_or_created}\")\n\n# Guaranteed non-null, mypy --strict compliant\n</code></pre>"},{"location":"development/adr-timestamp-handling/#pattern-2-distinguish-never-updated","title":"Pattern 2: Distinguish \"Never Updated\"","text":"<pre><code># When you need to know if conversation was EVER updated\nif conv.updated_at is None:\n    print(\"Conversation never modified since creation\")\nelse:\n    print(f\"Last updated: {conv.updated_at}\")\n</code></pre>"},{"location":"development/adr-timestamp-handling/#pattern-3-date-range-filtering","title":"Pattern 3: Date Range Filtering","text":"<pre><code># Search filtering uses created_at (required field)\nif query.from_date is not None:\n    if conv.created_at.date() &lt; query.from_date:\n        continue  # Skip conversation outside date range\n</code></pre>"},{"location":"development/adr-timestamp-handling/#pattern-4-sorting-by-modification-time","title":"Pattern 4: Sorting by Modification Time","text":"<pre><code># Sort by \"last modified\" (with fallback)\nconversations.sort(key=lambda c: c.updated_at_or_created, reverse=True)\n</code></pre>"},{"location":"development/adr-timestamp-handling/#type-safety-guarantees","title":"Type Safety Guarantees","text":""},{"location":"development/adr-timestamp-handling/#mypy-strict-compliance","title":"mypy --strict Compliance","text":"<p>All code passes <code>mypy --strict</code> without warnings:</p> <pre><code>$ mypy --strict src/echomine/models/conversation.py\nSuccess: no issues found in 1 source file\n\n$ mypy --strict src/echomine/adapters/openai.py\nSuccess: no issues found in 1 source file\n\n$ mypy --strict src/echomine/cli/formatters.py\nSuccess: no issues found in 1 source file\n</code></pre>"},{"location":"development/adr-timestamp-handling/#type-narrowing","title":"Type Narrowing","text":"<pre><code># Direct field access (Optional[datetime])\ntimestamp: Optional[datetime] = conv.updated_at\nif timestamp is not None:\n    # Type narrowed to datetime (non-null)\n    print(timestamp.isoformat())\n\n# Property access (datetime, never None)\ntimestamp: datetime = conv.updated_at_or_created\nprint(timestamp.isoformat())  # No null check needed\n</code></pre>"},{"location":"development/adr-timestamp-handling/#impact-analysis","title":"Impact Analysis","text":""},{"location":"development/adr-timestamp-handling/#downstream-components","title":"Downstream Components","text":"Component Impact Mitigation CLI Formatters Must use <code>updated_at_or_created</code> \u2705 Updated in formatters.py JSON Output Must never output null \u2705 Uses <code>updated_at_or_created</code> Search Filtering Date ranges use <code>created_at</code> \u2705 No changes needed BM25 Scoring Not timestamp-dependent \u2705 No changes needed Tree Navigation Not timestamp-dependent \u2705 No changes needed"},{"location":"development/adr-timestamp-handling/#api-stability","title":"API Stability","text":"<p>Breaking Change: NO - JSON output schema unchanged (both timestamps always present) - Search API unchanged (filters on <code>created_at</code>) - CLI output unchanged (uses fallback property)</p> <p>Internal Change: YES - <code>Conversation.updated_at</code> is now <code>Optional[datetime]</code> - New property <code>updated_at_or_created</code> for guaranteed access - Adapters must handle null <code>update_time</code> explicitly</p>"},{"location":"development/adr-timestamp-handling/#testing","title":"Testing","text":""},{"location":"development/adr-timestamp-handling/#unit-tests","title":"Unit Tests","text":"<pre><code>from datetime import UTC, datetime, timedelta\nfrom echomine.models.conversation import Conversation\nfrom echomine.models.message import Message\n\ndef test_conversation_never_updated():\n    \"\"\"Test conversation with None updated_at.\"\"\"\n    created = datetime(2024, 3, 15, 14, 30, 0, tzinfo=UTC)\n\n    msg = Message(\n        id=\"msg-1\",\n        content=\"Hello\",\n        role=\"user\",\n        timestamp=created,\n        parent_id=None,\n    )\n\n    conv = Conversation(\n        id=\"conv-001\",\n        title=\"Test\",\n        created_at=created,\n        updated_at=None,  # Never updated\n        messages=[msg],\n    )\n\n    assert conv.updated_at is None\n    assert conv.updated_at_or_created == created\n\ndef test_conversation_with_updates():\n    \"\"\"Test conversation with explicit updated_at.\"\"\"\n    created = datetime(2024, 3, 15, 14, 30, 0, tzinfo=UTC)\n    updated = created + timedelta(hours=2)\n\n    msg = Message(\n        id=\"msg-1\",\n        content=\"Hello\",\n        role=\"user\",\n        timestamp=created,\n        parent_id=None,\n    )\n\n    conv = Conversation(\n        id=\"conv-002\",\n        title=\"Test\",\n        created_at=created,\n        updated_at=updated,\n        messages=[msg],\n    )\n\n    assert conv.updated_at == updated\n    assert conv.updated_at_or_created == updated\n</code></pre>"},{"location":"development/adr-timestamp-handling/#integration-tests","title":"Integration Tests","text":"<p>All existing integration tests pass without modifications: - <code>tests/integration/test_list_flow.py</code> (10/10 passed) - <code>tests/integration/test_search_flow.py</code> (16/16 passed)</p>"},{"location":"development/adr-timestamp-handling/#migration-guide","title":"Migration Guide","text":""},{"location":"development/adr-timestamp-handling/#for-existing-code","title":"For Existing Code","text":"<p>If you were using <code>conv.updated_at</code> for display: <pre><code># Before\nprint(f\"Updated: {conv.updated_at}\")\n\n# After (handles None gracefully)\nprint(f\"Updated: {conv.updated_at_or_created}\")\n</code></pre></p> <p>If you need to distinguish null: <pre><code># Before (always had a value)\nprint(f\"Updated: {conv.updated_at}\")\n\n# After (check for None)\nif conv.updated_at is not None:\n    print(f\"Updated: {conv.updated_at}\")\nelse:\n    print(\"Never updated\")\n</code></pre></p>"},{"location":"development/adr-timestamp-handling/#for-new-code","title":"For New Code","text":"<p>Recommended pattern: <pre><code># For display/sorting (guaranteed non-null)\ntimestamp = conv.updated_at_or_created\n\n# For business logic (check if updated)\nif conv.updated_at is not None:\n    # Handle updated case\n    pass\nelse:\n    # Handle never-updated case\n    pass\n</code></pre></p>"},{"location":"development/adr-timestamp-handling/#references","title":"References","text":""},{"location":"development/adr-timestamp-handling/#related-files","title":"Related Files","text":"<ul> <li><code>/Users/omarcontreras/PycharmProjects/echomine/src/echomine/models/conversation.py</code></li> <li><code>/Users/omarcontreras/PycharmProjects/echomine/src/echomine/adapters/openai.py</code></li> <li><code>/Users/omarcontreras/PycharmProjects/echomine/src/echomine/cli/formatters.py</code></li> <li><code>/Users/omarcontreras/PycharmProjects/echomine/examples/timestamp_handling.py</code></li> </ul>"},{"location":"development/adr-timestamp-handling/#constitution-principles","title":"Constitution Principles","text":"<ul> <li>Principle VI: Strict typing with mypy --strict compliance</li> <li>Principle I: Library-first (data models are pure, reusable)</li> <li>FR-222, FR-227: Immutability via frozen=True</li> <li>FR-244, FR-245, FR-246: Timezone-aware timestamps</li> <li>FR-273: updated_at &gt;= created_at validation</li> <li>FR-301-306: JSON output schema</li> </ul>"},{"location":"development/adr-timestamp-handling/#external-resources","title":"External Resources","text":"<ul> <li>Pydantic Field Validators</li> <li>Python datetime Best Practices</li> <li>mypy Optional Type Handling</li> </ul>"},{"location":"development/documentation/","title":"Documentation Guide","text":"<p>This guide covers writing documentation for Echomine, including docstrings, user guides, and API reference.</p>"},{"location":"development/documentation/#documentation-types","title":"Documentation Types","text":""},{"location":"development/documentation/#1-api-reference-auto-generated","title":"1. API Reference (Auto-Generated)","text":"<p>Generated automatically from Python docstrings using mkdocstrings:</p> <ul> <li>Source: Docstrings in <code>src/echomine/</code></li> <li>Output: <code>docs/api/</code> pages</li> <li>Updates: Automatic when code changes</li> </ul>"},{"location":"development/documentation/#2-user-guides-manual","title":"2. User Guides (Manual)","text":"<p>Written manually in Markdown:</p> <ul> <li>Source: <code>docs/*.md</code> files</li> <li>Examples: quickstart.md, cli-usage.md, library-usage.md</li> <li>Updates: Manual when features change</li> </ul>"},{"location":"development/documentation/#3-developer-docs-manual","title":"3. Developer Docs (Manual)","text":"<p>This section and related guides:</p> <ul> <li>Source: <code>docs/development/</code>, <code>docs/maintaining/</code></li> <li>Updates: Manual when processes change</li> </ul>"},{"location":"development/documentation/#docstring-format-google-style","title":"Docstring Format (Google Style)","text":"<p>Echomine uses Google-style docstrings for consistency with mkdocstrings.</p>"},{"location":"development/documentation/#function-docstring","title":"Function Docstring","text":"<pre><code>def search(\n    self,\n    file_path: Path,\n    query: SearchQuery,\n    *,\n    progress_callback: Optional[Callable[[int], None]] = None,\n) -&gt; Iterator[SearchResult[Conversation]]:\n    \"\"\"Search conversations matching query criteria with BM25 ranking.\n\n    Streams through the export file and returns matching conversations\n    ranked by relevance score. Uses O(1) memory regardless of file size.\n\n    Args:\n        file_path: Path to OpenAI export JSON file.\n        query: Search parameters including keywords, date filters, and limit.\n        progress_callback: Optional callback invoked with count of processed\n            conversations. Called every 100 items or 100ms.\n\n    Yields:\n        SearchResult containing matched conversation and relevance score (0.0-1.0).\n        Results are yielded in descending order by score.\n\n    Raises:\n        FileNotFoundError: If file_path does not exist.\n        PermissionError: If file is not readable.\n        ParseError: If export format is invalid or unsupported.\n\n    Example:\n        ```python\n        from echomine import OpenAIAdapter, SearchQuery\n        from pathlib import Path\n\n        adapter = OpenAIAdapter()\n        query = SearchQuery(keywords=[\"python\", \"algorithm\"], limit=10)\n\n        for result in adapter.search(Path(\"export.json\"), query):\n            print(f\"{result.score:.2f}: {result.conversation.title}\")\n        ```\n\n    Note:\n        The search uses BM25 ranking algorithm. Title-only searches\n        (no keywords) skip full-text ranking for better performance.\n\n    See Also:\n        - `stream_conversations`: For unfiltered iteration\n        - `get_conversation_by_id`: For direct ID lookup\n    \"\"\"\n</code></pre>"},{"location":"development/documentation/#class-docstring","title":"Class Docstring","text":"<pre><code>class OpenAIAdapter:\n    \"\"\"Adapter for parsing and searching OpenAI ChatGPT exports.\n\n    Implements the ConversationProvider protocol for ChatGPT export files.\n    Stateless design allows reuse across multiple files.\n\n    The adapter handles:\n        - Streaming conversations with O(1) memory usage\n        - BM25-ranked keyword search\n        - Date range and title filtering\n        - Graceful handling of malformed entries\n\n    Attributes:\n        None. This adapter is stateless by design.\n\n    Example:\n        ```python\n        from echomine import OpenAIAdapter\n        from pathlib import Path\n\n        adapter = OpenAIAdapter()\n\n        # List all conversations\n        for conv in adapter.stream_conversations(Path(\"export.json\")):\n            print(conv.title)\n\n        # Search with keywords\n        from echomine import SearchQuery\n        query = SearchQuery(keywords=[\"python\"])\n        for result in adapter.search(Path(\"export.json\"), query):\n            print(f\"{result.score:.2f}: {result.conversation.title}\")\n        ```\n\n    Note:\n        OpenAI exports use a specific JSON schema. This adapter supports\n        the current export format as of 2024. Future format changes may\n        require adapter updates.\n\n    See Also:\n        - `ConversationProvider`: The protocol this adapter implements\n        - `Conversation`: The data model returned by this adapter\n    \"\"\"\n</code></pre>"},{"location":"development/documentation/#property-docstring","title":"Property Docstring","text":"<pre><code>@property\ndef message_count(self) -&gt; int:\n    \"\"\"Total number of messages across all branches.\n\n    Counts all messages in the conversation tree, including\n    messages in alternate branches.\n\n    Returns:\n        Non-negative integer count of messages.\n\n    Example:\n        ```python\n        conv = adapter.get_conversation_by_id(path, conv_id)\n        print(f\"Conversation has {conv.message_count} messages\")\n        ```\n    \"\"\"\n    return len(self.messages)\n</code></pre>"},{"location":"development/documentation/#module-docstring","title":"Module Docstring","text":"<pre><code>\"\"\"OpenAI ChatGPT export adapter.\n\nThis module provides the OpenAIAdapter class for parsing and searching\nChatGPT conversation exports.\n\nExample:\n    ```python\n    from echomine.adapters.openai import OpenAIAdapter\n\n    adapter = OpenAIAdapter()\n    for conv in adapter.stream_conversations(Path(\"export.json\")):\n        print(conv.title)\n    ```\n\nTypical usage:\n    1. Create adapter instance (stateless, reusable)\n    2. Call stream_conversations() for listing\n    3. Call search() for filtered/ranked results\n    4. Call get_conversation_by_id() for specific lookup\n\nSee Also:\n    - `echomine.protocols.ConversationProvider`: Protocol definition\n    - `echomine.models.Conversation`: Data model\n\"\"\"\n</code></pre>"},{"location":"development/documentation/#mkdocs-configuration","title":"MkDocs Configuration","text":""},{"location":"development/documentation/#mkdocsyml-structure","title":"mkdocs.yml Structure","text":"<pre><code>site_name: Echomine\nnav:\n  - Home: index.md\n  - Getting Started:\n    - Installation: installation.md\n    - Quickstart: quickstart.md\n  - User Guide:\n    - CLI Usage: cli-usage.md\n    - Library Usage: library-usage.md\n  - API Reference:\n    - api/index.md\n    - Models:\n      - Conversation: api/models/conversation.md\n      - Message: api/models/message.md\n    - Adapters:\n      - OpenAI: api/adapters/openai.md\n  - Development:\n    - development/index.md\n    - Setup: development/setup.md\n    - Testing: development/testing.md\n\ntheme:\n  name: material\n  features:\n    - content.code.copy\n    - navigation.sections\n\nplugins:\n  - search\n  - mkdocstrings:\n      handlers:\n        python:\n          options:\n            docstring_style: google\n            show_source: true\n</code></pre>"},{"location":"development/documentation/#api-reference-pages","title":"API Reference Pages","text":"<p>Create stub pages that pull from docstrings:</p> <pre><code>&lt;!-- docs/api/adapters/openai.md --&gt;\n# OpenAI Adapter\n\n::: echomine.adapters.openai.OpenAIAdapter\n    options:\n      show_root_heading: true\n      show_source: true\n      members:\n        - stream_conversations\n        - search\n        - get_conversation_by_id\n</code></pre>"},{"location":"development/documentation/#writing-user-guides","title":"Writing User Guides","text":""},{"location":"development/documentation/#structure","title":"Structure","text":"<ol> <li>Start with \"why\": What problem does this solve?</li> <li>Quick example: Working code immediately</li> <li>Detailed explanation: Step-by-step guide</li> <li>Advanced usage: Edge cases, options</li> <li>Troubleshooting: Common issues</li> </ol>"},{"location":"development/documentation/#example-page-structure","title":"Example Page Structure","text":"<pre><code># Feature Name\n\nBrief description of what this feature does and why you'd use it.\n\n## Quick Start\n\n\\`\\`\\`python\n# Minimal working example\nfrom echomine import OpenAIAdapter\nadapter = OpenAIAdapter()\n\\`\\`\\`\n\n## Basic Usage\n\n### Step 1: Setup\n\nExplanation...\n\n### Step 2: Execute\n\nExplanation...\n\n## Options\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| limit | int | 10 | Maximum results |\n\n## Examples\n\n### Example: Search by Keywords\n\n\\`\\`\\`python\n# Full example with context\n\\`\\`\\`\n\n### Example: Filter by Date\n\n\\`\\`\\`python\n# Another example\n\\`\\`\\`\n\n## Troubleshooting\n\n### Error: FileNotFoundError\n\n**Cause**: File path doesn't exist.\n\n**Solution**: Verify the path...\n\n## See Also\n\n- [Related Feature](other-page.md)\n- [API Reference](api/module.md)\n</code></pre>"},{"location":"development/documentation/#building-documentation","title":"Building Documentation","text":""},{"location":"development/documentation/#local-development","title":"Local Development","text":"<pre><code># Install docs dependencies\npip install -e \".[dev]\"\n\n# Serve with live reload\nmkdocs serve\n# View at http://127.0.0.1:8000\n\n# Build static site\nmkdocs build\n# Output in site/ directory\n</code></pre>"},{"location":"development/documentation/#checking-for-issues","title":"Checking for Issues","text":"<pre><code># Build with strict mode (fails on warnings)\nmkdocs build --strict\n\n# Common warnings:\n# - Broken links\n# - Missing pages in nav\n# - Invalid markdown\n</code></pre>"},{"location":"development/documentation/#deployment","title":"Deployment","text":"<p>Documentation deploys automatically via GitHub Actions when pushed to main:</p> <pre><code>git add docs/\ngit commit -m \"docs: update user guide\"\ngit push origin main\n# GitHub Actions runs mkdocs gh-deploy\n</code></pre>"},{"location":"development/documentation/#style-guidelines","title":"Style Guidelines","text":""},{"location":"development/documentation/#do","title":"Do","text":"<ul> <li>Use active voice (\"Returns the count\" not \"The count is returned\")</li> <li>Include working code examples</li> <li>Document all public APIs</li> <li>Keep examples minimal but complete</li> <li>Link to related documentation</li> </ul>"},{"location":"development/documentation/#dont","title":"Don't","text":"<ul> <li>Document private methods (prefix with <code>_</code>)</li> <li>Repeat type information in description</li> <li>Use jargon without explanation</li> <li>Leave placeholder TODOs in docs</li> </ul>"},{"location":"development/documentation/#code-examples","title":"Code Examples","text":"<pre><code># Good: Complete, runnable example\nfrom echomine import OpenAIAdapter, SearchQuery\nfrom pathlib import Path\n\nadapter = OpenAIAdapter()\nquery = SearchQuery(keywords=[\"python\"], limit=5)\nresults = list(adapter.search(Path(\"export.json\"), query))\nprint(f\"Found {len(results)} results\")\n\n# Bad: Incomplete, won't run\nresults = adapter.search(path, query)  # What's adapter? path? query?\n</code></pre>"},{"location":"development/documentation/#updating-documentation","title":"Updating Documentation","text":""},{"location":"development/documentation/#when-to-update","title":"When to Update","text":"<ul> <li>New features: Add to user guide + API reference</li> <li>Changed behavior: Update affected pages</li> <li>Bug fixes: Update if behavior was documented incorrectly</li> <li>Breaking changes: Add migration guide</li> </ul>"},{"location":"development/documentation/#checklist","title":"Checklist","text":"<ul> <li> Docstrings updated for changed functions</li> <li> User guide reflects new behavior</li> <li> Examples still work</li> <li> Links not broken</li> <li> <code>mkdocs build --strict</code> passes</li> </ul>"},{"location":"development/documentation/#next-steps","title":"Next Steps","text":"<ul> <li>Testing Guide: Writing tests</li> <li>Type Checking: Type hints</li> <li>Contributing: Full contribution guide</li> </ul> <p>Last Updated: 2025-11-30</p>"},{"location":"development/setup/","title":"Development Setup Guide","text":"<p>This guide walks you through setting up a development environment for Echomine.</p>"},{"location":"development/setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+: Required for modern type hints (PEP 695)</li> <li>Git: For version control</li> <li>pyenv (recommended): For managing Python versions</li> </ul>"},{"location":"development/setup/#verify-python-version","title":"Verify Python Version","text":"<pre><code>python --version\n# Should output: Python 3.12.x or higher\n</code></pre> <p>If you need to install Python 3.12+:</p> <pre><code># Using pyenv (recommended)\npyenv install 3.12.2\npyenv local 3.12.2\n\n# Or using homebrew (macOS)\nbrew install python@3.12\n</code></pre>"},{"location":"development/setup/#installation","title":"Installation","text":""},{"location":"development/setup/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/aucontraire/echomine.git\ncd echomine\n</code></pre>"},{"location":"development/setup/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<pre><code># Using venv (standard library)\npython -m venv .venv\nsource .venv/bin/activate  # Linux/macOS\n# .venv\\Scripts\\activate   # Windows\n\n# Or using pyenv-virtualenv\npyenv virtualenv 3.12.2 echomine\npyenv activate echomine\n</code></pre>"},{"location":"development/setup/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code># Install in development mode with all dev dependencies\npip install -e \".[dev]\"\n</code></pre> <p>This installs: - Core dependencies (pydantic, ijson, typer, rich, structlog) - Development tools (pytest, mypy, ruff, pre-commit) - Documentation tools (mkdocs, mkdocstrings)</p>"},{"location":"development/setup/#4-install-pre-commit-hooks","title":"4. Install Pre-Commit Hooks","text":"<pre><code>pre-commit install\n</code></pre> <p>This enables automatic quality checks on every commit: - mypy --strict type checking - ruff linting and formatting - pytest test execution</p>"},{"location":"development/setup/#5-verify-installation","title":"5. Verify Installation","text":"<pre><code># Run tests\npytest --cov=echomine\n\n# Check type safety\nmypy --strict src/echomine/\n\n# Check linting\nruff check src/ tests/\n\n# Run CLI\nechomine --help\n</code></pre> <p>All commands should complete without errors.</p>"},{"location":"development/setup/#ide-setup","title":"IDE Setup","text":""},{"location":"development/setup/#vs-code-recommended","title":"VS Code (Recommended)","text":"<p>Install these extensions: - Python (ms-python.python) - Pylance (ms-python.vscode-pylance) - Ruff (charliermarsh.ruff)</p> <p>Add to <code>.vscode/settings.json</code>:</p> <pre><code>{\n    \"python.defaultInterpreterPath\": \".venv/bin/python\",\n    \"python.analysis.typeCheckingMode\": \"strict\",\n    \"editor.formatOnSave\": true,\n    \"[python]\": {\n        \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n        \"editor.codeActionsOnSave\": {\n            \"source.fixAll.ruff\": \"explicit\",\n            \"source.organizeImports.ruff\": \"explicit\"\n        }\n    },\n    \"ruff.lint.args\": [\"--config=pyproject.toml\"]\n}\n</code></pre>"},{"location":"development/setup/#pycharm","title":"PyCharm","text":"<ol> <li>Set Python Interpreter: File \u2192 Settings \u2192 Project \u2192 Python Interpreter \u2192 Select <code>.venv</code></li> <li>Enable Type Checking: File \u2192 Settings \u2192 Editor \u2192 Inspections \u2192 Python \u2192 Type checker compatibility</li> <li>Configure Ruff: File \u2192 Settings \u2192 Tools \u2192 External Tools \u2192 Add ruff</li> </ol>"},{"location":"development/setup/#project-structure","title":"Project Structure","text":"<pre><code>echomine/\n\u251c\u2500\u2500 src/echomine/           # Library source code\n\u2502   \u251c\u2500\u2500 models/             # Pydantic data models\n\u2502   \u251c\u2500\u2500 protocols/          # Protocol definitions\n\u2502   \u251c\u2500\u2500 adapters/           # Provider implementations\n\u2502   \u251c\u2500\u2500 search/             # Search and ranking\n\u2502   \u251c\u2500\u2500 utils/              # Utilities\n\u2502   \u2514\u2500\u2500 cli/                # CLI commands\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 unit/               # Fast, isolated tests (70%)\n\u2502   \u251c\u2500\u2500 integration/        # Component interaction (20%)\n\u2502   \u251c\u2500\u2500 contract/           # Protocol compliance (5%)\n\u2502   \u251c\u2500\u2500 performance/        # Benchmarks (5%)\n\u2502   \u2514\u2500\u2500 fixtures/           # Test data\n\u251c\u2500\u2500 docs/                   # Documentation (MkDocs)\n\u251c\u2500\u2500 specs/                  # Feature specifications\n\u2514\u2500\u2500 pyproject.toml          # Project configuration\n</code></pre>"},{"location":"development/setup/#common-issues","title":"Common Issues","text":""},{"location":"development/setup/#module-not-found-errors","title":"\"Module not found\" errors","text":"<p>Ensure you're in the virtual environment: <pre><code>source .venv/bin/activate\npip install -e \".[dev]\"\n</code></pre></p>"},{"location":"development/setup/#mypy-errors-after-installation","title":"mypy errors after installation","text":"<p>Clear the mypy cache: <pre><code>rm -rf .mypy_cache\nmypy --strict src/echomine/\n</code></pre></p>"},{"location":"development/setup/#pre-commit-hook-failures","title":"Pre-commit hook failures","text":"<p>Run hooks manually to see detailed output: <pre><code>pre-commit run --all-files\n</code></pre></p>"},{"location":"development/setup/#tests-failing-with-fixture-errors","title":"Tests failing with fixture errors","text":"<p>Ensure test fixtures exist: <pre><code>ls tests/fixtures/sample_export.json\n</code></pre></p> <p>If missing, the fixture is generated automatically by pytest.</p>"},{"location":"development/setup/#next-steps","title":"Next Steps","text":"<ul> <li>Development Workflows: Common development patterns</li> <li>Testing Guide: Writing and running tests</li> <li>Type Checking: mypy --strict patterns</li> <li>Contributing: Contribution guidelines</li> </ul> <p>Last Updated: 2025-11-30</p>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>Echomine follows strict Test-Driven Development (TDD) practices. This guide covers how to write, organize, and run tests.</p>"},{"location":"development/testing/#tdd-workflow-mandatory","title":"TDD Workflow (Mandatory)","text":"<p>All development must follow the RED-GREEN-REFACTOR cycle:</p> <pre><code>graph LR\n    A[RED: Write Failing Test] --&gt; B[Verify Test Fails]\n    B --&gt; C[GREEN: Minimal Code to Pass]\n    C --&gt; D[Verify Test Passes]\n    D --&gt; E[REFACTOR: Improve Code]\n    E --&gt; F{More Tests?}\n    F --&gt;|Yes| A\n    F --&gt;|No| G[Commit]\n</code></pre>"},{"location":"development/testing/#1-red-phase","title":"1. RED Phase","text":"<p>Write a test that describes the expected behavior. Run it to verify it fails.</p> <pre><code>def test_search_returns_matching_conversations():\n    \"\"\"Test that search finds conversations containing keywords.\"\"\"\n    adapter = OpenAIAdapter()\n    query = SearchQuery(keywords=[\"python\", \"algorithm\"])\n\n    results = list(adapter.search(SAMPLE_EXPORT, query))\n\n    assert len(results) &gt; 0\n    assert all(\"python\" in r.conversation.title.lower() or\n               \"algorithm\" in r.conversation.title.lower()\n               for r in results)\n</code></pre> <p>Run and confirm failure: <pre><code>pytest tests/unit/test_search.py::test_search_returns_matching_conversations -v\n# Should FAIL because feature isn't implemented\n</code></pre></p>"},{"location":"development/testing/#2-green-phase","title":"2. GREEN Phase","text":"<p>Write the minimum code to make the test pass. Don't over-engineer.</p>"},{"location":"development/testing/#3-refactor-phase","title":"3. REFACTOR Phase","text":"<p>Improve the code while keeping tests green. Run tests after each change.</p>"},{"location":"development/testing/#test-organization","title":"Test Organization","text":""},{"location":"development/testing/#test-pyramid","title":"Test Pyramid","text":"<pre><code>        /\\\n       /  \\      Contract Tests (5%)\n      /----\\     - Protocol compliance\n     /      \\    - FR validation\n    /--------\\   Integration Tests (20%)\n   /          \\  - Component interaction\n  /------------\\ Unit Tests (70%)\n /              \\- Fast, isolated, focused\n/________________\\\n</code></pre>"},{"location":"development/testing/#directory-structure","title":"Directory Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/                    # Fast, isolated tests\n\u2502   \u251c\u2500\u2500 test_conversation.py # Model tests\n\u2502   \u251c\u2500\u2500 test_message.py\n\u2502   \u251c\u2500\u2500 test_search.py\n\u2502   \u2514\u2500\u2500 adapters/\n\u2502       \u2514\u2500\u2500 test_openai.py\n\u251c\u2500\u2500 integration/             # Component interaction\n\u2502   \u251c\u2500\u2500 test_cli_integration.py\n\u2502   \u2514\u2500\u2500 test_search_pipeline.py\n\u251c\u2500\u2500 contract/                # Protocol/spec compliance\n\u2502   \u251c\u2500\u2500 test_fr_search.py    # Functional requirements\n\u2502   \u2514\u2500\u2500 test_provider_protocol.py\n\u251c\u2500\u2500 performance/             # Benchmarks\n\u2502   \u2514\u2500\u2500 test_search_performance.py\n\u251c\u2500\u2500 fixtures/                # Test data\n\u2502   \u251c\u2500\u2500 sample_export.json\n\u2502   \u2514\u2500\u2500 conftest.py          # Shared fixtures\n\u2514\u2500\u2500 conftest.py              # Root fixtures\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#all-tests","title":"All Tests","text":"<pre><code># Run all tests with coverage\npytest --cov=echomine --cov-report=term-missing\n\n# Run with verbose output\npytest -v\n\n# Run with extra verbose (show print statements)\npytest -vvs\n</code></pre>"},{"location":"development/testing/#specific-test-types","title":"Specific Test Types","text":"<pre><code># Unit tests only (fast)\npytest tests/unit/ -v\n\n# Integration tests\npytest tests/integration/ -v\n\n# Contract tests\npytest tests/contract/ -v\n\n# Performance benchmarks\npytest tests/performance/ --benchmark-only\n</code></pre>"},{"location":"development/testing/#specific-tests","title":"Specific Tests","text":"<pre><code># Single file\npytest tests/unit/test_search.py -v\n\n# Single test function\npytest tests/unit/test_search.py::test_search_with_keywords -v\n\n# Tests matching pattern\npytest -k \"search\" -v\n\n# Tests with specific marker\npytest -m \"slow\" -v\npytest -m \"not slow\" -v\n</code></pre>"},{"location":"development/testing/#watch-mode","title":"Watch Mode","text":"<pre><code># Auto-run tests on file changes (requires pytest-watch)\npip install pytest-watch\nptw -- tests/unit/\n</code></pre>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"development/testing/#test-naming","title":"Test Naming","text":"<p>Use descriptive names that explain the expected behavior:</p> <pre><code># Good: Describes behavior\ndef test_search_returns_empty_list_when_no_matches():\n    pass\n\ndef test_search_ranks_results_by_relevance_score():\n    pass\n\ndef test_stream_conversations_skips_malformed_entries():\n    pass\n\n# Bad: Vague names\ndef test_search():\n    pass\n\ndef test_it_works():\n    pass\n</code></pre>"},{"location":"development/testing/#test-structure-arrange-act-assert","title":"Test Structure (Arrange-Act-Assert)","text":"<pre><code>def test_search_filters_by_date_range():\n    # Arrange: Set up test data and dependencies\n    adapter = OpenAIAdapter()\n    query = SearchQuery(\n        keywords=[\"python\"],\n        from_date=datetime(2024, 1, 1, tzinfo=UTC),\n        to_date=datetime(2024, 6, 30, tzinfo=UTC),\n    )\n\n    # Act: Execute the code under test\n    results = list(adapter.search(SAMPLE_EXPORT, query))\n\n    # Assert: Verify expected behavior\n    assert len(results) &gt; 0\n    for result in results:\n        assert result.conversation.created_at &gt;= query.from_date\n        assert result.conversation.created_at &lt;= query.to_date\n</code></pre>"},{"location":"development/testing/#using-fixtures","title":"Using Fixtures","text":"<pre><code># conftest.py\nimport pytest\nfrom pathlib import Path\n\n@pytest.fixture\ndef sample_export() -&gt; Path:\n    \"\"\"Path to sample export file.\"\"\"\n    return Path(__file__).parent / \"fixtures\" / \"sample_export.json\"\n\n@pytest.fixture\ndef adapter() -&gt; OpenAIAdapter:\n    \"\"\"Fresh adapter instance.\"\"\"\n    return OpenAIAdapter()\n\n# test_search.py\ndef test_search_with_keywords(adapter, sample_export):\n    query = SearchQuery(keywords=[\"python\"])\n    results = list(adapter.search(sample_export, query))\n    assert len(results) &gt; 0\n</code></pre>"},{"location":"development/testing/#parametrized-tests","title":"Parametrized Tests","text":"<pre><code>import pytest\n\n@pytest.mark.parametrize(\"keywords,expected_min_results\", [\n    ([\"python\"], 3),\n    ([\"algorithm\"], 2),\n    ([\"nonexistent\"], 0),\n])\ndef test_search_keyword_variations(adapter, sample_export, keywords, expected_min_results):\n    query = SearchQuery(keywords=keywords)\n    results = list(adapter.search(sample_export, query))\n    assert len(results) &gt;= expected_min_results\n</code></pre>"},{"location":"development/testing/#testing-exceptions","title":"Testing Exceptions","text":"<pre><code>import pytest\n\ndef test_search_raises_on_nonexistent_file(adapter):\n    query = SearchQuery(keywords=[\"test\"])\n\n    with pytest.raises(FileNotFoundError):\n        list(adapter.search(Path(\"/nonexistent/file.json\"), query))\n\ndef test_validation_error_on_invalid_query():\n    with pytest.raises(ValidationError) as exc_info:\n        SearchQuery(limit=-1)  # Invalid: must be positive\n\n    assert \"limit\" in str(exc_info.value)\n</code></pre>"},{"location":"development/testing/#mocking","title":"Mocking","text":"<pre><code>from unittest.mock import Mock, patch\n\ndef test_progress_callback_invoked(adapter, sample_export):\n    callback = Mock()\n    query = SearchQuery(keywords=[\"python\"])\n\n    list(adapter.search(sample_export, query, progress_callback=callback))\n\n    assert callback.call_count &gt; 0\n\n@patch(\"echomine.adapters.openai.adapter.ijson\")\ndef test_handles_parse_error(mock_ijson, adapter, sample_export):\n    mock_ijson.items.side_effect = ijson.JSONError(\"Parse error\")\n\n    with pytest.raises(ParseError):\n        list(adapter.stream_conversations(sample_export))\n</code></pre>"},{"location":"development/testing/#test-coverage","title":"Test Coverage","text":""},{"location":"development/testing/#running-coverage","title":"Running Coverage","text":"<pre><code># Terminal report\npytest --cov=echomine --cov-report=term-missing\n\n# HTML report (detailed)\npytest --cov=echomine --cov-report=html\nopen htmlcov/index.html\n\n# XML report (for CI)\npytest --cov=echomine --cov-report=xml\n</code></pre>"},{"location":"development/testing/#coverage-requirements","title":"Coverage Requirements","text":"<ul> <li>Critical paths: &gt;80% coverage required</li> <li>Overall: &gt;70% coverage target</li> <li>New code: 100% coverage for new features</li> </ul>"},{"location":"development/testing/#excluding-from-coverage","title":"Excluding from Coverage","text":"<pre><code># pragma: no cover - for unreachable code\ndef _debug_helper():  # pragma: no cover\n    \"\"\"Only used in development.\"\"\"\n    pass\n</code></pre>"},{"location":"development/testing/#performance-testing","title":"Performance Testing","text":""},{"location":"development/testing/#using-pytest-benchmark","title":"Using pytest-benchmark","text":"<pre><code>def test_search_performance(benchmark, adapter, large_export):\n    \"\"\"Search 10K conversations completes in &lt;5 seconds.\"\"\"\n    query = SearchQuery(keywords=[\"python\"], limit=100)\n\n    result = benchmark(lambda: list(adapter.search(large_export, query)))\n\n    # Verify result correctness\n    assert len(result) &lt;= 100\n\n    # Performance assertion\n    assert benchmark.stats.stats.mean &lt; 5.0  # seconds\n\ndef test_stream_memory_efficiency(benchmark, adapter, large_export):\n    \"\"\"Streaming uses O(1) memory.\"\"\"\n    def consume_stream():\n        count = 0\n        for _ in adapter.stream_conversations(large_export):\n            count += 1\n        return count\n\n    result = benchmark(consume_stream)\n    assert result &gt; 0\n</code></pre>"},{"location":"development/testing/#running-benchmarks","title":"Running Benchmarks","text":"<pre><code># Run benchmarks\npytest tests/performance/ --benchmark-only\n\n# Compare with baseline\npytest tests/performance/ --benchmark-compare\n\n# Save baseline\npytest tests/performance/ --benchmark-save=baseline\n</code></pre>"},{"location":"development/testing/#test-markers","title":"Test Markers","text":"<pre><code># Mark slow tests\n@pytest.mark.slow\ndef test_large_file_processing():\n    pass\n\n# Mark integration tests\n@pytest.mark.integration\ndef test_cli_search_command():\n    pass\n\n# Skip conditionally\n@pytest.mark.skipif(sys.platform == \"win32\", reason=\"Unix-only test\")\ndef test_unix_permissions():\n    pass\n</code></pre> <p>Run specific markers: <pre><code>pytest -m \"not slow\"  # Skip slow tests\npytest -m \"integration\"  # Only integration\n</code></pre></p>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":""},{"location":"development/testing/#do","title":"Do","text":"<ul> <li>Write tests FIRST (TDD)</li> <li>Use descriptive test names</li> <li>Test one behavior per test</li> <li>Use fixtures for shared setup</li> <li>Test edge cases and error conditions</li> <li>Keep tests fast (&lt;1s per unit test)</li> </ul>"},{"location":"development/testing/#dont","title":"Don't","text":"<ul> <li>Write tests after implementation</li> <li>Test implementation details (test behavior)</li> <li>Use sleep() in tests (use mocks)</li> <li>Share state between tests</li> <li>Ignore flaky tests (fix them)</li> </ul>"},{"location":"development/testing/#debugging-tests","title":"Debugging Tests","text":"<pre><code># Show print output\npytest -vvs tests/unit/test_search.py\n\n# Drop into debugger on failure\npytest --pdb tests/unit/test_search.py\n\n# Run specific failing test\npytest tests/unit/test_search.py::test_search_with_keywords -vvs\n</code></pre>"},{"location":"development/testing/#next-steps","title":"Next Steps","text":"<ul> <li>Type Checking: mypy --strict patterns</li> <li>Development Workflows: Common patterns</li> <li>Contributing: Contribution guidelines</li> </ul> <p>Last Updated: 2025-11-30</p>"},{"location":"development/type-checking/","title":"Type Checking Guide","text":"<p>Echomine enforces mypy --strict with zero tolerance for errors. This guide covers type hint patterns and common issues.</p>"},{"location":"development/type-checking/#core-principle","title":"Core Principle","text":"<p>ZERO TOLERANCE: mypy --strict MUST pass with no errors before any commit.</p> <p>This is a non-negotiable Constitution Principle (VI). Type safety catches bugs at development time, not runtime.</p>"},{"location":"development/type-checking/#running-mypy","title":"Running mypy","text":"<pre><code># Check all source code (required before commit)\nmypy --strict src/echomine/\n\n# Check tests too\nmypy --strict src/echomine/ tests/\n\n# Quick incremental check\nmypy src/echomine/\n\n# Verbose output for debugging\nmypy --strict --show-error-codes --show-error-context src/echomine/\n</code></pre>"},{"location":"development/type-checking/#type-hint-basics","title":"Type Hint Basics","text":""},{"location":"development/type-checking/#function-signatures","title":"Function Signatures","text":"<pre><code># All parameters and return types must be annotated\ndef search(\n    file_path: Path,\n    query: SearchQuery,\n    *,\n    limit: int = 10,\n) -&gt; Iterator[SearchResult[Conversation]]:\n    \"\"\"Search conversations.\"\"\"\n    pass\n\n# Use Optional for nullable parameters\ndef get_conversation(\n    file_path: Path,\n    conversation_id: str,\n) -&gt; Optional[Conversation]:\n    \"\"\"Returns None if not found.\"\"\"\n    pass\n</code></pre>"},{"location":"development/type-checking/#variables","title":"Variables","text":"<pre><code># Explicit annotation when type isn't obvious\nresults: list[SearchResult[Conversation]] = []\ncount: int = 0\nfound: bool = False\n\n# Not needed when type is obvious from assignment\nname = \"test\"  # str is inferred\nadapter = OpenAIAdapter()  # Type is inferred\n</code></pre>"},{"location":"development/type-checking/#collections","title":"Collections","text":"<pre><code>from typing import Dict, List, Set, Tuple  # Deprecated in 3.9+\n\n# Use built-in generics (Python 3.9+)\nnames: list[str] = []\nscores: dict[str, float] = {}\nunique_ids: set[str] = set()\npair: tuple[str, int] = (\"id\", 42)\n\n# For variable-length tuples\nargs: tuple[str, ...] = (\"a\", \"b\", \"c\")\n</code></pre>"},{"location":"development/type-checking/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"development/type-checking/#generics-with-typevar","title":"Generics with TypeVar","text":"<pre><code>from typing import TypeVar, Generic, Iterator\n\nConversationT = TypeVar(\"ConversationT\", bound=\"Conversation\")\n\nclass SearchResult(Generic[ConversationT]):\n    \"\"\"Generic search result.\"\"\"\n    conversation: ConversationT\n    score: float\n\ndef search(query: SearchQuery) -&gt; Iterator[SearchResult[Conversation]]:\n    pass\n</code></pre>"},{"location":"development/type-checking/#protocol-classes","title":"Protocol Classes","text":"<p>Use Protocol instead of ABC for duck typing:</p> <pre><code>from typing import Protocol, Iterator, runtime_checkable\n\n@runtime_checkable\nclass ConversationProvider(Protocol[ConversationT]):\n    \"\"\"Protocol for conversation providers.\"\"\"\n\n    def stream_conversations(\n        self,\n        file_path: Path,\n    ) -&gt; Iterator[ConversationT]:\n        \"\"\"Stream conversations from file.\"\"\"\n        ...\n\n    def search(\n        self,\n        file_path: Path,\n        query: SearchQuery,\n    ) -&gt; Iterator[SearchResult[ConversationT]]:\n        \"\"\"Search with ranking.\"\"\"\n        ...\n\n# Implementation doesn't need to inherit\nclass OpenAIAdapter:\n    \"\"\"Implements ConversationProvider implicitly.\"\"\"\n\n    def stream_conversations(self, file_path: Path) -&gt; Iterator[Conversation]:\n        pass\n\n    def search(self, file_path: Path, query: SearchQuery) -&gt; Iterator[SearchResult[Conversation]]:\n        pass\n</code></pre>"},{"location":"development/type-checking/#callable-types","title":"Callable Types","text":"<pre><code>from typing import Callable, Optional\n\n# Function that takes int, returns None\nProgressCallback = Callable[[int], None]\n\ndef search(\n    file_path: Path,\n    query: SearchQuery,\n    *,\n    progress_callback: Optional[ProgressCallback] = None,\n) -&gt; Iterator[SearchResult[Conversation]]:\n    if progress_callback:\n        progress_callback(count)\n</code></pre>"},{"location":"development/type-checking/#union-types","title":"Union Types","text":"<pre><code>from typing import Union\n\n# Python 3.10+ syntax\ndef process(value: str | int) -&gt; str:\n    return str(value)\n\n# Pre-3.10 syntax\ndef process(value: Union[str, int]) -&gt; str:\n    return str(value)\n</code></pre>"},{"location":"development/type-checking/#literal-types","title":"Literal Types","text":"<pre><code>from typing import Literal\n\nRole = Literal[\"user\", \"assistant\", \"system\"]\n\ndef create_message(role: Role, content: str) -&gt; Message:\n    pass\n\n# Only these values are allowed\ncreate_message(\"user\", \"Hello\")      # OK\ncreate_message(\"admin\", \"Hello\")     # mypy error!\n</code></pre>"},{"location":"development/type-checking/#pydantic-model-patterns","title":"Pydantic Model Patterns","text":""},{"location":"development/type-checking/#basic-model","title":"Basic Model","text":"<pre><code>from pydantic import BaseModel, ConfigDict, Field\n\nclass Message(BaseModel):\n    model_config = ConfigDict(\n        frozen=True,\n        strict=True,\n        extra=\"forbid\",\n    )\n\n    id: str = Field(..., min_length=1)\n    content: str\n    role: Literal[\"user\", \"assistant\", \"system\"]\n    timestamp: datetime\n</code></pre>"},{"location":"development/type-checking/#optional-fields-critical","title":"Optional Fields (CRITICAL)","text":"<pre><code>from typing import Optional\nfrom pydantic import Field\n\n# CORRECT: Use Optional when data can be null\nclass Conversation(BaseModel):\n    title: str\n    updated_at: Optional[datetime] = Field(default=None)\n\n# WRONG: Don't hide nullable with defaults (violates Data Integrity)\nclass Conversation(BaseModel):\n    updated_at: datetime = Field(default_factory=datetime.now)  # NO!\n</code></pre>"},{"location":"development/type-checking/#field-with-default-mypy-strict-requirement","title":"Field with Default (mypy --strict requirement)","text":"<pre><code># CORRECT: Explicit default= keyword\nclass SearchQuery(BaseModel):\n    keywords: Optional[list[str]] = Field(default=None)\n    limit: int = Field(default=10, gt=0)\n\n# WRONG: Positional defaults (fails mypy --strict)\nclass SearchQuery(BaseModel):\n    keywords: Optional[list[str]] = Field(None)  # Ambiguous!\n    limit: int = Field(10, gt=0)  # Ambiguous!\n</code></pre>"},{"location":"development/type-checking/#common-errors-and-fixes","title":"Common Errors and Fixes","text":""},{"location":"development/type-checking/#error-missing-return-type","title":"Error: Missing return type","text":"<pre><code># Error: Function is missing a return type annotation\ndef get_count():\n    return 42\n\n# Fix: Add return type\ndef get_count() -&gt; int:\n    return 42\n</code></pre>"},{"location":"development/type-checking/#error-incompatible-return-type","title":"Error: Incompatible return type","text":"<pre><code># Error: Incompatible return value type (got \"None\", expected \"str\")\ndef get_name() -&gt; str:\n    if condition:\n        return \"name\"\n    # Missing return!\n\n# Fix: Return in all branches or use Optional\ndef get_name() -&gt; Optional[str]:\n    if condition:\n        return \"name\"\n    return None\n</code></pre>"},{"location":"development/type-checking/#error-no-return-statement","title":"Error: No return statement","text":"<pre><code># Error: Missing return statement\ndef process(items: list[str]) -&gt; list[str]:\n    for item in items:\n        pass  # Forgot to return!\n\n# Fix: Add return\ndef process(items: list[str]) -&gt; list[str]:\n    result = []\n    for item in items:\n        result.append(item.upper())\n    return result\n</code></pre>"},{"location":"development/type-checking/#error-cannot-infer-type","title":"Error: Cannot infer type","text":"<pre><code># Error: Need type annotation for \"items\"\nitems = []\nitems.append(\"test\")\n\n# Fix: Explicit annotation\nitems: list[str] = []\nitems.append(\"test\")\n</code></pre>"},{"location":"development/type-checking/#error-incompatible-types-in-assignment","title":"Error: Incompatible types in assignment","text":"<pre><code># Error: Incompatible types in assignment\ncount: int = \"10\"  # str != int\n\n# Fix: Use correct type\ncount: int = 10\n</code></pre>"},{"location":"development/type-checking/#error-x-has-no-attribute-y","title":"Error: \"X\" has no attribute \"Y\"","text":"<pre><code># Error: \"Optional[str]\" has no attribute \"upper\"\ndef process(name: Optional[str]) -&gt; str:\n    return name.upper()  # name might be None!\n\n# Fix: Handle None case\ndef process(name: Optional[str]) -&gt; str:\n    if name is None:\n        return \"\"\n    return name.upper()\n\n# Or use assert for guaranteed non-None\ndef process(name: Optional[str]) -&gt; str:\n    assert name is not None\n    return name.upper()\n</code></pre>"},{"location":"development/type-checking/#error-iterator-vs-list","title":"Error: Iterator vs List","text":"<pre><code># Error: Incompatible return value type (got \"list\", expected \"Iterator\")\ndef stream() -&gt; Iterator[int]:\n    return [1, 2, 3]  # list != Iterator\n\n# Fix: Use yield (generator)\ndef stream() -&gt; Iterator[int]:\n    yield 1\n    yield 2\n    yield 3\n\n# Or convert with iter()\ndef stream() -&gt; Iterator[int]:\n    return iter([1, 2, 3])\n</code></pre>"},{"location":"development/type-checking/#type-narrowing","title":"Type Narrowing","text":"<pre><code>from typing import Optional, Union\n\ndef process(value: Optional[str]) -&gt; str:\n    # Type narrowing with if\n    if value is None:\n        return \"default\"\n    # mypy knows value is str here\n    return value.upper()\n\ndef handle(item: Union[str, int]) -&gt; str:\n    # Type narrowing with isinstance\n    if isinstance(item, str):\n        return item.upper()\n    # mypy knows item is int here\n    return str(item)\n</code></pre>"},{"location":"development/type-checking/#ignoring-errors-last-resort","title":"Ignoring Errors (Last Resort)","text":"<p>Only use when absolutely necessary and document why:</p> <pre><code># type: ignore[error-code] - Use specific error code\nresult = external_lib.call()  # type: ignore[no-untyped-call]\n\n# Document the reason\n# mypy doesn't understand this third-party library's types\nresult = weird_api.fetch()  # type: ignore[arg-type]\n</code></pre> <p>Rules for ignoring: 1. Only for third-party libraries with bad type stubs 2. Always use specific error code 3. Add comment explaining why 4. Never ignore errors in your own code</p>"},{"location":"development/type-checking/#configuration","title":"Configuration","text":"<p>pyproject.toml settings:</p> <pre><code>[tool.mypy]\npython_version = \"3.12\"\nstrict = true\nwarn_return_any = true\nwarn_unused_ignores = true\ndisallow_untyped_defs = true\ndisallow_incomplete_defs = true\ncheck_untyped_defs = true\ndisallow_untyped_decorators = true\nno_implicit_optional = true\nwarn_redundant_casts = true\nwarn_unused_configs = true\n\n[[tool.mypy.overrides]]\nmodule = \"tests.*\"\ndisallow_untyped_defs = false  # Less strict for tests\n</code></pre>"},{"location":"development/type-checking/#pre-commit-integration","title":"Pre-Commit Integration","text":"<p>mypy runs automatically on commit:</p> <pre><code># .pre-commit-config.yaml\n- repo: https://github.com/pre-commit/mirrors-mypy\n  rev: v1.8.0\n  hooks:\n    - id: mypy\n      args: [--strict, --config-file=pyproject.toml]\n      additional_dependencies:\n        - pydantic&gt;=2.0\n        - types-all\n</code></pre>"},{"location":"development/type-checking/#debugging-type-issues","title":"Debugging Type Issues","text":"<pre><code># Show what mypy infers\nmypy --strict --show-error-codes src/echomine/\n\n# Reveal inferred types\nfrom typing import reveal_type\nreveal_type(variable)  # mypy will print the inferred type\n\n# Check specific file\nmypy --strict src/echomine/adapters/openai/adapter.py\n</code></pre>"},{"location":"development/type-checking/#next-steps","title":"Next Steps","text":"<ul> <li>Testing Guide: TDD practices</li> <li>Documentation: Writing docstrings</li> <li>Architecture: Design patterns</li> </ul> <p>Last Updated: 2025-11-30</p>"},{"location":"development/workflows/","title":"Development Workflows","text":"<p>This guide covers common development workflows for echomine contributors.</p>"},{"location":"development/workflows/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Adding a New Feature</li> <li>Fixing a Bug</li> <li>Updating Dependencies</li> <li>Updating Documentation</li> <li>Performance Optimization</li> </ul>"},{"location":"development/workflows/#adding-a-new-feature","title":"Adding a New Feature","text":""},{"location":"development/workflows/#workflow-overview","title":"Workflow Overview","text":"<ol> <li>Understand the Requirement</li> <li>Write Failing Tests (RED)</li> <li>Implement Feature (GREEN)</li> <li>Refactor (REFACTOR)</li> <li>Update Documentation</li> <li>Submit Pull Request</li> </ol>"},{"location":"development/workflows/#detailed-steps","title":"Detailed Steps","text":""},{"location":"development/workflows/#1-understand-the-requirement","title":"1. Understand the Requirement","text":"<pre><code># Read the feature specification\ncat specs/001-ai-chat-parser/spec.md | grep \"FR-XXX\"\n\n# Check related issues\ngh issue view &lt;issue-number&gt;\n\n# Create feature branch\ngit checkout -b feature/descriptive-name\n</code></pre>"},{"location":"development/workflows/#2-write-failing-tests-red","title":"2. Write Failing Tests (RED)","text":"<p>Always write tests FIRST. This is non-negotiable.</p> <pre><code># tests/unit/test_new_feature.py\nimport pytest\nfrom echomine.models import NewModel\n\n\ndef test_new_feature_basic_functionality() -&gt; None:\n    \"\"\"NewModel should validate required fields.\"\"\"\n    from pydantic import ValidationError\n\n    # This test should FAIL initially (RED phase)\n    with pytest.raises(ValidationError):\n        NewModel()  # Missing required fields\n\n\ndef test_new_feature_with_valid_data() -&gt; None:\n    \"\"\"NewModel should accept valid data.\"\"\"\n    # This test should also FAIL (feature doesn't exist yet)\n    model = NewModel(field1=\"value1\", field2=\"value2\")\n    assert model.field1 == \"value1\"\n    assert model.field2 == \"value2\"\n</code></pre> <p>Verify tests fail: <pre><code>pytest tests/unit/test_new_feature.py -v\n# EXPECTED: FAILED (tests are RED)\n</code></pre></p>"},{"location":"development/workflows/#3-implement-feature-green","title":"3. Implement Feature (GREEN)","text":"<p>Write minimal code to make tests pass:</p> <pre><code># src/echomine/models/new_model.py\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass NewModel(BaseModel):\n    \"\"\"New feature model.\"\"\"\n\n    model_config = ConfigDict(\n        frozen=True,\n        strict=True,\n        extra=\"forbid\",\n    )\n\n    field1: str = Field(..., description=\"First field\")\n    field2: str = Field(..., description=\"Second field\")\n</code></pre> <p>Verify tests pass: <pre><code>pytest tests/unit/test_new_feature.py -v\n# EXPECTED: PASSED (tests are GREEN)\n</code></pre></p>"},{"location":"development/workflows/#4-refactor-refactor","title":"4. Refactor (REFACTOR)","text":"<p>Improve code quality while keeping tests green:</p> <pre><code># Add validation, better error messages, helper methods\nfrom pydantic import field_validator\n\n\nclass NewModel(BaseModel):\n    \"\"\"New feature model with enhanced validation.\"\"\"\n\n    model_config = ConfigDict(\n        frozen=True,\n        strict=True,\n        extra=\"forbid\",\n    )\n\n    field1: str = Field(..., min_length=1, description=\"First field\")\n    field2: str = Field(..., min_length=1, description=\"Second field\")\n\n    @field_validator(\"field1\")\n    @classmethod\n    def validate_field1(cls, v: str) -&gt; str:\n        if not v.strip():\n            raise ValueError(\"field1 cannot be empty or whitespace\")\n        return v.strip()\n\n    @property\n    def display_name(self) -&gt; str:\n        \"\"\"Human-readable display name.\"\"\"\n        return f\"{self.field1} - {self.field2}\"\n</code></pre> <p>Verify tests still pass: <pre><code>pytest tests/unit/test_new_feature.py -v\n# EXPECTED: Still PASSED\n</code></pre></p>"},{"location":"development/workflows/#5-run-quality-checks","title":"5. Run Quality Checks","text":"<pre><code># Type checking (must pass)\nmypy --strict src/echomine/\n\n# Linting and formatting\nruff check --fix src/ tests/\nruff format src/ tests/\n\n# Full test suite with coverage\npytest --cov=echomine --cov-report=term-missing\n\n# Pre-commit hooks\npre-commit run --all-files\n</code></pre>"},{"location":"development/workflows/#6-update-documentation","title":"6. Update Documentation","text":"<pre><code># Add comprehensive docstrings (Google style)\nclass NewModel(BaseModel):\n    \"\"\"New feature model with validation.\n\n    This model provides [description of what it does and why it's useful].\n\n    Attributes:\n        field1: Description of field1\n        field2: Description of field2\n\n    Example:\n        ```python\n        from echomine.models import NewModel\n\n        model = NewModel(field1=\"value1\", field2=\"value2\")\n        print(model.display_name)  # \"value1 - value2\"\n        ```\n\n    Raises:\n        ValidationError: If fields are invalid\n    \"\"\"\n</code></pre> <p>Update user-facing docs: <pre><code># If public API changed, update library-usage.md\nvim docs/library-usage.md\n\n# If CLI changed, update cli-usage.md\nvim docs/cli-usage.md\n\n# Update CHANGELOG.md\nvim CHANGELOG.md\n</code></pre></p>"},{"location":"development/workflows/#7-commit-and-push","title":"7. Commit and Push","text":"<pre><code># Stage changes\ngit add .\n\n# Commit with conventional commit message\ngit commit -m \"feat(models): add NewModel with validation\n\nImplements FR-XXX for [feature description].\n\n- Validates field1 and field2 with Pydantic\n- Provides display_name property\n- Includes comprehensive unit tests (100% coverage)\n- Adds docstrings with examples\"\n\n# Push to remote\ngit push origin feature/descriptive-name\n</code></pre>"},{"location":"development/workflows/#8-create-pull-request","title":"8. Create Pull Request","text":"<pre><code># Using GitHub CLI\ngh pr create --title \"feat(models): add NewModel with validation\" \\\n  --body \"## Summary\nImplements FR-XXX for [feature description].\n\n## Changes\n- Added NewModel to src/echomine/models/\n- Added unit tests with 100% coverage\n- Updated documentation\n\n## Related Issues\n- Closes #123\n- Implements FR-XXX\n\n## Testing\n- [x] Unit tests added\n- [x] Type checking passes\n- [x] Documentation updated\"\n</code></pre>"},{"location":"development/workflows/#fixing-a-bug","title":"Fixing a Bug","text":""},{"location":"development/workflows/#workflow-overview_1","title":"Workflow Overview","text":"<ol> <li>Reproduce the Bug</li> <li>Write Failing Test (RED)</li> <li>Fix the Bug (GREEN)</li> <li>Verify Fix</li> <li>Submit Pull Request</li> </ol>"},{"location":"development/workflows/#detailed-steps_1","title":"Detailed Steps","text":""},{"location":"development/workflows/#1-reproduce-the-bug","title":"1. Reproduce the Bug","text":"<pre><code># Create bug fix branch\ngit checkout -b fix/issue-123-description\n\n# Try to reproduce bug\npython -c \"from echomine import ...; # trigger bug\"\n</code></pre> <p>Document reproduction steps in issue or comments.</p>"},{"location":"development/workflows/#2-write-failing-test-red","title":"2. Write Failing Test (RED)","text":"<p>CRITICAL: Write a test that reproduces the bug FIRST.</p> <pre><code># tests/unit/test_bug_fix.py\ndef test_bug_123_conversation_with_null_timestamp() -&gt; None:\n    \"\"\"Bug: Parser crashes on null timestamp (issue #123).\"\"\"\n    from echomine.adapters.openai import OpenAIAdapter\n    from pathlib import Path\n\n    # Create fixture with null timestamp\n    fixture = Path(\"tests/fixtures/null_timestamp.json\")\n\n    adapter = OpenAIAdapter()\n    conversations = list(adapter.stream_conversations(fixture))\n\n    # This should NOT raise an error\n    # Currently fails with: AttributeError: 'NoneType' object has no attribute 'isoformat'\n    assert len(conversations) &gt; 0\n</code></pre> <p>Verify test fails: <pre><code>pytest tests/unit/test_bug_fix.py::test_bug_123_conversation_with_null_timestamp -v\n# EXPECTED: FAILED (reproduces bug)\n</code></pre></p>"},{"location":"development/workflows/#3-fix-the-bug-green","title":"3. Fix the Bug (GREEN)","text":"<pre><code># src/echomine/models/conversation.py\nfrom datetime import datetime\nfrom typing import Optional\n\nclass Conversation(BaseModel):\n    # Before (buggy):\n    # updated_at: datetime  # Crashes if null\n\n    # After (fixed):\n    updated_at: Optional[datetime] = Field(\n        default=None,\n        description=\"Last update time, null if never updated\"\n    )\n\n    @property\n    def updated_at_or_created(self) -&gt; datetime:\n        \"\"\"Fallback to created_at if updated_at is null.\"\"\"\n        return self.updated_at if self.updated_at is not None else self.created_at\n</code></pre> <p>Verify test passes: <pre><code>pytest tests/unit/test_bug_fix.py::test_bug_123_conversation_with_null_timestamp -v\n# EXPECTED: PASSED (bug fixed)\n</code></pre></p>"},{"location":"development/workflows/#4-run-full-test-suite","title":"4. Run Full Test Suite","text":"<pre><code># Ensure fix doesn't break existing tests\npytest --cov=echomine\n\n# Type checking\nmypy --strict src/echomine/\n\n# Pre-commit\npre-commit run --all-files\n</code></pre>"},{"location":"development/workflows/#5-update-changelog","title":"5. Update CHANGELOG","text":"<pre><code>vim CHANGELOG.md\n</code></pre> <pre><code>## [Unreleased]\n\n### Fixed\n- Fixed crash when parsing conversations with null updated_at timestamp (#123)\n</code></pre>"},{"location":"development/workflows/#6-commit-and-create-pr","title":"6. Commit and Create PR","text":"<pre><code>git add .\ngit commit -m \"fix(parser): handle null timestamp in conversations\n\nFixes #123 where parser crashed on conversations with null updated_at.\n\n- Made updated_at Optional[datetime] instead of datetime\n- Added updated_at_or_created property for fallback\n- Added regression test with null timestamp fixture\"\n\ngh pr create --title \"fix(parser): handle null timestamp in conversations\"\n</code></pre>"},{"location":"development/workflows/#updating-dependencies","title":"Updating Dependencies","text":""},{"location":"development/workflows/#workflow","title":"Workflow","text":""},{"location":"development/workflows/#security-updates-immediate","title":"Security Updates (Immediate)","text":"<pre><code># Check for security vulnerabilities\npip-audit\n\n# If vulnerability found:\n# 1. Update pyproject.toml\nvim pyproject.toml\n# Change: pydantic&gt;=2.6.0,&lt;3.0.0\n# To: pydantic&gt;=2.6.1,&lt;3.0.0  (patched version)\n\n# 2. Install updated version\npip install --upgrade pydantic\n\n# 3. Run full test suite\npytest --cov=echomine\nmypy --strict src/echomine/\n\n# 4. Commit and release patch version\ngit add pyproject.toml\ngit commit -m \"chore(deps): update pydantic to 2.6.1 (security fix)\"\n</code></pre>"},{"location":"development/workflows/#minor-updates-monthly","title":"Minor Updates (Monthly)","text":"<pre><code># Check outdated packages\npip list --outdated\n\n# Update in isolated environment\npython -m venv test-env\nsource test-env/bin/activate\npip install -e \".[dev]\"\npip install --upgrade &lt;package&gt;\n\n# Run tests\npytest --cov=echomine\nmypy --strict src/echomine/\n\n# If all pass, update pyproject.toml\ndeactivate\n</code></pre>"},{"location":"development/workflows/#major-updates-evaluate-breaking-changes","title":"Major Updates (Evaluate Breaking Changes)","text":"<pre><code># Example: Pydantic v2 \u2192 v3 (hypothetical)\n\n# 1. Read migration guide\nopen https://docs.pydantic.dev/latest/migration/\n\n# 2. Create branch\ngit checkout -b chore/upgrade-pydantic-v3\n\n# 3. Update in isolated env, fix breaking changes\n# 4. Run full test suite\n# 5. Update docs if API changed\n# 6. Bump MAJOR version of echomine (1.x \u2192 2.x)\n</code></pre>"},{"location":"development/workflows/#updating-documentation","title":"Updating Documentation","text":""},{"location":"development/workflows/#when-to-update-docs","title":"When to Update Docs","text":"<ul> <li>Public API changes (new classes, functions, parameters)</li> <li>CLI command changes</li> <li>Breaking changes (migration guides)</li> <li>Bug fixes affecting documented behavior</li> <li>Performance improvements</li> </ul>"},{"location":"development/workflows/#workflow_1","title":"Workflow","text":""},{"location":"development/workflows/#1-update-docstrings-code","title":"1. Update Docstrings (Code)","text":"<pre><code># src/echomine/adapters/openai.py\ndef new_method(self, param: str) -&gt; Iterator[Result]:\n    \"\"\"New method description.\n\n    Args:\n        param: Description of parameter\n\n    Yields:\n        Result objects with [description]\n\n    Raises:\n        ValueError: If param is invalid\n\n    Example:\n        ```python\n        adapter = OpenAIAdapter()\n        for result in adapter.new_method(\"value\"):\n            print(result)\n        ```\n    \"\"\"\n</code></pre>"},{"location":"development/workflows/#2-update-user-guides-markdown","title":"2. Update User Guides (Markdown)","text":"<pre><code># Library API changes\nvim docs/library-usage.md\n\n# CLI changes\nvim docs/cli-usage.md\n\n# New features\nvim docs/quickstart.md\n</code></pre>"},{"location":"development/workflows/#3-build-and-preview-docs","title":"3. Build and Preview Docs","text":"<pre><code># Build locally\nmkdocs build\n\n# Serve with live reload\nmkdocs serve\n# Visit http://127.0.0.1:8000\n\n# Check for broken links, formatting issues\n</code></pre>"},{"location":"development/workflows/#4-update-changelog","title":"4. Update CHANGELOG","text":"<pre><code>vim CHANGELOG.md\n</code></pre> <pre><code>## [Unreleased]\n\n### Added\n- New `new_method` in OpenAIAdapter for [purpose]\n\n### Documentation\n- Updated library-usage.md with new_method examples\n- Added migration guide for breaking changes\n</code></pre>"},{"location":"development/workflows/#5-deploy-maintainers-only","title":"5. Deploy (Maintainers Only)","text":"<pre><code># Deploy to GitHub Pages\nmkdocs gh-deploy\n</code></pre>"},{"location":"development/workflows/#performance-optimization","title":"Performance Optimization","text":""},{"location":"development/workflows/#workflow_2","title":"Workflow","text":""},{"location":"development/workflows/#1-identify-performance-issue","title":"1. Identify Performance Issue","text":"<pre><code># Run performance benchmarks\npytest tests/performance/ --benchmark-only\n\n# Profile code\npython -m cProfile -o profile.stats -m echomine.cli search large_export.json\npython -c \"import pstats; p = pstats.Stats('profile.stats'); p.sort_stats('cumulative'); p.print_stats(20)\"\n</code></pre>"},{"location":"development/workflows/#2-write-performance-test-red","title":"2. Write Performance Test (RED)","text":"<pre><code># tests/performance/test_optimization.py\ndef test_search_performance_under_30s(benchmark, large_export):\n    \"\"\"Search must complete in &lt;30s for 1.6GB file.\"\"\"\n    from echomine.adapters.openai import OpenAIAdapter\n    from echomine.models.search import SearchQuery\n\n    adapter = OpenAIAdapter()\n    query = SearchQuery(keywords=[\"python\"], limit=10)\n\n    def search():\n        return list(adapter.search(large_export, query))\n\n    result = benchmark(search)\n    assert benchmark.stats[\"mean\"] &lt; 30.0  # Currently FAILS at 45s\n</code></pre>"},{"location":"development/workflows/#3-optimize-green","title":"3. Optimize (GREEN)","text":"<pre><code># Before (slow):\ndef search(self, file_path: Path, query: SearchQuery):\n    conversations = list(self.stream_conversations(file_path))  # Loads all into memory!\n    for conv in conversations:\n        # Score and rank\n\n# After (fast):\ndef search(self, file_path: Path, query: SearchQuery):\n    for conv in self.stream_conversations(file_path):  # Streaming!\n        score = self._calculate_score(conv, query)\n        if score &gt; 0:\n            yield SearchResult(conversation=conv, score=score)\n</code></pre>"},{"location":"development/workflows/#4-verify-performance-improvement","title":"4. Verify Performance Improvement","text":"<pre><code>pytest tests/performance/test_optimization.py --benchmark-only\n# EXPECTED: Now passes (&lt;30s)\n\n# Compare before/after\npytest tests/performance/ --benchmark-compare=baseline\n</code></pre>"},{"location":"development/workflows/#5-document-performance-change","title":"5. Document Performance Change","text":"<pre><code>vim CHANGELOG.md\n</code></pre> <pre><code>## [Unreleased]\n\n### Performance\n- Improved search performance by 50% (45s \u2192 22s for 1.6GB files) via streaming optimization\n</code></pre>"},{"location":"development/workflows/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"development/workflows/#writing-code-before-tests","title":"\u274c Writing Code Before Tests","text":"<pre><code># WRONG: Implementation first\ndef new_feature():\n    return \"implementation\"\n\n# Then later...\ndef test_new_feature():\n    assert new_feature() == \"implementation\"\n</code></pre> <p>Why wrong? You don't know if test actually validates behavior.</p>"},{"location":"development/workflows/#tests-first-tdd","title":"\u2705 Tests First (TDD)","text":"<pre><code># CORRECT: Test first (RED)\ndef test_new_feature():\n    result = new_feature()  # Doesn't exist yet - test FAILS\n    assert result == \"expected\"\n\n# Then implement (GREEN)\ndef new_feature():\n    return \"expected\"  # Minimal code to pass\n</code></pre>"},{"location":"development/workflows/#skipping-mypy-strict","title":"\u274c Skipping mypy --strict","text":"<pre><code># WRONG: Ignoring type errors\nmypy src/echomine/  # Has errors\n# \"I'll fix them later\" \u2190 Never happens\n</code></pre>"},{"location":"development/workflows/#fix-type-errors-immediately","title":"\u2705 Fix Type Errors Immediately","text":"<pre><code># CORRECT: Fix before committing\nmypy --strict src/echomine/  # Must be zero errors\n</code></pre>"},{"location":"development/workflows/#quick-reference","title":"Quick Reference","text":""},{"location":"development/workflows/#feature-development","title":"Feature Development","text":"<pre><code>git checkout -b feature/name\n# Write failing test \u2192 Implement \u2192 Refactor \u2192 Quality checks \u2192 Commit\n</code></pre>"},{"location":"development/workflows/#bug-fix","title":"Bug Fix","text":"<pre><code>git checkout -b fix/issue-123\n# Reproduce \u2192 Write failing test \u2192 Fix \u2192 Verify \u2192 Commit\n</code></pre>"},{"location":"development/workflows/#dependency-update","title":"Dependency Update","text":"<pre><code>pip list --outdated\n# Update pyproject.toml \u2192 Test \u2192 Commit\n</code></pre>"},{"location":"development/workflows/#documentation-update","title":"Documentation Update","text":"<pre><code># Update docstrings and markdown\nmkdocs serve  # Preview\ngit commit -m \"docs: update ...\"\n</code></pre> <p>Last Updated: 2025-11-28</p>"},{"location":"maintaining/","title":"Maintaining Echomine","text":"<p>This section contains guides for project maintainers. For contributor guidelines, see Contributing.</p>"},{"location":"maintaining/#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Release Process: Step-by-step release workflow</li> <li>Versioning: Semantic versioning policy</li> <li>PyPI Publishing: Publishing to PyPI workflow</li> </ul> <p>For the complete maintainer guide, see MAINTAINING.md in the repository root.</p>"},{"location":"maintaining/#overview","title":"Overview","text":"<p>Maintainers are responsible for:</p> <ul> <li>Releases: Planning, executing, and documenting releases</li> <li>Quality: Ensuring code quality standards are met</li> <li>Community: Reviewing PRs, triaging issues, supporting contributors</li> <li>Security: Responding to security issues, updating dependencies</li> <li>Documentation: Keeping docs accurate and up to date</li> </ul>"},{"location":"maintaining/#maintainer-quick-reference","title":"Maintainer Quick Reference","text":""},{"location":"maintaining/#release-workflow","title":"Release Workflow","text":"<pre><code># 1. Update version in pyproject.toml\n# 2. Update CHANGELOG.md\n# 3. Run quality checks\npytest --cov=echomine &amp;&amp; mypy --strict src/echomine/\n\n# 4. Build distribution\npython -m build\n\n# 5. Test on TestPyPI\npython -m twine upload --repository testpypi dist/*\n\n# 6. Publish to PyPI\npython -m twine upload dist/*\n\n# 7. Tag release\ngit tag -a v1.1.0 -m \"Release v1.1.0\" &amp;&amp; git push origin v1.1.0\n\n# 8. Create GitHub release\ngh release create v1.1.0 --title \"v1.1.0\" --notes-file RELEASE_NOTES.md\n</code></pre>"},{"location":"maintaining/#issue-triage","title":"Issue Triage","text":"<p>Labels: - Type: <code>bug</code>, <code>feature</code>, <code>documentation</code>, <code>question</code> - Priority: <code>P0-critical</code>, <code>P1-high</code>, <code>P2-medium</code>, <code>P3-low</code> - Status: <code>needs-triage</code>, <code>blocked</code>, <code>ready</code>, <code>in-progress</code> - Area: <code>cli</code>, <code>library</code>, <code>search</code>, <code>adapters</code>, <code>tests</code> - Meta: <code>good-first-issue</code>, <code>help-wanted</code>, <code>duplicate</code>, <code>wontfix</code></p> <p>Triage Process: 1. Review new issues/PRs daily 2. Apply appropriate labels 3. Ask for clarification if needed 4. Close duplicates 5. Assign to milestone</p>"},{"location":"maintaining/#pr-review-checklist","title":"PR Review Checklist","text":"<p>Before merging:</p> <ul> <li> CI/CD passes (tests, mypy, ruff)</li> <li> TDD followed (tests included)</li> <li> Type hints present, mypy --strict passes</li> <li> Docstrings added/updated</li> <li> CHANGELOG.md updated (if user-facing)</li> <li> No merge conflicts</li> <li> Conventional commit format</li> <li> Reviewer approval</li> </ul>"},{"location":"maintaining/#security","title":"Security","text":"<p>Reporting: Security issues via private email, NOT public issues</p> <p>Update Process: 1. Assess severity 2. Patch privately 3. Test thoroughly 4. Release patch version 5. Announce (CHANGELOG, GitHub Security Advisory)</p>"},{"location":"maintaining/#versioning-policy","title":"Versioning Policy","text":"<p>Echomine follows Semantic Versioning 2.0.0.</p>"},{"location":"maintaining/#version-format-majorminorpatch","title":"Version Format: MAJOR.MINOR.PATCH","text":"<ul> <li>MAJOR: Breaking changes (remove functions, change signatures)</li> <li>MINOR: New features, backward compatible</li> <li>PATCH: Bug fixes, backward compatible</li> </ul>"},{"location":"maintaining/#examples","title":"Examples","text":"<ul> <li><code>1.0.0</code> \u2192 <code>2.0.0</code>: Breaking change (removed old API)</li> <li><code>1.0.0</code> \u2192 <code>1.1.0</code>: New feature (added search filtering)</li> <li><code>1.0.0</code> \u2192 <code>1.0.1</code>: Bug fix (fixed null timestamp crash)</li> </ul> <p>See Versioning Guide for details.</p>"},{"location":"maintaining/#release-cadence","title":"Release Cadence","text":"<ul> <li>Patch: As needed (bug fixes, security updates)</li> <li>Minor: Quarterly or when features ready</li> <li>Major: Annually or when breaking changes justified</li> </ul>"},{"location":"maintaining/#documentation-maintenance","title":"Documentation Maintenance","text":""},{"location":"maintaining/#when-to-update","title":"When to Update","text":"<ul> <li>Public API changes</li> <li>CLI command changes</li> <li>Breaking changes (migration guides)</li> <li>Bug fixes affecting behavior</li> <li>Performance improvements</li> </ul>"},{"location":"maintaining/#deployment","title":"Deployment","text":"<pre><code># Build and serve locally\nmkdocs serve\n\n# Deploy to GitHub Pages\nmkdocs gh-deploy\n</code></pre> <p>API reference is auto-generated from docstrings (mkdocstrings).</p>"},{"location":"maintaining/#dependency-management","title":"Dependency Management","text":""},{"location":"maintaining/#update-strategy","title":"Update Strategy","text":"<ul> <li>Security: Immediate</li> <li>Minor: Monthly review</li> <li>Major: Evaluate breaking changes</li> </ul>"},{"location":"maintaining/#commands","title":"Commands","text":"<pre><code># Check for security issues\npip-audit\n\n# Check outdated packages\npip list --outdated\n\n# Update specific package\npip install --upgrade &lt;package&gt;\n# Update pyproject.toml, test, commit\n</code></pre>"},{"location":"maintaining/#governance","title":"Governance","text":""},{"location":"maintaining/#decision-making","title":"Decision Making","text":"<ul> <li>Minor changes: Single maintainer approval</li> <li>Major changes: Consensus</li> <li>Breaking changes: RFC + consensus</li> </ul>"},{"location":"maintaining/#maintainer-responsibilities","title":"Maintainer Responsibilities","text":"<ul> <li>Review PRs within 1 week</li> <li>Triage issues within 2 business days</li> <li>Release patches for critical bugs within 24 hours</li> <li>Release minor versions quarterly</li> <li>Respond to security issues immediately</li> </ul>"},{"location":"maintaining/#support-channels","title":"Support Channels","text":"<ul> <li>GitHub Issues: https://github.com/echomine/echomine/issues</li> <li>GitHub Discussions: https://github.com/echomine/echomine/discussions</li> <li>PyPI: https://pypi.org/project/echomine/</li> </ul> <p>For complete details, see MAINTAINING.md.</p> <p>Last Updated: 2025-11-28</p>"},{"location":"maintaining/pypi-publishing/","title":"PyPI Publishing Guide","text":"<p>This guide covers publishing Echomine to the Python Package Index (PyPI).</p>"},{"location":"maintaining/pypi-publishing/#overview","title":"Overview","text":"<p>Echomine uses Trusted Publishing (OIDC) for secure, tokenless publishing from GitHub Actions.</p>"},{"location":"maintaining/pypi-publishing/#benefits-of-trusted-publishing","title":"Benefits of Trusted Publishing","text":"<ul> <li>No API tokens to manage or rotate</li> <li>No secrets to configure in GitHub</li> <li>Audit trail of which workflow published each release</li> <li>More secure than long-lived API tokens</li> </ul>"},{"location":"maintaining/pypi-publishing/#prerequisites","title":"Prerequisites","text":""},{"location":"maintaining/pypi-publishing/#1-pypi-account","title":"1. PyPI Account","text":"<p>Create accounts on: - PyPI (production) - TestPyPI (testing)</p>"},{"location":"maintaining/pypi-publishing/#2-public-repository","title":"2. Public Repository","text":"<p>Trusted Publishing requires a public GitHub repository. PyPI verifies the GitHub Actions workflow is running from your registered repository.</p>"},{"location":"maintaining/pypi-publishing/#3-package-name","title":"3. Package Name","text":"<p>Ensure <code>echomine</code> is available on PyPI or you own it.</p>"},{"location":"maintaining/pypi-publishing/#setup-trusted-publishing","title":"Setup Trusted Publishing","text":""},{"location":"maintaining/pypi-publishing/#step-1-configure-on-pypi","title":"Step 1: Configure on PyPI","text":"<ol> <li> <p>Go to PyPI Publishing Settings</p> </li> <li> <p>Click Add a new pending publisher</p> </li> <li> <p>Fill in the form:</p> </li> <li>PyPI Project Name: <code>echomine</code></li> <li>Owner: <code>aucontraire</code> (your GitHub username)</li> <li>Repository name: <code>echomine</code></li> <li>Workflow name: <code>release.yml</code></li> <li> <p>Environment name: <code>pypi</code> (optional but recommended)</p> </li> <li> <p>Click Add</p> </li> </ol>"},{"location":"maintaining/pypi-publishing/#step-2-configure-on-testpypi-optional","title":"Step 2: Configure on TestPyPI (Optional)","text":"<p>Repeat for TestPyPI at TestPyPI Publishing Settings: - Environment name: <code>testpypi</code></p>"},{"location":"maintaining/pypi-publishing/#step-3-verify-github-workflow","title":"Step 3: Verify GitHub Workflow","text":"<p>The workflow (<code>.github/workflows/release.yml</code>) should include:</p> <pre><code>jobs:\n  publish:\n    runs-on: ubuntu-latest\n    environment: pypi  # Must match PyPI config\n    permissions:\n      id-token: write  # Required for OIDC\n      contents: read\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Build package\n        run: python -m build\n\n      - name: Publish to PyPI\n        uses: pypa/gh-action-pypi-publish@release/v1\n        # No credentials needed - uses OIDC\n</code></pre>"},{"location":"maintaining/pypi-publishing/#publishing-workflow","title":"Publishing Workflow","text":""},{"location":"maintaining/pypi-publishing/#automatic-publishing-recommended","title":"Automatic Publishing (Recommended)","text":"<p>Releases are published automatically when you push a version tag:</p> <pre><code># Create and push tag\ngit tag v1.2.0\ngit push origin v1.2.0\n\n# GitHub Actions will:\n# 1. Run tests\n# 2. Build packages\n# 3. Publish to PyPI\n# 4. Create GitHub Release\n</code></pre>"},{"location":"maintaining/pypi-publishing/#manual-publishing-emergency","title":"Manual Publishing (Emergency)","text":"<p>If automated publishing fails:</p> <pre><code># Build packages locally\npython -m build\n\n# Check packages\ntwine check dist/*\n\n# Upload to TestPyPI first\ntwine upload --repository testpypi dist/*\n\n# Test installation\npip install --index-url https://test.pypi.org/simple/ echomine\n\n# Upload to PyPI\ntwine upload dist/*\n</code></pre> <p>Note: Manual upload requires API token. Generate at PyPI \u2192 Account Settings \u2192 API tokens.</p>"},{"location":"maintaining/pypi-publishing/#package-configuration","title":"Package Configuration","text":""},{"location":"maintaining/pypi-publishing/#pyprojecttoml","title":"pyproject.toml","text":"<pre><code>[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"echomine\"\nversion = \"1.0.0\"\ndescription = \"Library-first AI conversation export parser\"\nreadme = \"README.md\"\nlicense = {text = \"AGPL-3.0-or-later\"}\nrequires-python = \"&gt;=3.12\"\nauthors = [\n    {name = \"Your Name\", email = \"you@example.com\"}\n]\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: GNU Affero General Public License v3 or later (AGPLv3+)\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Programming Language :: Python :: 3.13\",\n    \"Topic :: Text Processing\",\n    \"Typing :: Typed\",\n]\nkeywords = [\"chatgpt\", \"openai\", \"conversation\", \"export\", \"parser\"]\n\ndependencies = [\n    \"pydantic&gt;=2.0\",\n    \"ijson&gt;=3.0\",\n    \"typer&gt;=0.9\",\n    \"rich&gt;=13.0\",\n    \"structlog&gt;=24.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest&gt;=8.0\",\n    \"pytest-cov&gt;=4.0\",\n    \"mypy&gt;=1.8\",\n    \"ruff&gt;=0.3\",\n]\n\n[project.urls]\nHomepage = \"https://github.com/aucontraire/echomine\"\nDocumentation = \"https://aucontraire.github.io/echomine/\"\nRepository = \"https://github.com/aucontraire/echomine\"\nChangelog = \"https://github.com/aucontraire/echomine/blob/main/CHANGELOG.md\"\n\n[project.scripts]\nechomine = \"echomine.cli:app\"\n</code></pre>"},{"location":"maintaining/pypi-publishing/#files-to-include","title":"Files to Include","text":"<p>Ensure these are included in the package:</p> <pre><code># pyproject.toml\n[tool.hatch.build.targets.sdist]\ninclude = [\n    \"/src\",\n    \"/LICENSE\",\n    \"/README.md\",\n    \"/CHANGELOG.md\",\n]\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src/echomine\"]\n</code></pre>"},{"location":"maintaining/pypi-publishing/#files-to-exclude","title":"Files to Exclude","text":"<p>Automatically excluded: - <code>__pycache__/</code> - <code>*.pyc</code> - <code>.git/</code> - <code>tests/</code> - <code>docs/</code></p>"},{"location":"maintaining/pypi-publishing/#testing-before-release","title":"Testing Before Release","text":""},{"location":"maintaining/pypi-publishing/#1-build-locally","title":"1. Build Locally","text":"<pre><code># Clean previous builds\nrm -rf dist/ build/ *.egg-info\n\n# Build\npython -m build\n\n# Check contents\ntar -tzf dist/echomine-1.0.0.tar.gz | head -20\nunzip -l dist/echomine-1.0.0-py3-none-any.whl | head -20\n</code></pre>"},{"location":"maintaining/pypi-publishing/#2-validate-package","title":"2. Validate Package","text":"<pre><code># Check package metadata\ntwine check dist/*\n\n# Should output:\n# Checking dist/echomine-1.0.0-py3-none-any.whl: PASSED\n# Checking dist/echomine-1.0.0.tar.gz: PASSED\n</code></pre>"},{"location":"maintaining/pypi-publishing/#3-test-on-testpypi","title":"3. Test on TestPyPI","text":"<pre><code># Upload to TestPyPI\ntwine upload --repository testpypi dist/*\n\n# Install from TestPyPI\npip install --index-url https://test.pypi.org/simple/ \\\n    --extra-index-url https://pypi.org/simple/ \\\n    echomine\n\n# Verify\nechomine --version\npython -c \"from echomine import OpenAIAdapter; print('OK')\"\n</code></pre>"},{"location":"maintaining/pypi-publishing/#4-test-in-clean-environment","title":"4. Test in Clean Environment","text":"<pre><code># Create fresh environment\npython -m venv test-env\nsource test-env/bin/activate\n\n# Install from TestPyPI\npip install --index-url https://test.pypi.org/simple/ \\\n    --extra-index-url https://pypi.org/simple/ \\\n    echomine\n\n# Run quick test\nechomine --help\npython -c \"from echomine import OpenAIAdapter, SearchQuery; print('All imports OK')\"\n\n# Clean up\ndeactivate\nrm -rf test-env\n</code></pre>"},{"location":"maintaining/pypi-publishing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"maintaining/pypi-publishing/#project-not-found-error","title":"\"Project not found\" Error","text":"<p>Cause: Trusted publisher not configured or repository not public.</p> <p>Solution: 1. Verify PyPI publishing settings 2. Ensure repository is public 3. Check workflow name matches exactly</p>"},{"location":"maintaining/pypi-publishing/#invalid-token-error","title":"\"Invalid token\" Error","text":"<p>Cause: OIDC token generation failed.</p> <p>Solution: 1. Ensure <code>permissions.id-token: write</code> in workflow 2. Check environment name matches PyPI config 3. Verify repository owner/name match</p>"},{"location":"maintaining/pypi-publishing/#file-already-exists-error","title":"\"File already exists\" Error","text":"<p>Cause: Trying to upload same version again.</p> <p>Solution: PyPI doesn't allow overwriting. Increment version number.</p>"},{"location":"maintaining/pypi-publishing/#package-installs-but-import-fails","title":"Package Installs But Import Fails","text":"<p>Cause: Package structure issues.</p> <p>Solution: 1. Check <code>[tool.hatch.build.targets.wheel].packages</code> 2. Verify <code>__init__.py</code> files exist 3. Test with <code>pip install -e .</code> first</p>"},{"location":"maintaining/pypi-publishing/#version-yanking","title":"Version Yanking","text":"<p>If you need to remove a bad release:</p> <pre><code># Yank (hide from default installs, still downloadable by version)\n# Must be done via PyPI web interface:\n# 1. Go to https://pypi.org/manage/project/echomine/releases/\n# 2. Click on the version\n# 3. Click \"Options\" \u2192 \"Yank\"\n\n# Or via API (requires token)\npip index yank echomine==1.2.0\n</code></pre> <p>Note: Yanking is reversible. For permanent removal, contact PyPI support.</p>"},{"location":"maintaining/pypi-publishing/#security-best-practices","title":"Security Best Practices","text":""},{"location":"maintaining/pypi-publishing/#do","title":"Do","text":"<ul> <li>Use Trusted Publishing (OIDC) instead of API tokens</li> <li>Use environment protection rules in GitHub</li> <li>Review workflow changes carefully</li> <li>Test on TestPyPI before production</li> </ul>"},{"location":"maintaining/pypi-publishing/#dont","title":"Don't","text":"<ul> <li>Commit API tokens to repository</li> <li>Share API tokens</li> <li>Use long-lived tokens when OIDC is available</li> <li>Skip TestPyPI testing</li> </ul>"},{"location":"maintaining/pypi-publishing/#environment-protection-optional","title":"Environment Protection (Optional)","text":"<p>Add protection rules in GitHub:</p> <ol> <li>Go to Settings \u2192 Environments</li> <li>Create <code>pypi</code> environment</li> <li>Add protection rules:</li> <li>Required reviewers</li> <li>Wait timer</li> <li>Deployment branches (only <code>main</code>)</li> </ol>"},{"location":"maintaining/pypi-publishing/#next-steps","title":"Next Steps","text":"<ul> <li>Release Process: Full release workflow</li> <li>Versioning: Version numbering</li> </ul> <p>Last Updated: 2025-11-30</p>"},{"location":"maintaining/release-process/","title":"Release Process","text":"<p>This guide covers the step-by-step process for releasing new versions of Echomine.</p>"},{"location":"maintaining/release-process/#release-types","title":"Release Types","text":"Type Version Change When to Use Patch 1.0.0 \u2192 1.0.1 Bug fixes, security patches Minor 1.0.0 \u2192 1.1.0 New features, backward compatible Major 1.0.0 \u2192 2.0.0 Breaking changes <p>See Versioning for detailed version policy.</p>"},{"location":"maintaining/release-process/#pre-release-checklist","title":"Pre-Release Checklist","text":"<p>Before starting a release:</p> <ul> <li> All tests pass: <code>pytest --cov=echomine</code></li> <li> Type checking passes: <code>mypy --strict src/echomine/</code></li> <li> Linting passes: <code>ruff check src/ tests/</code></li> <li> Documentation builds: <code>mkdocs build --strict</code></li> <li> No blocking issues in GitHub Issues</li> <li> CHANGELOG.md is up to date</li> </ul>"},{"location":"maintaining/release-process/#release-workflow","title":"Release Workflow","text":""},{"location":"maintaining/release-process/#step-1-create-release-branch","title":"Step 1: Create Release Branch","text":"<pre><code># Ensure main is up to date\ngit checkout main\ngit pull origin main\n\n# Create release branch\ngit checkout -b release/v1.2.0\n</code></pre>"},{"location":"maintaining/release-process/#step-2-update-version","title":"Step 2: Update Version","text":"<p>Update version in <code>pyproject.toml</code>:</p> <pre><code>[project]\nname = \"echomine\"\nversion = \"1.2.0\"  # Update this\n</code></pre>"},{"location":"maintaining/release-process/#step-3-update-changelogmd","title":"Step 3: Update CHANGELOG.md","text":"<p>Add release notes following Keep a Changelog format:</p> <pre><code>## [1.2.0] - 2024-01-15\n\n### Added\n- New `--title` flag for search command (#123)\n- Support for date-only filtering (#125)\n\n### Changed\n- Improved BM25 ranking performance by 30%\n\n### Fixed\n- Fixed crash when export contains null timestamps (#127)\n\n### Deprecated\n- `--keywords-only` flag (use `--title` instead)\n</code></pre>"},{"location":"maintaining/release-process/#step-4-run-final-quality-checks","title":"Step 4: Run Final Quality Checks","text":"<pre><code># All tests\npytest --cov=echomine --cov-report=term-missing\n\n# Type checking\nmypy --strict src/echomine/\n\n# Linting\nruff check src/ tests/\nruff format --check src/ tests/\n\n# Documentation\nmkdocs build --strict\n\n# Build packages\npython -m build\n</code></pre>"},{"location":"maintaining/release-process/#step-5-commit-and-tag","title":"Step 5: Commit and Tag","text":"<pre><code># Commit version bump\ngit add pyproject.toml CHANGELOG.md\ngit commit -m \"chore: release v1.2.0\"\n\n# Push release branch\ngit push origin release/v1.2.0\n\n# Create and push tag\ngit tag -a v1.2.0 -m \"Release v1.2.0\"\ngit push origin v1.2.0\n</code></pre>"},{"location":"maintaining/release-process/#step-6-create-pull-request","title":"Step 6: Create Pull Request","text":"<p>Create PR from <code>release/v1.2.0</code> to <code>main</code>:</p> <pre><code>gh pr create \\\n  --title \"Release v1.2.0\" \\\n  --body \"## Release v1.2.0\n\nSee CHANGELOG.md for details.\n\n## Checklist\n- [x] Version updated in pyproject.toml\n- [x] CHANGELOG.md updated\n- [x] All tests pass\n- [x] Documentation builds\"\n</code></pre>"},{"location":"maintaining/release-process/#step-7-merge-and-publish","title":"Step 7: Merge and Publish","text":"<p>After PR approval:</p> <ol> <li>Merge PR to main</li> <li>GitHub Actions automatically:</li> <li>Runs all tests</li> <li>Builds packages</li> <li>Publishes to PyPI (via trusted publishing)</li> <li>Creates GitHub Release</li> </ol>"},{"location":"maintaining/release-process/#step-8-verify-release","title":"Step 8: Verify Release","text":"<pre><code># Check PyPI\npip install echomine==1.2.0\n\n# Verify version\nechomine --version\n# echomine 1.2.0\n\n# Check GitHub Release\ngh release view v1.2.0\n</code></pre>"},{"location":"maintaining/release-process/#automated-release-github-actions","title":"Automated Release (GitHub Actions)","text":"<p>The release workflow (<code>.github/workflows/release.yml</code>) handles:</p> <ol> <li>Build: Creates wheel and sdist packages</li> <li>Test: Installs on all platforms (Ubuntu, macOS, Windows)</li> <li>Publish: Uploads to PyPI via trusted publishing</li> <li>Release: Creates GitHub Release with changelog</li> </ol>"},{"location":"maintaining/release-process/#triggering-a-release","title":"Triggering a Release","text":"<p>Releases are triggered by version tags:</p> <pre><code>git tag v1.2.0\ngit push origin v1.2.0\n# GitHub Actions takes over\n</code></pre>"},{"location":"maintaining/release-process/#manual-trigger","title":"Manual Trigger","text":"<p>For testing or re-runs:</p> <pre><code>gh workflow run release.yml \\\n  --field version=1.2.0\n</code></pre>"},{"location":"maintaining/release-process/#hotfix-process","title":"Hotfix Process","text":"<p>For urgent bug fixes:</p> <pre><code># Branch from latest release tag\ngit checkout -b hotfix/v1.2.1 v1.2.0\n\n# Fix the bug\n# ... make changes ...\n\n# Update version to patch\n# pyproject.toml: version = \"1.2.1\"\n\n# Update CHANGELOG\n# Add entry under [1.2.1]\n\n# Commit, tag, push\ngit add -A\ngit commit -m \"fix: critical bug in search (#999)\"\ngit tag v1.2.1\ngit push origin hotfix/v1.2.1 v1.2.1\n\n# Create PR to main\ngh pr create --title \"Hotfix v1.2.1\"\n</code></pre>"},{"location":"maintaining/release-process/#release-notes-template","title":"Release Notes Template","text":"<pre><code>## What's New in v1.2.0\n\n### Highlights\n\nBrief summary of the most important changes.\n\n### New Features\n\n- **Feature Name**: Description of what it does and why it's useful.\n  ```python\n  # Example usage\n  ```\n\n### Improvements\n\n- Performance: 30% faster search on large files\n- Memory: Reduced memory usage for streaming\n\n### Bug Fixes\n\n- Fixed crash when... (#123)\n- Fixed incorrect results when... (#125)\n\n### Breaking Changes\n\n(Only for major releases)\n\n- Removed deprecated `old_function()`. Use `new_function()` instead.\n- Changed return type of `search()` from `List` to `Iterator`.\n\n### Migration Guide\n\n(Only for breaking changes)\n\n```python\n# Before (v1.x)\nresults = adapter.search(path, query)\nfor r in results:\n    print(r)\n\n# After (v2.x)\nresults = list(adapter.search(path, query))\nfor r in results:\n    print(r)\n</code></pre>"},{"location":"maintaining/release-process/#contributors","title":"Contributors","text":"<p>Thanks to everyone who contributed: - @username1 - @username2</p>"},{"location":"maintaining/release-process/#full-changelog","title":"Full Changelog","text":"<p>See CHANGELOG.md for complete details. <pre><code>## Rollback Process\n\nIf a release has critical issues:\n\n### 1. Yank from PyPI\n\n```bash\n# Yank specific version (makes it uninstallable for new users)\npip index yank echomine==1.2.0\n</code></pre></p>"},{"location":"maintaining/release-process/#2-create-hotfix","title":"2. Create Hotfix","text":"<p>Follow the hotfix process above.</p>"},{"location":"maintaining/release-process/#3-communicate","title":"3. Communicate","text":"<ul> <li>Update GitHub Release notes with warning</li> <li>Post in Discussions if applicable</li> <li>Update CHANGELOG with note about yanked version</li> </ul>"},{"location":"maintaining/release-process/#release-schedule","title":"Release Schedule","text":"<ul> <li>Patch releases: As needed for bug fixes</li> <li>Minor releases: When features are ready (no fixed schedule)</li> <li>Major releases: Planned, with advance notice</li> </ul>"},{"location":"maintaining/release-process/#next-steps","title":"Next Steps","text":"<ul> <li>Versioning: Version numbering policy</li> <li>PyPI Publishing: Publishing setup</li> </ul> <p>Last Updated: 2025-11-30</p>"},{"location":"maintaining/versioning/","title":"Versioning Policy","text":"<p>Echomine follows Semantic Versioning 2.0.0 (SemVer).</p>"},{"location":"maintaining/versioning/#version-format","title":"Version Format","text":"<pre><code>MAJOR.MINOR.PATCH\n  \u2502     \u2502     \u2502\n  \u2502     \u2502     \u2514\u2500\u2500 Bug fixes (backward compatible)\n  \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 New features (backward compatible)\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Breaking changes\n</code></pre> <p>Example: <code>1.2.3</code> - Major: 1 - Minor: 2 - Patch: 3</p>"},{"location":"maintaining/versioning/#when-to-increment","title":"When to Increment","text":""},{"location":"maintaining/versioning/#patch-100-101","title":"PATCH (1.0.0 \u2192 1.0.1)","text":"<p>Increment for backward-compatible bug fixes:</p> <ul> <li>Fix crashes or errors</li> <li>Fix incorrect behavior</li> <li>Security patches</li> <li>Performance improvements (same API)</li> <li>Documentation fixes</li> </ul> <p>Examples: - Fix: search returning wrong results for date filters - Fix: crash when export has null timestamps - Fix: memory leak in streaming parser</p>"},{"location":"maintaining/versioning/#minor-100-110","title":"MINOR (1.0.0 \u2192 1.1.0)","text":"<p>Increment for backward-compatible new features:</p> <ul> <li>New functions or methods</li> <li>New optional parameters</li> <li>New CLI commands or flags</li> <li>Deprecations (old API still works)</li> </ul> <p>Examples: - Add: <code>--title</code> flag to search command - Add: <code>export_to_html()</code> function - Add: Claude adapter support - Deprecate: <code>--keywords-only</code> (replaced by <code>--title</code>)</p>"},{"location":"maintaining/versioning/#major-100-200","title":"MAJOR (1.0.0 \u2192 2.0.0)","text":"<p>Increment for breaking changes:</p> <ul> <li>Remove functions or methods</li> <li>Change function signatures (required params)</li> <li>Change return types</li> <li>Change default behavior</li> <li>Remove deprecated features</li> </ul> <p>Examples: - Remove: deprecated <code>search_legacy()</code> function - Change: <code>search()</code> returns <code>Iterator</code> instead of <code>List</code> - Change: <code>--output</code> is now required for export command - Change: minimum Python version from 3.11 to 3.12</p>"},{"location":"maintaining/versioning/#backward-compatibility","title":"Backward Compatibility","text":""},{"location":"maintaining/versioning/#what-is-backward-compatible","title":"What is Backward Compatible?","text":"<p>Changes that don't break existing code:</p> <pre><code># v1.0.0 - Original\ndef search(path: Path, query: SearchQuery) -&gt; Iterator[SearchResult]:\n    pass\n\n# v1.1.0 - Backward compatible (new optional param)\ndef search(\n    path: Path,\n    query: SearchQuery,\n    *,\n    progress_callback: Optional[Callable] = None,  # New, optional\n) -&gt; Iterator[SearchResult]:\n    pass\n</code></pre>"},{"location":"maintaining/versioning/#what-is-not-backward-compatible","title":"What is NOT Backward Compatible?","text":"<p>Changes that break existing code:</p> <pre><code># v1.0.0 - Original\ndef search(path: Path, query: SearchQuery) -&gt; list[SearchResult]:\n    pass\n\n# v2.0.0 - Breaking (return type changed)\ndef search(path: Path, query: SearchQuery) -&gt; Iterator[SearchResult]:\n    pass\n</code></pre>"},{"location":"maintaining/versioning/#pre-release-versions","title":"Pre-Release Versions","text":"<p>For testing before stable release:</p> <pre><code>1.0.0-alpha.1  # Early development\n1.0.0-beta.1   # Feature complete, testing\n1.0.0-rc.1     # Release candidate\n1.0.0          # Stable release\n</code></pre>"},{"location":"maintaining/versioning/#usage","title":"Usage","text":"<pre><code># Install pre-release\npip install echomine==1.0.0-beta.1\n\n# Install stable only (default)\npip install echomine\n</code></pre>"},{"location":"maintaining/versioning/#development-versions","title":"Development Versions","text":"<p>For local development:</p> <pre><code>1.0.0.dev1     # Development snapshot\n1.0.0.dev2     # Later snapshot\n</code></pre> <p>These are never published to PyPI.</p>"},{"location":"maintaining/versioning/#version-in-code","title":"Version in Code","text":"<p>Version is defined in <code>pyproject.toml</code>:</p> <pre><code>[project]\nname = \"echomine\"\nversion = \"1.2.0\"\n</code></pre> <p>Access programmatically:</p> <pre><code>from importlib.metadata import version\n\nechomine_version = version(\"echomine\")\nprint(f\"Echomine {echomine_version}\")\n</code></pre> <p>Or via CLI:</p> <pre><code>echomine --version\n# echomine 1.2.0\n</code></pre>"},{"location":"maintaining/versioning/#deprecation-policy","title":"Deprecation Policy","text":""},{"location":"maintaining/versioning/#how-to-deprecate","title":"How to Deprecate","text":"<ol> <li>Mark as deprecated with warning:</li> </ol> <pre><code>import warnings\n\ndef old_function():\n    \"\"\"Old function.\n\n    .. deprecated:: 1.2.0\n        Use :func:`new_function` instead.\n    \"\"\"\n    warnings.warn(\n        \"old_function is deprecated, use new_function instead\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return new_function()\n</code></pre> <ol> <li>Document in CHANGELOG:</li> </ol> <pre><code>### Deprecated\n- `old_function()` - Use `new_function()` instead. Will be removed in v2.0.0.\n</code></pre> <ol> <li> <p>Keep working for at least one minor version</p> </li> <li> <p>Remove in next major version</p> </li> </ol>"},{"location":"maintaining/versioning/#deprecation-timeline","title":"Deprecation Timeline","text":"<pre><code>v1.0.0: Function introduced\nv1.1.0: Function deprecated (still works, emits warning)\nv1.2.0: Function still works (minimum one minor version)\nv2.0.0: Function removed\n</code></pre>"},{"location":"maintaining/versioning/#api-stability","title":"API Stability","text":""},{"location":"maintaining/versioning/#stable-public-api","title":"Stable (Public) API","text":"<p>Everything documented in: - API Reference documentation - README examples - CLI <code>--help</code> output</p> <p>Stability guarantee: Follow SemVer strictly.</p>"},{"location":"maintaining/versioning/#unstable-internal-api","title":"Unstable (Internal) API","text":"<ul> <li>Functions/classes prefixed with <code>_</code></li> <li>Modules in <code>_internal/</code> directories</li> <li>Anything not documented</li> </ul> <p>No stability guarantee: May change without notice.</p> <pre><code># Stable - follows SemVer\nfrom echomine import OpenAIAdapter, SearchQuery\n\n# Unstable - may change anytime\nfrom echomine._internal import _parse_raw_message\n</code></pre>"},{"location":"maintaining/versioning/#python-version-support","title":"Python Version Support","text":""},{"location":"maintaining/versioning/#policy","title":"Policy","text":"<p>Support Python versions that are: - Not end-of-life (EOL) - At least 12 months old (for ecosystem stability)</p>"},{"location":"maintaining/versioning/#current-support","title":"Current Support","text":"Python Status Notes 3.12+ Supported Minimum version 3.11 Not supported Missing required features 3.10 Not supported Missing required features"},{"location":"maintaining/versioning/#dropping-python-versions","title":"Dropping Python Versions","text":"<p>Dropping a Python version is a major version bump:</p> <pre><code>echomine 1.x: Python 3.12+\nechomine 2.x: Python 3.13+ (hypothetical)\n</code></pre>"},{"location":"maintaining/versioning/#version-comparison","title":"Version Comparison","text":"<p>Users can compare versions:</p> <pre><code>from packaging.version import Version\n\ncurrent = Version(\"1.2.0\")\nrequired = Version(\"1.1.0\")\n\nif current &gt;= required:\n    print(\"Version OK\")\n</code></pre>"},{"location":"maintaining/versioning/#faq","title":"FAQ","text":""},{"location":"maintaining/versioning/#when-should-i-use-a-pre-release","title":"When should I use a pre-release?","text":"<p>For significant changes that need community testing before stable release.</p>"},{"location":"maintaining/versioning/#can-i-skip-version-numbers","title":"Can I skip version numbers?","text":"<p>Yes. Version numbers don't need to be sequential: - 1.0.0 \u2192 1.0.5 is fine (skipped 1.0.1-1.0.4) - 1.2.0 \u2192 2.0.0 is fine (skipped 1.3.0+)</p>"},{"location":"maintaining/versioning/#what-about-0x-versions","title":"What about 0.x versions?","text":"<p>For initial development before first stable release: - 0.x.y: Anything may change at any time - Breaking changes don't require major bump</p> <p>Once you release 1.0.0, commit to SemVer.</p>"},{"location":"maintaining/versioning/#how-do-i-handle-urgent-security-fixes","title":"How do I handle urgent security fixes?","text":"<p>Release a patch version immediately: 1. Fix on a hotfix branch 2. Increment patch version 3. Release ASAP</p>"},{"location":"maintaining/versioning/#next-steps","title":"Next Steps","text":"<ul> <li>Release Process: How to release</li> <li>PyPI Publishing: Publishing setup</li> </ul> <p>Last Updated: 2025-11-30</p>"}]}